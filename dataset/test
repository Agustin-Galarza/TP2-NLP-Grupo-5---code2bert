public String getLocalizerId() {return localizerId;}public FileHandle getHandle() {return this.handle;}
public void serialize(XDR xdr) {handle.serialize(xdr);}public static void setDatagramSocket(AbstractGangliaSink gangliaSink, DatagramSocket datagramSocket) {gangliaSink.setDatagramSocket(datagramSocket);}public float getProgress() {return this.progress;}public static void createCluster() {HDFSContract.createCluster();}
public static void teardownCluster() {HDFSContract.destroyCluster();}
protected AbstractFSContract createContract(Configuration conf) {return new HDFSContract(conf);}public static Object createInstance(String className) {return org.apache.hadoop.mapreduce.lib.aggregate.UserDefinedValueAggregatorDescriptor.createInstance(className);}
public void configure(JobConf job) {}public void extend(double newProgress, int newValue) {real.extend(newProgress, newValue);}String getMinimumNameNodeVersion() {return this.minimumNameNodeVersion;}
public boolean getEncryptDataTransfer() {return encryptDataTransfer;}
public String getEncryptionAlgorithm() {return encryptionAlgorithm;}
public long getXceiverStopTimeout() {return xceiverStopTimeout;}
public long getMaxLockedMemory() {return maxLockedMemory;}
public SaslPropertiesResolver getSaslPropsResolver() {return saslPropsResolver;}
public TrustedChannelResolver getTrustedChannelResolver() {return trustedChannelResolver;}void initialize(int live, int decommissioned, int corrupt, int excess, int stale) {liveReplicas = live;decommissionedReplicas = decommissioned;corruptReplicas = corrupt;excessReplicas = excess;replicasOnStaleNodes = stale;}
public int liveReplicas() {return liveReplicas;}
public int decommissionedReplicas() {return decommissionedReplicas;}
public int corruptReplicas() {return corruptReplicas;}
public int excessReplicas() {return excessReplicas;}
public int replicasOnStaleNodes() {return replicasOnStaleNodes;}public LocatedBlocks getBlockLocations() {return locations;}
public final LocatedFileStatus makeQualifiedLocated(URI defaultUri, Path path) {return new LocatedFileStatus(getLen(), isDir(), getReplication(), getBlockSize(), getModificationTime(), getAccessTime(), getPermission(), getOwner(), getGroup(), isSymlink() ? new Path(getSymlink()) : null, (getFullPath(path)).makeQualified(defaultUri, // fully-qualify pathnull), DFSUtil.locatedBlocks2Locations(getBlockLocations()));}public ZombieJob getNextJob() {LoggedJob job = reader.getNext();if (job == null) {return null;} else if (hasRandomSeed) {long subRandomSeed = RandomSeedGenerator.getSeed("forZombieJob" + job.getJobID(), randomSeed);return new ZombieJob(job, cluster, subRandomSeed);} else {return new ZombieJob(job, cluster);}}
public void close() {reader.close();}public void go() {BufferedReader in = new BufferedReader(new InputStreamReader(System.in));String line;String prevLine = null;while ((line = in.readLine()) != null) {if (!line.equals(prevLine)) {System.out.println(header + line);}prevLine = line;}}
public static void main(String[] args) {String h = (args.length < 1) ? "" : args[0];UniqApp app = new UniqApp(h);app.go();}public static DFSClient getDFSClient(DistributedFileSystem dfs) {return dfs.dfs;}
public static void setDFSClient(DistributedFileSystem dfs, DFSClient client) {dfs.dfs = client;}
public static void stopLeaseRenewer(DistributedFileSystem dfs) {try {dfs.dfs.getLeaseRenewer().interruptAndJoin();} catch (InterruptedException e) {throw new IOException(e);}}
public static LocatedBlocks callGetBlockLocations(ClientProtocol namenode, String src, long start, long length) {return DFSClient.callGetBlockLocations(namenode, src, start, length);}
public static ClientProtocol getNamenode(DFSClient client) {return client.namenode;}
public static DFSClient getClient(DistributedFileSystem dfs) {return dfs.dfs;}
public static ExtendedBlock getPreviousBlock(DFSClient client, long fileId) {return client.getPreviousBlock(fileId);}
public static long getFileId(DFSOutputStream out) {return out.getFileId();}public ReadableByteChannel getInputStreamChannel() {return in;}
public void setReadTimeout(int timeoutMs) {in.setTimeout(timeoutMs);}
public int getReceiveBufferSize() {return socket.getReceiveBufferSize();}
public boolean getTcpNoDelay() {return socket.getTcpNoDelay();}
public void setWriteTimeout(int timeoutMs) {out.setTimeout(timeoutMs);}
public boolean isClosed() {return socket.isClosed();}
public void close() {// Closing either one of these will also close the Socket.try {in.close();} finally {out.close();}}
public String getRemoteAddressString() {return socket.getRemoteSocketAddress().toString();}
public String getLocalAddressString() {return socket.getLocalSocketAddress().toString();}
public InputStream getInputStream() {return in;}
public OutputStream getOutputStream() {return out;}
public boolean isLocal() {return isLocal;}
public String toString() {return "NioInetPeer(" + socket.toString() + ")";}
public DomainSocket getDomainSocket() {return null;}
public boolean hasSecureChannel() {return false;}public RegisterNodeManagerResponse registerNodeManager(RegisterNodeManagerRequest request) {RegisterNodeManagerResponse response = recordFactory.newRecordInstance(RegisterNodeManagerResponse.class);MasterKey masterKey = new MasterKeyPBImpl();masterKey.setKeyId(123);masterKey.setBytes(ByteBuffer.wrap(new byte[] { new Integer(123).byteValue() }));response.setContainerTokenMasterKey(masterKey);response.setNMTokenMasterKey(masterKey);return response;}
public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) {NodeHeartbeatResponse response = recordFactory.newRecordInstance(NodeHeartbeatResponse.class);return response;}public int compareTo(Step other) {// step and sort on read.return new CompareToBuilder().append(file, other.file).append(sequenceNumber, other.sequenceNumber).toComparison();}
public boolean equals(Object otherObj) {if (otherObj == null || otherObj.getClass() != getClass()) {return false;}Step other = (Step) otherObj;return new EqualsBuilder().append(this.file, other.file).append(this.size, other.size).append(this.type, other.type).isEquals();}
public String getFile() {return file;}
public long getSize() {return size;}
public StepType getType() {return type;}
public int hashCode() {return new HashCodeBuilder().append(file).append(size).append(type).toHashCode();}public boolean metricsAvailable() {return userMetricsAvailable;}
public int getAppsSubmitted() {return this.appsSubmitted;}
public int getAppsCompleted() {return appsCompleted;}
public int getAppsPending() {return appsPending;}
public int getAppsRunning() {return appsRunning;}
public int getAppsFailed() {return appsFailed;}
public int getAppsKilled() {return appsKilled;}
public long getReservedMB() {return this.reservedMB;}
public long getAllocatedMB() {return this.allocatedMB;}
public long getPendingMB() {return this.pendingMB;}
public long getReservedVirtualCores() {return this.reservedVirtualCores;}
public long getAllocatedVirtualCores() {return this.allocatedVirtualCores;}
public long getPendingVirtualCores() {return this.pendingVirtualCores;}
public int getReservedContainers() {return this.reservedContainers;}
public int getRunningContainers() {return this.runningContainers;}
public int getPendingContainers() {return this.pendingContainers;}protected ResourceTracker createResourceTracker() {return new MockResourceTracker();}
protected ResourceTracker getRMClient() {return resourceTracker;}
protected void stopRMProxy() {return;}
public RegisterNodeManagerResponse registerNodeManager(RegisterNodeManagerRequest request) {RegisterNodeManagerResponse response = recordFactory.newRecordInstance(RegisterNodeManagerResponse.class);MasterKey masterKey = new MasterKeyPBImpl();masterKey.setKeyId(123);masterKey.setBytes(ByteBuffer.wrap(new byte[] { new Integer(123).byteValue() }));response.setContainerTokenMasterKey(masterKey);response.setNMTokenMasterKey(masterKey);return response;}
public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) {NodeStatus nodeStatus = request.getNodeStatus();LOG.info("Got heartbeat number " + heartBeatID);nodeStatus.setResponseId(heartBeatID++);NodeHeartbeatResponse nhResponse = YarnServerBuilderUtils.newNodeHeartbeatResponse(heartBeatID, null, null, null, null, null, 1000L);return nhResponse;}protected String getContractXml() {return CONTRACT_XML;}
public void init() {super.init();fs = getLocalFS();adjustContractToLocalEnvironment();}
protected void adjustContractToLocalEnvironment() {if (Shell.WINDOWS) {//NTFS doesn't do case sensitivity, and its permissions are ACL-basedgetConf().setBoolean(getConfKey(ContractOptions.IS_CASE_SENSITIVE), false);getConf().setBoolean(getConfKey(ContractOptions.SUPPORTS_UNIX_PERMISSIONS), false);} else if (ContractTestUtils.isOSX()) {//OSX HFS+ is not case sensitivegetConf().setBoolean(getConfKey(ContractOptions.IS_CASE_SENSITIVE), false);}}
protected FileSystem getLocalFS() {return FileSystem.getLocal(getConf());}
public FileSystem getTestFileSystem() {return fs;}
public String getScheme() {return "file";}
public Path getTestPath() {Path path = fs.makeQualified(new Path(getTestDataDir()));return path;}
protected String getTestDataDir() {return System.getProperty(SYSPROP_TEST_BUILD_DATA, DEFAULT_TEST_BUILD_DATA_DIR);}public LocalResource getResource() {return rsrc;}
public ResourceStatusType getStatus() {return tag;}
public long getLocalSize() {return size;}
public URL getLocalPath() {return localPath;}
public SerializedException getException() {return ex;}
public void setResource(LocalResource rsrc) {this.rsrc = rsrc;}
public void setStatus(ResourceStatusType tag) {this.tag = tag;}
public void setLocalPath(URL localPath) {this.localPath = localPath;}
public void setLocalSize(long size) {this.size = size;}
public void setException(SerializedException ex) {this.ex = ex;}
public boolean equals(Object o) {if (!(o instanceof MockLocalResourceStatus)) {return false;}MockLocalResourceStatus other = (MockLocalResourceStatus) o;return getResource().equals(other.getResource()) && getStatus().equals(other.getStatus()) && (null != getLocalPath() && getLocalPath().equals(other.getLocalPath())) && (null != getException() && getException().equals(other.getException()));}
public int hashCode() {return 4344;}public Service[] getServices() {return null;}public void testRemoveLeaseWithPrefixPath() {MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();cluster.waitActive();LeaseManager lm = NameNodeAdapter.getLeaseManager(cluster.getNamesystem());lm.addLease("holder1", "/a/b");lm.addLease("holder2", "/a/c");assertNotNull(lm.getLeaseByPath("/a/b"));assertNotNull(lm.getLeaseByPath("/a/c"));lm.removeLeaseWithPrefixPath("/a");assertNull(lm.getLeaseByPath("/a/b"));assertNull(lm.getLeaseByPath("/a/c"));lm.addLease("holder1", "/a/b");lm.addLease("holder2", "/a/c");lm.removeLeaseWithPrefixPath("/a/");assertNull(lm.getLeaseByPath("/a/b"));assertNull(lm.getLeaseByPath("/a/c"));}public void setUp() {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION).build();cluster.waitActive();hdfs = cluster.getFileSystem();}
public void tearDown() {if (cluster != null) {cluster.shutdown();cluster = null;}}
public void TestSnapshotWithInvalidName() {Path file1 = new Path(dir1, file1Name);DFSTestUtil.createFile(hdfs, file1, BLOCKSIZE, REPLICATION, SEED);hdfs.allowSnapshot(dir1);try {hdfs.createSnapshot(dir1, snapshot1);} catch (RemoteException e) {}}
public void TestSnapshotWithInvalidName1() {Path file1 = new Path(dir1, file1Name);DFSTestUtil.createFile(hdfs, file1, BLOCKSIZE, REPLICATION, SEED);hdfs.allowSnapshot(dir1);try {hdfs.createSnapshot(dir1, snapshot2);} catch (RemoteException e) {}}protected void setUp() {testAccount = AzureBlobStorageTestAccount.createForEmulator();if (testAccount != null) {fs = testAccount.getFileSystem();}}
protected void tearDown() {if (testAccount != null) {testAccount.cleanup();testAccount = null;fs = null;}}
protected void runTest() {if (testAccount != null) {super.runTest();}}public static RpcServerFactory getServerFactory(Configuration conf) {if (conf == null) {conf = new Configuration();}String serverFactoryClassName = conf.get(YarnConfiguration.IPC_SERVER_FACTORY_CLASS, YarnConfiguration.DEFAULT_IPC_SERVER_FACTORY_CLASS);return (RpcServerFactory) getFactoryClassInstance(serverFactoryClassName);}
public static RpcClientFactory getClientFactory(Configuration conf) {String clientFactoryClassName = conf.get(YarnConfiguration.IPC_CLIENT_FACTORY_CLASS, YarnConfiguration.DEFAULT_IPC_CLIENT_FACTORY_CLASS);return (RpcClientFactory) getFactoryClassInstance(clientFactoryClassName);}
private static Object getFactoryClassInstance(String factoryClassName) {try {Class<?> clazz = Class.forName(factoryClassName);Method method = clazz.getMethod("get", null);method.setAccessible(true);return method.invoke(null, null);} catch (ClassNotFoundException e) {throw new YarnRuntimeException(e);} catch (NoSuchMethodException e) {throw new YarnRuntimeException(e);} catch (InvocationTargetException e) {throw new YarnRuntimeException(e);} catch (IllegalAccessException e) {throw new YarnRuntimeException(e);}}public void testUserFromEnvironment() {System.setProperty(UserGroupInformation.HADOOP_USER_NAME, "randomUser");Assert.assertEquals("randomUser", UserGroupInformation.getLoginUser().getUserName());}static String[] getTrimmedStrings(String str) {if (null == str || "".equals(str.trim())) {return emptyStringArray;}return str.trim().split("\\s*,\\s*");}
static String toByteInfo(long bytes) {StringBuilder str = new StringBuilder();if (bytes < 0) {bytes = 0;}str.append(bytes);str.append(" bytes or ");str.append(bytes / 1024);str.append(" kilobytes or ");str.append(bytes / (1024 * 1024));str.append(" megabytes or ");str.append(bytes / (1024 * 1024 * 1024));str.append(" gigabytes");return str.toString();}
static String stringifyArray(Object[] args, String sep) {StringBuilder optStr = new StringBuilder();for (int i = 0; i < args.length; ++i) {optStr.append(args[i]);if ((i + 1) != args.length) {optStr.append(sep);}}return optStr.toString();}public void testConstructQuery() {String actual = format.constructQuery("hadoop_output", fieldNames);assertEquals(expected, actual);actual = format.constructQuery("hadoop_output", nullFieldNames);assertEquals(nullExpected, actual);}
public void testSetOutput() {JobConf job = new JobConf();DBOutputFormat.setOutput(job, "hadoop_output", fieldNames);DBConfiguration dbConf = new DBConfiguration(job);String actual = format.constructQuery(dbConf.getOutputTableName(), dbConf.getOutputFieldNames());assertEquals(expected, actual);job = new JobConf();dbConf = new DBConfiguration(job);DBOutputFormat.setOutput(job, "hadoop_output", nullFieldNames.length);assertNull(dbConf.getOutputFieldNames());assertEquals(nullFieldNames.length, dbConf.getOutputFieldCount());actual = format.constructQuery(dbConf.getOutputTableName(), new String[dbConf.getOutputFieldCount()]);assertEquals(nullExpected, actual);}public HAServiceState getState() {return state;}
public HAServiceStatus setReadyToBecomeActive() {this.readyToBecomeActive = true;this.notReadyReason = null;return this;}
public HAServiceStatus setNotReadyToBecomeActive(String reason) {this.readyToBecomeActive = false;this.notReadyReason = reason;return this;}
public boolean isReadyToBecomeActive() {return readyToBecomeActive;}
public String getNotReadyReason() {return notReadyReason;}public void testConstructQuery() {String actual = format.constructQuery("hadoop_output", fieldNames);assertEquals(expected, actual);actual = format.constructQuery("hadoop_output", nullFieldNames);assertEquals(nullExpected, actual);}
public void testSetOutput() {Job job = Job.getInstance(new Configuration());DBOutputFormat.setOutput(job, "hadoop_output", fieldNames);DBConfiguration dbConf = new DBConfiguration(job.getConfiguration());String actual = format.constructQuery(dbConf.getOutputTableName(), dbConf.getOutputFieldNames());assertEquals(expected, actual);job = Job.getInstance(new Configuration());dbConf = new DBConfiguration(job.getConfiguration());DBOutputFormat.setOutput(job, "hadoop_output", nullFieldNames.length);assertNull(dbConf.getOutputFieldNames());assertEquals(nullFieldNames.length, dbConf.getOutputFieldCount());actual = format.constructQuery(dbConf.getOutputTableName(), new String[dbConf.getOutputFieldCount()]);assertEquals(nullExpected, actual);}public void add(Duration duration) {add(duration.value());}
public void add(long x) {n++;sum += x;double delta = x - mean;mean += delta / n;m2 += delta * (x - mean);if (x < min) {min = x;}if (x > max) {max = x;}}
public void reset() {n = 0;sum = 0;sum = 0;min = 10000000;max = 0;mean = 0;m2 = 0;}
public int getCount() {return n;}
public long getSum() {return sum;}
public double getArithmeticMean() {return mean;}
public double getVariance() {return n > 0 ? (m2 / (n - 1)) : 0;}
public double getDeviation() {double variance = getVariance();return (variance > 0) ? Math.sqrt(variance) : 0;}
public String toString() {return String.format("%s count=%d total=%.3fs mean=%.3fs stddev=%.3fs min=%.3fs max=%.3fs", operation, n, sum / 1000.0, mean / 1000.0, getDeviation() / 1000000.0, min / 1000.0, max / 1000.0);}protected boolean combine(Object[] srcs, TupleWritable dst) {assert srcs.length == dst.size();return true;}public void testExportPoint() {NfsConfiguration config = new NfsConfiguration();MiniDFSCluster cluster = null;String exportPoint = "/myexport1";config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);// Use emphral port in case tests are running in parallelconfig.setInt("nfs3.mountd.port", 0);config.setInt("nfs3.server.port", 0);try {cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();cluster.waitActive();// Start nfsfinal Nfs3 nfsServer = new Nfs3(config);nfsServer.startServiceInternal(false);Mountd mountd = nfsServer.getMountd();RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();assertTrue(rpcMount.getExports().size() == 1);String exportInMountd = rpcMount.getExports().get(0);assertTrue(exportInMountd.equals(exportPoint));} finally {if (cluster != null) {cluster.shutdown();}}}public void testConstructor() {Credentials credential = new CredentialsNone();Verifier verifier = new VerifierNone();int rpcVersion = RpcCall.RPC_VERSION;int program = 2;int version = 3;int procedure = 4;RpcCall call = new RpcCall(0, RpcMessage.Type.RPC_CALL, rpcVersion, program, version, procedure, credential, verifier);assertEquals(0, call.getXid());assertEquals(RpcMessage.Type.RPC_CALL, call.getMessageType());assertEquals(rpcVersion, call.getRpcVersion());assertEquals(program, call.getProgram());assertEquals(version, call.getVersion());assertEquals(procedure, call.getProcedure());assertEquals(credential, call.getCredential());assertEquals(verifier, call.getVerifier());}
public void testInvalidRpcVersion() {int invalidRpcVersion = 3;new RpcCall(0, RpcMessage.Type.RPC_CALL, invalidRpcVersion, 2, 3, 4, null, null);}
public void testInvalidRpcMessageType() {// Message typ is not RpcMessage.RPC_CALLRpcMessage.Type invalidMessageType = RpcMessage.Type.RPC_REPLY;new RpcCall(0, invalidMessageType, RpcCall.RPC_VERSION, 2, 3, 4, null, null);}public static GetQueueUserAclsInfoRequest newInstance() {GetQueueUserAclsInfoRequest request = Records.newRecord(GetQueueUserAclsInfoRequest.class);return request;}public void testPrintJobQueueInfo() {JobQueueClient queueClient = new JobQueueClient();JobQueueInfo parent = new JobQueueInfo();JobQueueInfo child = new JobQueueInfo();JobQueueInfo grandChild = new JobQueueInfo();child.addChild(grandChild);parent.addChild(child);grandChild.setQueueName("GrandChildQueue");ByteArrayOutputStream bbos = new ByteArrayOutputStream();PrintWriter writer = new PrintWriter(bbos);queueClient.printJobQueueInfo(parent, writer);Assert.assertTrue("printJobQueueInfo did not print grandchild's name", bbos.toString().contains("GrandChildQueue"));}public void testNegativeStartTimes() {long elapsed = Times.elapsed(-5, 10, true);Assert.assertEquals("Elapsed time is not 0", 0, elapsed);elapsed = Times.elapsed(-5, 10, false);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);}
public void testNegativeFinishTimes() {long elapsed = Times.elapsed(5, -10, false);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);}
public void testNegativeStartandFinishTimes() {long elapsed = Times.elapsed(-5, -10, false);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);}
public void testPositiveStartandFinishTimes() {long elapsed = Times.elapsed(5, 10, true);Assert.assertEquals("Elapsed time is not 5", 5, elapsed);elapsed = Times.elapsed(5, 10, false);Assert.assertEquals("Elapsed time is not 5", 5, elapsed);}
public void testFinishTimesAheadOfStartTimes() {long elapsed = Times.elapsed(10, 5, true);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);elapsed = Times.elapsed(10, 5, false);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);// use Long.MAX_VALUE to ensure started time is after the current oneelapsed = Times.elapsed(Long.MAX_VALUE, 0, true);Assert.assertEquals("Elapsed time is not -1", -1, elapsed);}public CipherSuite getCipherSuite() {return SUITE;}
public void calculateIV(byte[] initIV, long counter, byte[] IV) {Preconditions.checkArgument(initIV.length == AES_BLOCK_SIZE);Preconditions.checkArgument(IV.length == AES_BLOCK_SIZE);System.arraycopy(initIV, 0, IV, 0, CTR_OFFSET);long l = 0;for (int i = 0; i < 8; i++) {l = ((l << 8) | (initIV[CTR_OFFSET + i] & 0xff));}l += counter;IV[CTR_OFFSET + 0] = (byte) (l >>> 56);IV[CTR_OFFSET + 1] = (byte) (l >>> 48);IV[CTR_OFFSET + 2] = (byte) (l >>> 40);IV[CTR_OFFSET + 3] = (byte) (l >>> 32);IV[CTR_OFFSET + 4] = (byte) (l >>> 24);IV[CTR_OFFSET + 5] = (byte) (l >>> 16);IV[CTR_OFFSET + 6] = (byte) (l >>> 8);IV[CTR_OFFSET + 7] = (byte) (l);}private static int getFilesCreated(AzureBlobStorageTestAccount testAccount) {return testAccount.getLatestMetricValue(WASB_FILES_CREATED, 0).intValue();}
public void testMetricsAcrossFileSystems() {AzureBlobStorageTestAccount a1, a2, a3;a1 = AzureBlobStorageTestAccount.createMock();assertEquals(0, getFilesCreated(a1));a2 = AzureBlobStorageTestAccount.createMock();assertEquals(0, getFilesCreated(a2));a1.getFileSystem().create(new Path("/foo")).close();a1.getFileSystem().create(new Path("/bar")).close();a2.getFileSystem().create(new Path("/baz")).close();assertEquals(0, getFilesCreated(a1));assertEquals(0, getFilesCreated(a2));// Causes the file system to close, which publishes metricsa1.closeFileSystem();a2.closeFileSystem();assertEquals(2, getFilesCreated(a1));assertEquals(1, getFilesCreated(a2));a3 = AzureBlobStorageTestAccount.createMock();assertEquals(0, getFilesCreated(a3));a3.closeFileSystem();assertEquals(0, getFilesCreated(a3));}
public void testMetricsSourceNames() {String name1 = NativeAzureFileSystem.newMetricsSourceName();String name2 = NativeAzureFileSystem.newMetricsSourceName();assertTrue(name1.startsWith("AzureFileSystemMetrics"));assertTrue(name2.startsWith("AzureFileSystemMetrics"));assertTrue(!name1.equals(name2));}public static void createCluster() {HDFSContract.createCluster();}
public static void teardownCluster() {HDFSContract.destroyCluster();}
protected AbstractFSContract createContract(Configuration conf) {return new HDFSContract(conf);}
public void testRenameFileBeingAppended() {try {super.testRenameFileBeingAppended();fail("Expected a FileNotFoundException");} catch (FileNotFoundException e) {ContractTestUtils.downgrade("Renaming an open file" + "still creates the old path", e);}}public SubmitApplicationResponseProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}void start() {final InetSocketAddress httpAddr = getAddress(conf);final String httpsAddrString = conf.get(DFSConfigKeys.DFS_JOURNALNODE_HTTPS_ADDRESS_KEY, DFSConfigKeys.DFS_JOURNALNODE_HTTPS_ADDRESS_DEFAULT);InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf, httpAddr, httpsAddr, "journal", DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY, DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY);httpServer = builder.build();httpServer.setAttribute(JN_ATTRIBUTE_KEY, localJournalNode);httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);httpServer.addInternalServlet("getJournal", "/getJournal", GetJournalEditServlet.class, true);httpServer.start();}
void stop() {if (httpServer != null) {try {httpServer.stop();} catch (Exception e) {throw new IOException(e);}}}
public InetSocketAddress getAddress() {InetSocketAddress addr = httpServer.getConnectorAddress(0);assert addr.getPort() != 0;return addr;}
URI getServerURI() {// for HTTPS_ONLY policy.InetSocketAddress addr = httpServer.getConnectorAddress(0);return URI.create(DFSUtil.getHttpClientScheme(conf) + "://" + NetUtils.getHostPortString(addr));}
private static InetSocketAddress getAddress(Configuration conf) {String addr = conf.get(DFSConfigKeys.DFS_JOURNALNODE_HTTP_ADDRESS_KEY, DFSConfigKeys.DFS_JOURNALNODE_HTTP_ADDRESS_DEFAULT);return NetUtils.createSocketAddr(addr, DFSConfigKeys.DFS_JOURNALNODE_HTTP_PORT_DEFAULT, DFSConfigKeys.DFS_JOURNALNODE_HTTP_ADDRESS_KEY);}
public static Journal getJournalFromContext(ServletContext context, String jid) {JournalNode jn = (JournalNode) context.getAttribute(JN_ATTRIBUTE_KEY);return jn.getOrCreateJournal(jid);}
public static Configuration getConfFromContext(ServletContext context) {return (Configuration) context.getAttribute(JspHelper.CURRENT_CONF);}public static Hash getInstance() {return _instance;}
public int hash(byte[] data, int length, int seed) {int m = 0x5bd1e995;int r = 24;int h = seed ^ length;int len_4 = length >> 2;for (int i = 0; i < len_4; i++) {int i_4 = i << 2;int k = data[i_4 + 3];k = k << 8;k = k | (data[i_4 + 2] & 0xff);k = k << 8;k = k | (data[i_4 + 1] & 0xff);k = k << 8;k = k | (data[i_4 + 0] & 0xff);k *= m;k ^= k >>> r;k *= m;h *= m;h ^= k;}// avoid calculating moduloint len_m = len_4 << 2;int left = length - len_m;if (left != 0) {if (left >= 3) {h ^= (int) data[length - 3] << 16;}if (left >= 2) {h ^= (int) data[length - 2] << 8;}if (left >= 1) {h ^= (int) data[length - 1];}h *= m;}h ^= h >>> 13;h *= m;h ^= h >>> 15;return h;}public XDR writeHeaderAndResponse(XDR out, int xid, Verifier verifier) {super.writeHeaderAndResponse(out, xid, verifier);out.writeBoolean(true);postOpAttr.serialize(out);if (getStatus() == Nfs3Status.NFS3_OK) {out.writeInt(rtmax);out.writeInt(rtpref);out.writeInt(rtmult);out.writeInt(wtmax);out.writeInt(wtpref);out.writeInt(wtmult);out.writeInt(dtpref);out.writeLongAsHyper(maxFileSize);timeDelta.serialize(out);out.writeInt(properties);}return out;}public float getCapacity() {return this.capacity;}
public float getUsedCapacity() {return this.usedCapacity;}
public float getMaxCapacity() {return this.maxCapacity;}
public String getQueueName() {return this.queueName;}
public CapacitySchedulerQueueInfoList getQueues() {return this.queues;}
protected CapacitySchedulerQueueInfoList getQueues(CSQueue parent) {CSQueue parentQueue = parent;CapacitySchedulerQueueInfoList queuesInfo = new CapacitySchedulerQueueInfoList();for (CSQueue queue : parentQueue.getChildQueues()) {CapacitySchedulerQueueInfo info;if (queue instanceof LeafQueue) {info = new CapacitySchedulerLeafQueueInfo((LeafQueue) queue);} else {info = new CapacitySchedulerQueueInfo(queue);info.queues = getQueues(queue);}queuesInfo.addToQueueInfoList(info);}return queuesInfo;}public static Connection getConnection() {Connection connection = mock(FakeConnection.class);try {Statement statement = mock(Statement.class);ResultSet results = mock(ResultSet.class);when(results.getLong(1)).thenReturn(15L);when(statement.executeQuery(any(String.class))).thenReturn(results);when(connection.createStatement()).thenReturn(statement);DatabaseMetaData metadata = mock(DatabaseMetaData.class);when(metadata.getDatabaseProductName()).thenReturn("Test");when(connection.getMetaData()).thenReturn(metadata);PreparedStatement reparedStatement0 = mock(PreparedStatement.class);when(connection.prepareStatement(anyString())).thenReturn(reparedStatement0);PreparedStatement preparedStatement = mock(PreparedStatement.class);ResultSet resultSet = mock(ResultSet.class);when(resultSet.next()).thenReturn(false);when(preparedStatement.executeQuery()).thenReturn(resultSet);when(connection.prepareStatement(anyString(), anyInt(), anyInt())).thenReturn(preparedStatement);} catch (SQLException e) {;}return connection;}
public boolean acceptsURL(String arg0) {return "testUrl".equals(arg0);}
public Connection connect(String arg0, Properties arg1) {return getConnection();}
public int getMajorVersion() {return 1;}
public int getMinorVersion() {return 1;}
public DriverPropertyInfo[] getPropertyInfo(String arg0, Properties arg1) {return null;}
public boolean jdbcCompliant() {return true;}
public Logger getParentLogger() {throw new SQLFeatureNotSupportedException();}public void reset() {numSamples = 0;a0 = s0 = 0.0;minmax.reset();}
void reset(long numSamples, double a0, double a1, double s0, double s1, MinMax minmax) {this.numSamples = numSamples;this.a0 = a0;this.a1 = a1;this.s0 = s0;this.s1 = s1;this.minmax.reset(minmax);}
public void copyTo(SampleStat other) {other.reset(numSamples, a0, a1, s0, s1, minmax);}
public SampleStat add(double x) {minmax.add(x);return add(1, x);}
public SampleStat add(long nSamples, double x) {numSamples += nSamples;if (numSamples == 1) {a0 = a1 = x;s0 = 0.0;} else {// The Welford method for numerical stabilitya1 = a0 + (x - a0) / numSamples;s1 = s0 + (x - a0) * (x - a1);a0 = a1;s0 = s1;}return this;}
public long numSamples() {return numSamples;}
public double mean() {return numSamples > 0 ? a1 : 0.0;}
public double variance() {return numSamples > 1 ? s1 / (numSamples - 1) : 0.0;}
public double stddev() {return Math.sqrt(variance());}
public double min() {return minmax.min();}
public double max() {return minmax.max();}
public void add(double value) {if (value > max)max = value;if (value < min)min = value;}
public double min() {return min;}
public double max() {return max;}
public void reset() {min = DEFAULT_MIN_VALUE;max = DEFAULT_MAX_VALUE;}
public void reset(MinMax other) {min = other.min();max = other.max();}public CheckpointWriteChannel create() {String name = namingPolicy.getNewName();Path p = new Path(name);if (p.isUriPathAbsolute()) {throw new IOException("Checkpoint cannot be an absolute path");}return createInternal(new Path(base, p));}
CheckpointWriteChannel createInternal(Path name) {//create a temp file, fail if file existsreturn new FSCheckpointWriteChannel(name, fs.create(tmpfile(name), replication));}
public int write(ByteBuffer b) {return out.write(b);}
public Path getDestination() {return finalDst;}
public void close() {isOpen = false;out.close();}
public boolean isOpen() {return isOpen;}
public CheckpointReadChannel open(CheckpointID id) {if (!(id instanceof FSCheckpointID)) {throw new IllegalArgumentException("Mismatched checkpoint type: " + id.getClass());}return new FSCheckpointReadChannel(fs.open(((FSCheckpointID) id).getPath()));}
public int read(ByteBuffer bb) {return in.read(bb);}
public void close() {isOpen = false;in.close();}
public boolean isOpen() {return isOpen;}
public CheckpointID commit(CheckpointWriteChannel ch) {if (ch.isOpen()) {ch.close();}FSCheckpointWriteChannel hch = (FSCheckpointWriteChannel) ch;Path dst = hch.getDestination();if (!fs.rename(tmpfile(dst), dst)) {// attempt to clean upabort(ch);throw new IOException("Failed to promote checkpoint" + tmpfile(dst) + " -> " + dst);}return new FSCheckpointID(hch.getDestination());}
public void abort(CheckpointWriteChannel ch) {if (ch.isOpen()) {ch.close();}FSCheckpointWriteChannel hch = (FSCheckpointWriteChannel) ch;Path tmp = tmpfile(hch.getDestination());try {if (!fs.delete(tmp, false)) {throw new IOException("Failed to delete checkpoint during abort");}} catch (FileNotFoundException e) {}}
public boolean delete(CheckpointID id) {if (!(id instanceof FSCheckpointID)) {throw new IllegalArgumentException("Mismatched checkpoint type: " + id.getClass());}Path tmp = ((FSCheckpointID) id).getPath();try {return fs.delete(tmp, false);} catch (FileNotFoundException e) {}return true;}
static final Path tmpfile(Path p) {return new Path(p.getParent(), p.getName() + ".tmp");}public void setUp() {super.setUp();//delete the test directoryPath test = path("/test");fs.delete(test, true);mkdirs(test);}
private void createTestSubdirs() {testDirs = new Path[] { path("/test/hadoop/a"), path("/test/hadoop/b"), path("/test/hadoop/c/1") };assertPathDoesNotExist("test directory setup", testDirs[0]);for (Path path : testDirs) {mkdirs(path);}}
public static void assertListFilesFinds(FileSystem fs, Path dir, Path subdir, boolean recursive) {RemoteIterator<LocatedFileStatus> iterator = fs.listFiles(dir, recursive);boolean found = false;int entries = 0;StringBuilder builder = new StringBuilder();while (iterator.hasNext()) {LocatedFileStatus next = iterator.next();entries++;builder.append(next.toString()).append('\n');if (next.getPath().equals(subdir)) {found = true;}}assertTrue("Path " + subdir + " not found in directory " + dir + " : " + " entries=" + entries + " content" + builder.toString(), found);}
public void testListFilesRootDir() {Path dir = path("/");Path child = new Path(dir, "test");fs.delete(child, true);SwiftTestUtils.writeTextFile(fs, child, "text", false);assertListFilesFinds(fs, dir, child, false);}
public void testListFilesSubDir() {createTestSubdirs();Path dir = path("/test/subdir");Path child = new Path(dir, "text.txt");SwiftTestUtils.writeTextFile(fs, child, "text", false);assertListFilesFinds(fs, dir, child, false);}
public void testListFilesRecursive() {createTestSubdirs();Path dir = path("/test/recursive");Path child = new Path(dir, "hadoop/a/a.txt");SwiftTestUtils.writeTextFile(fs, child, "text", false);assertListFilesFinds(fs, dir, child, true);}protected final void initialize(int maxSize) {size = 0;int heapSize = maxSize + 1;heap = (T[]) new Object[heapSize];this.maxSize = maxSize;}
public final void put(T element) {size++;heap[size] = element;upHeap();}
public boolean insert(T element) {if (size < maxSize) {put(element);return true;} else if (size > 0 && !lessThan(element, top())) {heap[1] = element;adjustTop();return true;} elsereturn false;}
public final T top() {if (size > 0)return heap[1];elsereturn null;}
public final T pop() {if (size > 0) {// save first valueT result = heap[1];// move last to firstheap[1] = heap[size];// permit GC of objectsheap[size] = null;size--;// adjust heapdownHeap();return result;} elsereturn null;}
public final void adjustTop() {downHeap();}
public final int size() {return size;}
public final void clear() {for (int i = 0; i <= size; i++) heap[i] = null;size = 0;}
private final void upHeap() {int i = size;// save bottom nodeT node = heap[i];int j = i >>> 1;while (j > 0 && lessThan(node, heap[j])) {// shift parents downheap[i] = heap[j];i = j;j = j >>> 1;}// install saved nodeheap[i] = node;}
private final void downHeap() {int i = 1;// save top nodeT node = heap[i];// find smaller childint j = i << 1;int k = j + 1;if (k <= size && lessThan(heap[k], heap[j])) {j = k;}while (j <= size && lessThan(heap[j], node)) {// shift up childheap[i] = heap[j];i = j;j = i << 1;k = j + 1;if (k <= size && lessThan(heap[k], heap[j])) {j = k;}}// install saved nodeheap[i] = node;}protected void serviceInit(Configuration conf) {Configuration config = new YarnConfiguration(conf);try {doSecureLogin(conf);} catch (IOException ie) {throw new YarnRuntimeException("Proxy Server Failed to login", ie);}proxy = new WebAppProxy();addService(proxy);super.serviceInit(config);}
protected void doSecureLogin(Configuration conf) {InetSocketAddress socAddr = getBindAddress(conf);SecurityUtil.login(conf, YarnConfiguration.PROXY_KEYTAB, YarnConfiguration.PROXY_PRINCIPAL, socAddr.getHostName());}
public static InetSocketAddress getBindAddress(Configuration conf) {return conf.getSocketAddr(YarnConfiguration.PROXY_ADDRESS, YarnConfiguration.DEFAULT_PROXY_ADDRESS, YarnConfiguration.DEFAULT_PROXY_PORT);}
public static void main(String[] args) {Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());StringUtils.startupShutdownMessage(WebAppProxyServer.class, args, LOG);try {YarnConfiguration configuration = new YarnConfiguration();WebAppProxyServer proxyServer = startServer(configuration);proxyServer.proxy.join();} catch (Throwable t) {LOG.fatal("Error starting Proxy server", t);System.exit(-1);}}
protected static WebAppProxyServer startServer(Configuration configuration) {WebAppProxyServer proxy = new WebAppProxyServer();ShutdownHookManager.get().addShutdownHook(new CompositeServiceShutdownHook(proxy), SHUTDOWN_HOOK_PRIORITY);proxy.init(configuration);proxy.start();return proxy;}private void printf(String format, Object... args) {String msg = String.format(format, args);System.out.printf(msg + "\n");LOG.info(msg);}
public void testWhichLog4JPropsFile() {locateResource("log4j.properties");}
public void testWhichLog4JXMLFile() {locateResource("log4j.XML");}
public void testCommonsLoggingProps() {locateResource("commons-logging.properties");}
private void locateResource(String resource) {URL url = this.getClass().getClassLoader().getResource(resource);if (url != null) {printf("resource %s is at %s", resource, url);} else {printf("resource %s is not on the classpath", resource);}}public void setUp() {super.setUp();conf.setClass(PolicyProvider.POLICY_PROVIDER_CONFIG, HDFSPolicyProvider.class, PolicyProvider.class);conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, 1);tmpDir = new File(System.getProperty("test.build.data", "target"), UUID.randomUUID().toString()).getAbsoluteFile();conf.set(KeyProviderFactory.KEY_PROVIDER_PATH, JavaKeyStoreProvider.SCHEME_NAME + "://file" + tmpDir + "/test.jks");dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();dfsCluster.waitClusterUp();createAKey("mykey", conf);namenode = conf.get(DFSConfigKeys.FS_DEFAULT_NAME_KEY, "file:///");username = System.getProperty("user.name");fs = dfsCluster.getFileSystem();assertTrue("Not an HDFS: " + fs.getUri(), fs instanceof DistributedFileSystem);}
public void tearDown() {if (fs != null) {fs.close();}if (dfsCluster != null) {dfsCluster.shutdown();}Thread.sleep(2000);super.tearDown();}
private void createAKey(String keyName, Configuration conf) {final KeyProvider provider = dfsCluster.getNameNode().getNamesystem().getProvider();final KeyProvider.Options options = KeyProvider.options(conf);provider.createKey(keyName, options);provider.flush();}
protected String getTestFile() {return "testCryptoConf.xml";}
protected String expandCommand(final String cmd) {String expCmd = cmd;expCmd = expCmd.replaceAll("NAMENODE", namenode);expCmd = expCmd.replaceAll("#LF#", System.getProperty("line.separator"));expCmd = super.expandCommand(expCmd);return expCmd;}
protected TestConfigFileParser getConfigParser() {return new TestConfigFileParserCryptoAdmin();}
public void endElement(String uri, String localName, String qName) {if (qName.equals("crypto-admin-command")) {if (testCommands != null) {testCommands.add(new CLITestCmdCryptoAdmin(charString, new CLICommandCryptoAdmin()));} else if (cleanupCommands != null) {cleanupCommands.add(new CLITestCmdCryptoAdmin(charString, new CLICommandCryptoAdmin()));}} else {super.endElement(uri, localName, qName);}}
public CommandExecutor getExecutor(String tag) {if (getType() instanceof CLICommandCryptoAdmin) {return new CryptoAdminCmdExecutor(tag, new CryptoAdmin(conf));}return super.getExecutor(tag);}
protected Result execute(CLICommand cmd) {return cmd.getExecutor(namenode).executeCommand(cmd.getCmd());}
public void testAll() {super.testAll();}public void setUp() {conf = new Configuration();cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION).build();cluster.waitActive();fsn = cluster.getNamesystem();hdfs = cluster.getFileSystem();hdfs.mkdirs(dir);}
public void tearDown() {if (cluster != null) {cluster.shutdown();}}
public void testListSnapshots() {final Path snapshotsPath = new Path(dir, ".snapshot");FileStatus[] stats = null;// special case: snapshots of rootstats = hdfs.listStatus(new Path("/.snapshot"));// should be 0 since root's snapshot quota is 0assertEquals(0, stats.length);// list before set dir as snapshottabletry {stats = hdfs.listStatus(snapshotsPath);fail("expect SnapshotException");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Directory is not a snapshottable directory: " + dir.toString(), e);}// list before creating snapshotshdfs.allowSnapshot(dir);stats = hdfs.listStatus(snapshotsPath);assertEquals(0, stats.length);// list while creating snapshotsfinal int snapshotNum = 5;for (int sNum = 0; sNum < snapshotNum; sNum++) {hdfs.createSnapshot(dir, "s_" + sNum);stats = hdfs.listStatus(snapshotsPath);assertEquals(sNum + 1, stats.length);for (int i = 0; i <= sNum; i++) {assertEquals("s_" + i, stats[i].getPath().getName());}}// list while deleting snapshotsfor (int sNum = snapshotNum - 1; sNum > 0; sNum--) {hdfs.deleteSnapshot(dir, "s_" + sNum);stats = hdfs.listStatus(snapshotsPath);assertEquals(sNum, stats.length);for (int i = 0; i < sNum; i++) {assertEquals("s_" + i, stats[i].getPath().getName());}}// remove the last snapshothdfs.deleteSnapshot(dir, "s_0");stats = hdfs.listStatus(snapshotsPath);assertEquals(0, stats.length);}void init() {try {Stat isCurrentInprogressNodeExists = zkc.exists(currentInprogressNode, false);if (isCurrentInprogressNodeExists == null) {try {zkc.create(currentInprogressNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);} catch (NodeExistsException e) {if (LOG.isDebugEnabled()) {LOG.debug(currentInprogressNode + " already created by other process.", e);}}}} catch (KeeperException e) {throw new IOException("Exception accessing Zookeeper", e);} catch (InterruptedException ie) {throw new IOException("Interrupted accessing Zookeeper", ie);}}
void update(String path) {CurrentInprogressProto.Builder builder = CurrentInprogressProto.newBuilder();builder.setPath(path).setHostname(hostName);String content = TextFormat.printToString(builder.build());try {zkc.setData(this.currentInprogressNode, content.getBytes(UTF_8), this.versionNumberForPermission);} catch (KeeperException e) {throw new IOException("Exception when setting the data " + "[" + content + "] to CurrentInprogress. ", e);} catch (InterruptedException e) {throw new IOException("Interrupted while setting the data " + "[" + content + "] to CurrentInprogress", e);}if (LOG.isDebugEnabled()) {LOG.debug("Updated data[" + content + "] to CurrentInprogress");}}
String read() {Stat stat = new Stat();byte[] data = null;try {data = zkc.getData(this.currentInprogressNode, false, stat);} catch (KeeperException e) {throw new IOException("Exception while reading the data from " + currentInprogressNode, e);} catch (InterruptedException e) {throw new IOException("Interrupted while reading data from " + currentInprogressNode, e);}this.versionNumberForPermission = stat.getVersion();if (data != null) {CurrentInprogressProto.Builder builder = CurrentInprogressProto.newBuilder();TextFormat.merge(new String(data, UTF_8), builder);if (!builder.isInitialized()) {throw new IOException("Invalid/Incomplete data in znode");}return builder.build().getPath();} else {LOG.debug("No data available in CurrentInprogress");}return null;}
void clear() {try {zkc.setData(this.currentInprogressNode, null, versionNumberForPermission);} catch (KeeperException e) {throw new IOException("Exception when setting the data to CurrentInprogress node", e);} catch (InterruptedException e) {throw new IOException("Interrupted when setting the data to CurrentInprogress node", e);}LOG.debug("Cleared the data from CurrentInprogress");}public static boolean isKdcRunning() {String startKdc = System.getProperty("startKdc");if (startKdc == null || !startKdc.equals("true")) {return false;}return true;}
public void testKdcRunning() {//Tests are skipped if KDC is not runningAssume.assumeTrue(isKdcRunning());}
public void testLogin() {String nn1keyTabFilepath = System.getProperty("kdc.resource.dir") + "/keytabs/nn1.keytab";String user1keyTabFilepath = System.getProperty("kdc.resource.dir") + "/keytabs/user1.keytab";Configuration conf = new Configuration();SecurityUtil.setAuthenticationMethod(AuthenticationMethod.KERBEROS, conf);UserGroupInformation.setConfiguration(conf);UserGroupInformation ugiNn = UserGroupInformation.loginUserFromKeytabAndReturnUGI("nn1/localhost@EXAMPLE.COM", nn1keyTabFilepath);UserGroupInformation ugiDn = UserGroupInformation.loginUserFromKeytabAndReturnUGI("user1@EXAMPLE.COM", user1keyTabFilepath);Assert.assertEquals(AuthenticationMethod.KERBEROS, ugiNn.getAuthenticationMethod());Assert.assertEquals(AuthenticationMethod.KERBEROS, ugiDn.getAuthenticationMethod());try {UserGroupInformation.loginUserFromKeytabAndReturnUGI("bogus@EXAMPLE.COM", nn1keyTabFilepath);Assert.fail("Login should have failed");} catch (Exception ex) {ex.printStackTrace();}}
public void testGetUGIFromKerberosSubject() {String user1keyTabFilepath = System.getProperty("kdc.resource.dir") + "/keytabs/user1.keytab";UserGroupInformation ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI("user1@EXAMPLE.COM", user1keyTabFilepath);Set<KerberosPrincipal> principals = ugi.getSubject().getPrincipals(KerberosPrincipal.class);if (principals.isEmpty()) {Assert.fail("There should be a kerberos principal in the subject.");} else {UserGroupInformation ugi2 = UserGroupInformation.getUGIFromSubject(ugi.getSubject());if (ugi2 != null) {ugi2.doAs(new PrivilegedAction<Object>() {
@Overridepublic Object run() {try {UserGroupInformation ugi3 = UserGroupInformation.getCurrentUser();String doAsUserName = ugi3.getUserName();assertEquals(doAsUserName, "user1@EXAMPLE.COM");System.out.println("DO AS USERNAME: " + doAsUserName);} catch (IOException e) {e.printStackTrace();}return null;}});}}}
public Object run() {try {UserGroupInformation ugi3 = UserGroupInformation.getCurrentUser();String doAsUserName = ugi3.getUserName();assertEquals(doAsUserName, "user1@EXAMPLE.COM");System.out.println("DO AS USERNAME: " + doAsUserName);} catch (IOException e) {e.printStackTrace();}return null;}public boolean equals(Object o) {return super.equals(o);}
public int hashCode() {return super.hashCode();}
public long getLen() {return myFs.getLen();}
public boolean isFile() {return myFs.isFile();}
public boolean isDirectory() {return myFs.isDirectory();}
public boolean isDir() {return myFs.isDirectory();}
public boolean isSymlink() {return myFs.isSymlink();}
public long getBlockSize() {return myFs.getBlockSize();}
public short getReplication() {return myFs.getReplication();}
public long getModificationTime() {return myFs.getModificationTime();}
public long getAccessTime() {return myFs.getAccessTime();}
public FsPermission getPermission() {return myFs.getPermission();}
public String getOwner() {return myFs.getOwner();}
public String getGroup() {return myFs.getGroup();}
public Path getPath() {return modifiedPath;}
public void setPath(final Path p) {modifiedPath = p;}
public Path getSymlink() {return myFs.getSymlink();}protected FileContextTestHelper createFileContextHelper() {return new FileContextTestHelper("/tmp/TestFcHdfsCreateMkdir");}
public static void clusterSetupAtBegining() {Configuration conf = new HdfsConfiguration();cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();fc = FileContext.getFileContext(cluster.getURI(0), conf);defaultWorkingDirectory = fc.makeQualified(new Path("/user/" + UserGroupInformation.getCurrentUser().getShortUserName()));fc.mkdir(defaultWorkingDirectory, FileContext.DEFAULT_PERM, true);}
public static void ClusterShutdownAtEnd() {cluster.shutdown();}
public void setUp() {super.setUp();}
public void tearDown() {super.tearDown();}public void testUnknownExceptionUnwrapping() {Class<? extends Throwable> exception = YarnException.class;String className = "UnknownException.class";verifyRemoteExceptionUnwrapping(exception, className);}
public void testRemoteIOExceptionUnwrapping() {Class<? extends Throwable> exception = IOException.class;verifyRemoteExceptionUnwrapping(exception, exception.getName());}
public void testRemoteIOExceptionDerivativeUnwrapping() {// Test IOException sub-classClass<? extends Throwable> exception = FileNotFoundException.class;verifyRemoteExceptionUnwrapping(exception, exception.getName());}
public void testRemoteYarnExceptionUnwrapping() {Class<? extends Throwable> exception = YarnException.class;verifyRemoteExceptionUnwrapping(exception, exception.getName());}
public void testRemoteYarnExceptionDerivativeUnwrapping() {Class<? extends Throwable> exception = YarnTestException.class;verifyRemoteExceptionUnwrapping(exception, exception.getName());}
public void testRemoteRuntimeExceptionUnwrapping() {Class<? extends Throwable> exception = NullPointerException.class;verifyRemoteExceptionUnwrapping(exception, exception.getName());}
public void testUnexpectedRemoteExceptionUnwrapping() {// Non IOException, YarnException thrown by the remote side.Class<? extends Throwable> exception = Exception.class;verifyRemoteExceptionUnwrapping(RemoteException.class, exception.getName());}
public void testRemoteYarnExceptionWithoutStringConstructor() {// Derivatives of YarnException should always define a string constructor.Class<? extends Throwable> exception = YarnTestExceptionNoConstructor.class;verifyRemoteExceptionUnwrapping(RemoteException.class, exception.getName());}
public void testRPCServiceExceptionUnwrapping() {String message = "ServiceExceptionMessage";ServiceException se = new ServiceException(message);Throwable t = null;try {RPCUtil.unwrapAndThrowException(se);} catch (Throwable thrown) {t = thrown;}Assert.assertTrue(IOException.class.isInstance(t));Assert.assertTrue(t.getMessage().contains(message));}
public void testRPCIOExceptionUnwrapping() {String message = "DirectIOExceptionMessage";IOException ioException = new FileNotFoundException(message);ServiceException se = new ServiceException(ioException);Throwable t = null;try {RPCUtil.unwrapAndThrowException(se);} catch (Throwable thrown) {t = thrown;}Assert.assertTrue(FileNotFoundException.class.isInstance(t));Assert.assertTrue(t.getMessage().contains(message));}
public void testRPCRuntimeExceptionUnwrapping() {String message = "RPCRuntimeExceptionUnwrapping";RuntimeException re = new NullPointerException(message);ServiceException se = new ServiceException(re);Throwable t = null;try {RPCUtil.unwrapAndThrowException(se);} catch (Throwable thrown) {t = thrown;}Assert.assertTrue(NullPointerException.class.isInstance(t));Assert.assertTrue(t.getMessage().contains(message));}
private void verifyRemoteExceptionUnwrapping(Class<? extends Throwable> expectedLocalException, String realExceptionClassName) {String message = realExceptionClassName + "Message";RemoteException re = new RemoteException(realExceptionClassName, message);ServiceException se = new ServiceException(re);Throwable t = null;try {RPCUtil.unwrapAndThrowException(se);} catch (Throwable thrown) {t = thrown;}Assert.assertTrue("Expected exception [" + expectedLocalException + "] but found " + t, expectedLocalException.isInstance(t));Assert.assertTrue("Expected message [" + message + "] but found " + t.getMessage(), t.getMessage().contains(message));}public boolean serialize(XDR out) {out.writeInt(handle.length);out.writeFixedOpaque(handle);return true;}
private long bytesToLong(byte[] data) {ByteBuffer buffer = ByteBuffer.allocate(8);for (int i = 0; i < 8; i++) {buffer.put(data[i]);}// need flipbuffer.flip();return buffer.getLong();}
public boolean deserialize(XDR xdr) {if (!XDR.verifyLength(xdr, 32)) {return false;}int size = xdr.readInt();handle = xdr.readFixedOpaque(size);fileId = bytesToLong(handle);return true;}
private static String hex(byte b) {StringBuilder strBuilder = new StringBuilder();strBuilder.append(HEXES.charAt((b & 0xF0) >> 4)).append(HEXES.charAt((b & 0x0F)));return strBuilder.toString();}
public long getFileId() {return fileId;}
public byte[] getContent() {return handle.clone();}
public String toString() {StringBuilder s = new StringBuilder();for (int i = 0; i < handle.length; i++) {s.append(hex(handle[i]));}return s.toString();}
public boolean equals(Object o) {if (this == o) {return true;}if (!(o instanceof FileHandle)) {return false;}FileHandle h = (FileHandle) o;return Arrays.equals(handle, h.handle);}
public int hashCode() {return Arrays.hashCode(handle);}public Object getDatum() {return datum;}
public void setDatum(Object datum) {this.datum = (TaskAttemptStarted) datum;}
public TaskID getTaskId() {return TaskID.forName(datum.taskid.toString());}
public String getTrackerName() {return datum.trackerName.toString();}
public long getStartTime() {return datum.startTime;}
public TaskType getTaskType() {return TaskType.valueOf(datum.taskType.toString());}
public int getHttpPort() {return datum.httpPort;}
public int getShufflePort() {return datum.shufflePort;}
public TaskAttemptID getTaskAttemptId() {return TaskAttemptID.forName(datum.attemptId.toString());}
public EventType getEventType() {// attempt-type can only be map/reduce.return getTaskId().getTaskType() == TaskType.MAP ? EventType.MAP_ATTEMPT_STARTED : EventType.REDUCE_ATTEMPT_STARTED;}
public ContainerId getContainerId() {return ConverterUtils.toContainerId(datum.containerId.toString());}
public String getLocality() {if (datum.locality != null) {return datum.locality.toString();}return null;}
public String getAvataar() {if (datum.avataar != null) {return datum.avataar.toString();}return null;}public String getDiagnosticMessage() {return diagnosticMesage;}public GetBlocksResponseProto getBlocks(RpcController unused, GetBlocksRequestProto request) {DatanodeInfo dnInfo = new DatanodeInfo(PBHelper.convert(request.getDatanode()));BlocksWithLocations blocks;try {blocks = impl.getBlocks(dnInfo, request.getSize());} catch (IOException e) {throw new ServiceException(e);}return GetBlocksResponseProto.newBuilder().setBlocks(PBHelper.convert(blocks)).build();}
public GetBlockKeysResponseProto getBlockKeys(RpcController unused, GetBlockKeysRequestProto request) {ExportedBlockKeys keys;try {keys = impl.getBlockKeys();} catch (IOException e) {throw new ServiceException(e);}GetBlockKeysResponseProto.Builder builder = GetBlockKeysResponseProto.newBuilder();if (keys != null) {builder.setKeys(PBHelper.convert(keys));}return builder.build();}
public GetTransactionIdResponseProto getTransactionId(RpcController unused, GetTransactionIdRequestProto request) {long txid;try {txid = impl.getTransactionID();} catch (IOException e) {throw new ServiceException(e);}return GetTransactionIdResponseProto.newBuilder().setTxId(txid).build();}
public GetMostRecentCheckpointTxIdResponseProto getMostRecentCheckpointTxId(RpcController unused, GetMostRecentCheckpointTxIdRequestProto request) {long txid;try {txid = impl.getMostRecentCheckpointTxId();} catch (IOException e) {throw new ServiceException(e);}return GetMostRecentCheckpointTxIdResponseProto.newBuilder().setTxId(txid).build();}
public RollEditLogResponseProto rollEditLog(RpcController unused, RollEditLogRequestProto request) {CheckpointSignature signature;try {signature = impl.rollEditLog();} catch (IOException e) {throw new ServiceException(e);}return RollEditLogResponseProto.newBuilder().setSignature(PBHelper.convert(signature)).build();}
public ErrorReportResponseProto errorReport(RpcController unused, ErrorReportRequestProto request) {try {impl.errorReport(PBHelper.convert(request.getRegistration()), request.getErrorCode(), request.getMsg());} catch (IOException e) {throw new ServiceException(e);}return VOID_ERROR_REPORT_RESPONSE;}
public RegisterResponseProto registerSubordinateNamenode(RpcController unused, RegisterRequestProto request) {NamenodeRegistration reg;try {reg = impl.registerSubordinateNamenode(PBHelper.convert(request.getRegistration()));} catch (IOException e) {throw new ServiceException(e);}return RegisterResponseProto.newBuilder().setRegistration(PBHelper.convert(reg)).build();}
public StartCheckpointResponseProto startCheckpoint(RpcController unused, StartCheckpointRequestProto request) {NamenodeCommand cmd;try {cmd = impl.startCheckpoint(PBHelper.convert(request.getRegistration()));} catch (IOException e) {throw new ServiceException(e);}return StartCheckpointResponseProto.newBuilder().setCommand(PBHelper.convert(cmd)).build();}
public EndCheckpointResponseProto endCheckpoint(RpcController unused, EndCheckpointRequestProto request) {try {impl.endCheckpoint(PBHelper.convert(request.getRegistration()), PBHelper.convert(request.getSignature()));} catch (IOException e) {throw new ServiceException(e);}return VOID_END_CHECKPOINT_RESPONSE;}
public GetEditLogManifestResponseProto getEditLogManifest(RpcController unused, GetEditLogManifestRequestProto request) {RemoteEditLogManifest manifest;try {manifest = impl.getEditLogManifest(request.getSinceTxId());} catch (IOException e) {throw new ServiceException(e);}return GetEditLogManifestResponseProto.newBuilder().setManifest(PBHelper.convert(manifest)).build();}
public VersionResponseProto versionRequest(RpcController controller, VersionRequestProto request) {NamespaceInfo info;try {info = impl.versionRequest();} catch (IOException e) {throw new ServiceException(e);}return VersionResponseProto.newBuilder().setInfo(PBHelper.convert(info)).build();}public static void logSuccess(String user, String operation, String target) {if (LOG.isInfoEnabled()) {LOG.info(createSuccessLog(user, operation, target));}}
static String createSuccessLog(String user, String operation, String target) {StringBuilder b = new StringBuilder();start(Keys.USER, user, b);addRemoteIP(b);add(Keys.OPERATION, operation, b);add(Keys.TARGET, target, b);add(Keys.RESULT, AuditConstants.SUCCESS, b);return b.toString();}
static void addRemoteIP(StringBuilder b) {InetAddress ip = Server.getRemoteIp();// ip address can be null for testcasesif (ip != null) {add(Keys.IP, ip.getHostAddress(), b);}}
static void add(Keys key, String value, StringBuilder b) {b.append(AuditConstants.PAIR_SEPARATOR).append(key.name()).append(AuditConstants.KEY_VAL_SEPARATOR).append(value);}
static void start(Keys key, String value, StringBuilder b) {b.append(key.name()).append(AuditConstants.KEY_VAL_SEPARATOR).append(value);}
public static void logFailure(String user, String operation, String perm, String target, String description) {if (LOG.isWarnEnabled()) {LOG.warn(createFailureLog(user, operation, perm, target, description));}}
static String createFailureLog(String user, String operation, String perm, String target, String description) {StringBuilder b = new StringBuilder();start(Keys.USER, user, b);addRemoteIP(b);add(Keys.OPERATION, operation, b);add(Keys.TARGET, target, b);add(Keys.RESULT, AuditConstants.FAILURE, b);add(Keys.DESCRIPTION, description, b);add(Keys.PERMISSIONS, perm, b);return b.toString();}public ReadableByteChannel getInputStreamChannel() {return channel;}
public void setReadTimeout(int timeoutMs) {socket.setAttribute(DomainSocket.RECEIVE_TIMEOUT, timeoutMs);}
public int getReceiveBufferSize() {return socket.getAttribute(DomainSocket.RECEIVE_BUFFER_SIZE);}
public boolean getTcpNoDelay() {/* No TCP, no TCP_NODELAY. */return false;}
public void setWriteTimeout(int timeoutMs) {socket.setAttribute(DomainSocket.SEND_TIMEOUT, timeoutMs);}
public boolean isClosed() {return !socket.isOpen();}
public void close() {socket.close();}
public String getRemoteAddressString() {return "unix:" + socket.getPath();}
public String getLocalAddressString() {return "<local>";}
public InputStream getInputStream() {return in;}
public OutputStream getOutputStream() {return out;}
public boolean isLocal() {/* UNIX domain sockets can only be used for local communication. */return true;}
public String toString() {return "DomainPeer(" + getRemoteAddressString() + ")";}
public DomainSocket getDomainSocket() {return socket;}
public boolean hasSecureChannel() {//return true;}protected AbstractFSContract createContract(Configuration conf) {return new SwiftContract(conf);}
public void testOpenReadDir() {ContractTestUtils.skip("Skipping object-store quirk");}
public void testOpenReadDirWithChild() {ContractTestUtils.skip("Skipping object-store quirk");}protected void createInput() {DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));for (int i = 0; i < 10000; ++i) {out.write(input.getBytes("UTF-8"));}out.close();}
protected String[] genArgs() {return new String[] { "-input", INPUT_FILE.getAbsolutePath(), "-output", OUTPUT_DIR.getAbsolutePath(), "-mapper", map, "-reducer", "org.apache.hadoop.mapred.lib.IdentityReducer", "-numReduceTasks", "0", "-jobconf", "mapreduce.task.files.preserve.failedtasks=true", "-jobconf", "stream.tmpdir=" + System.getProperty("test.build.data", "/tmp") };}
public void testUnconsumedInput() {String outFileName = "part-00000";File outFile = null;try {try {FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());} catch (Exception e) {}createInput();// setup config to ignore unconsumed inputConfiguration conf = new Configuration();conf.set("stream.minRecWrittenToEnableSkip_", "0");job = new StreamJob();job.setConf(conf);int exitCode = job.run(genArgs());assertEquals("Job failed", 0, exitCode);outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();String output = StreamUtil.slurp(outFile);assertEquals("Output was truncated", EXPECTED_OUTPUT_SIZE, StringUtils.countMatches(output, "\t"));} finally {INPUT_FILE.delete();FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());}}public void setUp() {conf = new Configuration();clientProtocol = mock(ClientProtocol.class);Cluster cluster = mock(Cluster.class);when(cluster.getConf()).thenReturn(conf);when(cluster.getClient()).thenReturn(clientProtocol);JobStatus jobStatus = new JobStatus(new JobID("job_000", 1), 0f, 0f, 0f, 0f, State.RUNNING, JobPriority.HIGH, "tmp-user", "tmp-jobname", "tmp-jobfile", "tmp-url");job = Job.getInstance(cluster, jobStatus, conf);job = spy(job);}
public void testJobMonitorAndPrint() {JobStatus jobStatus_1 = new JobStatus(new JobID("job_000", 1), 1f, 0.1f, 0.1f, 0f, State.RUNNING, JobPriority.HIGH, "tmp-user", "tmp-jobname", "tmp-queue", "tmp-jobfile", "tmp-url", true);JobStatus jobStatus_2 = new JobStatus(new JobID("job_000", 1), 1f, 1f, 1f, 1f, State.SUCCEEDED, JobPriority.HIGH, "tmp-user", "tmp-jobname", "tmp-queue", "tmp-jobfile", "tmp-url", true);doAnswer(new Answer<TaskCompletionEvent[]>() {
@Overridepublic TaskCompletionEvent[] answer(InvocationOnMock invocation) throws Throwable {return new TaskCompletionEvent[0];}}).when(job).getTaskCompletionEvents(anyInt(), anyInt());doReturn(new TaskReport[5]).when(job).getTaskReports(isA(TaskType.class));when(clientProtocol.getJobStatus(any(JobID.class))).thenReturn(jobStatus_1, jobStatus_2);// setup the logger to capture all logsLayout layout = Logger.getRootLogger().getAppender("stdout").getLayout();ByteArrayOutputStream os = new ByteArrayOutputStream();WriterAppender appender = new WriterAppender(layout, os);appender.setThreshold(Level.ALL);Logger qlogger = Logger.getLogger(Job.class);qlogger.addAppender(appender);job.monitorAndPrintJob();qlogger.removeAppender(appender);LineNumberReader r = new LineNumberReader(new StringReader(os.toString()));String line;boolean foundHundred = false;boolean foundComplete = false;boolean foundUber = false;String uberModeMatch = "uber mode : true";String progressMatch = "map 100% reduce 100%";String completionMatch = "completed successfully";while ((line = r.readLine()) != null) {if (line.contains(uberModeMatch)) {foundUber = true;}foundHundred = line.contains(progressMatch);if (foundHundred)break;}line = r.readLine();foundComplete = line.contains(completionMatch);assertTrue(foundUber);assertTrue(foundHundred);assertTrue(foundComplete);System.out.println("The output of job.toString() is : \n" + job.toString());assertTrue(job.toString().contains("Number of maps: 5\n"));assertTrue(job.toString().contains("Number of reduces: 5\n"));}
public TaskCompletionEvent[] answer(InvocationOnMock invocation) {return new TaskCompletionEvent[0];}public synchronized LocalResourceProto getProto() {mergeLocalToBuilder();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private synchronized void mergeLocalToBuilder() {LocalResourceProtoOrBuilder l = viaProto ? proto : builder;if (this.url != null && !(l.getResource().equals(((URLPBImpl) url).getProto()))) {maybeInitBuilder();l = builder;builder.setResource(convertToProtoFormat(this.url));}}
private synchronized void maybeInitBuilder() {if (viaProto || builder == null) {builder = LocalResourceProto.newBuilder(proto);}viaProto = false;}
public synchronized long getSize() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;return (p.getSize());}
public synchronized void setSize(long size) {maybeInitBuilder();builder.setSize((size));}
public synchronized long getTimestamp() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;return (p.getTimestamp());}
public synchronized void setTimestamp(long timestamp) {maybeInitBuilder();builder.setTimestamp((timestamp));}
public synchronized LocalResourceType getType() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasType()) {return null;}return convertFromProtoFormat(p.getType());}
public synchronized void setType(LocalResourceType type) {maybeInitBuilder();if (type == null) {builder.clearType();return;}builder.setType(convertToProtoFormat(type));}
public synchronized URL getResource() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;if (this.url != null) {return this.url;}if (!p.hasResource()) {return null;}this.url = convertFromProtoFormat(p.getResource());return this.url;}
public synchronized void setResource(URL resource) {maybeInitBuilder();if (resource == null)builder.clearResource();this.url = resource;}
public synchronized LocalResourceVisibility getVisibility() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasVisibility()) {return null;}return convertFromProtoFormat(p.getVisibility());}
public synchronized void setVisibility(LocalResourceVisibility visibility) {maybeInitBuilder();if (visibility == null) {builder.clearVisibility();return;}builder.setVisibility(convertToProtoFormat(visibility));}
public synchronized String getPattern() {LocalResourceProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasPattern()) {return null;}return p.getPattern();}
public synchronized void setPattern(String pattern) {maybeInitBuilder();if (pattern == null) {builder.clearPattern();return;}builder.setPattern(pattern);}
private LocalResourceTypeProto convertToProtoFormat(LocalResourceType e) {return ProtoUtils.convertToProtoFormat(e);}
private LocalResourceType convertFromProtoFormat(LocalResourceTypeProto e) {return ProtoUtils.convertFromProtoFormat(e);}
private URLPBImpl convertFromProtoFormat(URLProto p) {return new URLPBImpl(p);}
private URLProto convertToProtoFormat(URL t) {return ((URLPBImpl) t).getProto();}
private LocalResourceVisibilityProto convertToProtoFormat(LocalResourceVisibility e) {return ProtoUtils.convertToProtoFormat(e);}
private LocalResourceVisibility convertFromProtoFormat(LocalResourceVisibilityProto e) {return ProtoUtils.convertFromProtoFormat(e);}public static ContainerResourceDecrease newInstance(ContainerId existingContainerId, Resource targetCapability) {ContainerResourceDecrease context = Records.newRecord(ContainerResourceDecrease.class);context.setContainerId(existingContainerId);context.setCapability(targetCapability);return context;}
public int hashCode() {return getCapability().hashCode() + getContainerId().hashCode();}
public boolean equals(Object other) {if (other instanceof ContainerResourceDecrease) {ContainerResourceDecrease ctx = (ContainerResourceDecrease) other;if (getContainerId() == null && ctx.getContainerId() != null) {return false;} else if (!getContainerId().equals(ctx.getContainerId())) {return false;}if (getCapability() == null && ctx.getCapability() != null) {return false;} else if (!getCapability().equals(ctx.getCapability())) {return false;}return true;} else {return false;}}public void close() {RPC.stopProxy(rpcProxy);}
public void refreshUserToGroupsMappings() {try {rpcProxy.refreshUserToGroupsMappings(NULL_CONTROLLER, VOID_REFRESH_USER_TO_GROUPS_MAPPING_REQUEST);} catch (ServiceException se) {throw ProtobufHelper.getRemoteException(se);}}
public void refreshSuperUserGroupsConfiguration() {try {rpcProxy.refreshSuperUserGroupsConfiguration(NULL_CONTROLLER, VOID_REFRESH_SUPERUSER_GROUPS_CONFIGURATION_REQUEST);} catch (ServiceException se) {throw ProtobufHelper.getRemoteException(se);}}
public boolean isMethodSupported(String methodName) {return RpcClientUtil.isMethodSupported(rpcProxy, RefreshUserMappingsProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(RefreshUserMappingsProtocolPB.class), methodName);}public void testJar() {//picking a class that is for sure in a JAR in the classpathString jar = JarFinder.getJar(LogFactory.class);Assert.assertTrue(new File(jar).exists());}
private static void delete(File file) {if (file.getAbsolutePath().length() < 5) {throw new IllegalArgumentException(MessageFormat.format("Path [{0}] is too short, not deleting", file.getAbsolutePath()));}if (file.exists()) {if (file.isDirectory()) {File[] children = file.listFiles();if (children != null) {for (File child : children) {delete(child);}}}if (!file.delete()) {throw new RuntimeException(MessageFormat.format("Could not delete path [{0}]", file.getAbsolutePath()));}}}
public void testExpandedClasspath() {//in this case the JAR is created on the flyString jar = JarFinder.getJar(TestJarFinder.class);Assert.assertTrue(new File(jar).exists());}
public void testExistingManifest() {File dir = new File(System.getProperty("test.build.dir", "target/test-dir"), TestJarFinder.class.getName() + "-testExistingManifest");delete(dir);dir.mkdirs();File metaInfDir = new File(dir, "META-INF");metaInfDir.mkdirs();File manifestFile = new File(metaInfDir, "MANIFEST.MF");Manifest manifest = new Manifest();OutputStream os = new FileOutputStream(manifestFile);manifest.write(os);os.close();File propsFile = new File(dir, "props.properties");Writer writer = new FileWriter(propsFile);new Properties().store(writer, "");writer.close();ByteArrayOutputStream baos = new ByteArrayOutputStream();JarOutputStream zos = new JarOutputStream(baos);JarFinder.jarDir(dir, "", zos);JarInputStream jis = new JarInputStream(new ByteArrayInputStream(baos.toByteArray()));Assert.assertNotNull(jis.getManifest());jis.close();}
public void testNoManifest() {File dir = new File(System.getProperty("test.build.dir", "target/test-dir"), TestJarFinder.class.getName() + "-testNoManifest");delete(dir);dir.mkdirs();File propsFile = new File(dir, "props.properties");Writer writer = new FileWriter(propsFile);new Properties().store(writer, "");writer.close();ByteArrayOutputStream baos = new ByteArrayOutputStream();JarOutputStream zos = new JarOutputStream(baos);JarFinder.jarDir(dir, "", zos);JarInputStream jis = new JarInputStream(new ByteArrayInputStream(baos.toByteArray()));Assert.assertNotNull(jis.getManifest());jis.close();}public static RpcCall read(XDR xdr) {return new RpcCall(xdr.readInt(), RpcMessage.Type.fromValue(xdr.readInt()), xdr.readInt(), xdr.readInt(), xdr.readInt(), xdr.readInt(), Credentials.readFlavorAndCredentials(xdr), Verifier.readFlavorAndVerifier(xdr));}
public static RpcCall getInstance(int xid, int program, int version, int procedure, Credentials cred, Verifier verifier) {return new RpcCall(xid, RpcMessage.Type.RPC_CALL, 2, program, version, procedure, cred, verifier);}
private void validateRpcVersion() {if (rpcVersion != RPC_VERSION) {throw new IllegalArgumentException("RPC version is expected to be " + RPC_VERSION + " but got " + rpcVersion);}}
public void validate() {validateMessageType(RpcMessage.Type.RPC_CALL);validateRpcVersion();}
public int getRpcVersion() {return rpcVersion;}
public int getProgram() {return program;}
public int getVersion() {return version;}
public int getProcedure() {return procedure;}
public Credentials getCredential() {return credentials;}
public Verifier getVerifier() {return verifier;}
public XDR write(XDR xdr) {xdr.writeInt(xid);xdr.writeInt(RpcMessage.Type.RPC_CALL.getValue());xdr.writeInt(2);xdr.writeInt(program);xdr.writeInt(version);xdr.writeInt(procedure);Credentials.writeFlavorAndCredentials(credentials, xdr);Verifier.writeFlavorAndVerifier(verifier, xdr);return xdr;}
public String toString() {return String.format("Xid:%d, messageType:%s, rpcVersion:%d, program:%d," + " version:%d, procedure:%d, credential:%s, verifier:%s", xid, messageType, rpcVersion, program, version, procedure, credentials.toString(), verifier.toString());}static Configuration getConfiguration(boolean loadHadoopDefaults, String... resources) {Configuration conf = new Configuration(loadHadoopDefaults);String confDir = System.getProperty(KMS_CONFIG_DIR);if (confDir != null) {try {if (!confDir.startsWith("/")) {throw new RuntimeException("System property '" + KMS_CONFIG_DIR + "' must be an absolute path: " + confDir);}if (!confDir.endsWith("/")) {confDir += "/";}for (String resource : resources) {conf.addResource(new URL("file://" + confDir + resource));}} catch (MalformedURLException ex) {throw new RuntimeException(ex);}} else {for (String resource : resources) {conf.addResource(resource);}}return conf;}
public static Configuration getKMSConf() {return getConfiguration(true, "core-site.xml", KMS_SITE_XML);}
public static Configuration getACLsConf() {return getConfiguration(false, KMS_ACLS_XML);}
public static boolean isACLsFileNewer(long time) {boolean newer = false;String confDir = System.getProperty(KMS_CONFIG_DIR);if (confDir != null) {if (!confDir.startsWith("/")) {throw new RuntimeException("System property '" + KMS_CONFIG_DIR + "' must be an absolute path: " + confDir);}if (!confDir.endsWith("/")) {confDir += "/";}File f = new File(confDir, KMS_ACLS_XML);// has been properly closed/flushednewer = f.lastModified() - time > 100;}return newer;}public Writable newInstance() {return new JobProfile();}
public String getUser() {return user;}
public JobID getJobID() {return jobid;}
public String getJobId() {return jobid.toString();}
public String getJobFile() {return jobFile;}
public URL getURL() {try {return new URL(url);} catch (IOException ie) {return null;}}
public String getJobName() {return name;}
public String getQueueName() {return queueName;}
public void write(DataOutput out) {jobid.write(out);Text.writeString(out, jobFile);Text.writeString(out, url);Text.writeString(out, user);Text.writeString(out, name);Text.writeString(out, queueName);}
public void readFields(DataInput in) {jobid.readFields(in);this.jobFile = StringInterner.weakIntern(Text.readString(in));this.url = StringInterner.weakIntern(Text.readString(in));this.user = StringInterner.weakIntern(Text.readString(in));this.name = StringInterner.weakIntern(Text.readString(in));this.queueName = StringInterner.weakIntern(Text.readString(in));}public static EncryptedKeyVersion createForDecryption(String encryptionKeyVersionName, byte[] encryptedKeyIv, byte[] encryptedKeyMaterial) {KeyVersion encryptedKeyVersion = new KeyVersion(null, EEK, encryptedKeyMaterial);return new EncryptedKeyVersion(null, encryptionKeyVersionName, encryptedKeyIv, encryptedKeyVersion);}
public String getEncryptionKeyName() {return encryptionKeyName;}
public String getEncryptionKeyVersionName() {return encryptionKeyVersionName;}
public byte[] getEncryptedKeyIv() {return encryptedKeyIv;}
public KeyVersion getEncryptedKeyVersion() {return encryptedKeyVersion;}
protected static byte[] deriveIV(byte[] encryptedKeyIV) {byte[] rIv = new byte[encryptedKeyIV.length];// Do a simple XOR transformation to flip all the bitsfor (int i = 0; i < encryptedKeyIV.length; i++) {rIv[i] = (byte) (encryptedKeyIV[i] ^ 0xff);}return rIv;}
protected SecureRandom initialValue() {return new SecureRandom();}
public EncryptedKeyVersion generateEncryptedKey(String encryptionKeyName) {// Fetch the encryption keyKeyVersion encryptionKey = keyProvider.getCurrentKey(encryptionKeyName);Preconditions.checkNotNull(encryptionKey, "No KeyVersion exists for key '%s' ", encryptionKeyName);// Generate random bytes for new key and IVCipher cipher = Cipher.getInstance("AES/CTR/NoPadding");final byte[] newKey = new byte[encryptionKey.getMaterial().length];RANDOM.get().nextBytes(newKey);final byte[] iv = new byte[cipher.getBlockSize()];RANDOM.get().nextBytes(iv);// Encryption key IV is derived from new key's IVfinal byte[] encryptionIV = EncryptedKeyVersion.deriveIV(iv);// Encrypt the new keycipher.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(encryptionKey.getMaterial(), "AES"), new IvParameterSpec(encryptionIV));final byte[] encryptedKey = cipher.doFinal(newKey);return new EncryptedKeyVersion(encryptionKeyName, encryptionKey.getVersionName(), iv, new KeyVersion(encryptionKey.getName(), EEK, encryptedKey));}
public KeyVersion decryptEncryptedKey(EncryptedKeyVersion encryptedKeyVersion) {// Fetch the encryption key materialfinal String encryptionKeyVersionName = encryptedKeyVersion.getEncryptionKeyVersionName();final KeyVersion encryptionKey = keyProvider.getKeyVersion(encryptionKeyVersionName);Preconditions.checkNotNull(encryptionKey, "KeyVersion name '%s' does not exist", encryptionKeyVersionName);Preconditions.checkArgument(encryptedKeyVersion.getEncryptedKeyVersion().getVersionName().equals(KeyProviderCryptoExtension.EEK), "encryptedKey version name must be '%s', is '%s'", KeyProviderCryptoExtension.EEK, encryptedKeyVersion.getEncryptedKeyVersion().getVersionName());final byte[] encryptionKeyMaterial = encryptionKey.getMaterial();// Encryption key IV is determined from encrypted key's IVfinal byte[] encryptionIV = EncryptedKeyVersion.deriveIV(encryptedKeyVersion.getEncryptedKeyIv());// Init the cipher with encryption key parametersCipher cipher = Cipher.getInstance("AES/CTR/NoPadding");cipher.init(Cipher.DECRYPT_MODE, new SecretKeySpec(encryptionKeyMaterial, "AES"), new IvParameterSpec(encryptionIV));// Decrypt the encrypted keyfinal KeyVersion encryptedKV = encryptedKeyVersion.getEncryptedKeyVersion();final byte[] decryptedKey = cipher.doFinal(encryptedKV.getMaterial());return new KeyVersion(encryptionKey.getName(), EK, decryptedKey);}
public void warmUpEncryptedKeys(String... keyNames) {}
public void warmUpEncryptedKeys(String... keyNames) {getExtension().warmUpEncryptedKeys(keyNames);}
public EncryptedKeyVersion generateEncryptedKey(String encryptionKeyName) {return getExtension().generateEncryptedKey(encryptionKeyName);}
public KeyVersion decryptEncryptedKey(EncryptedKeyVersion encryptedKey) {return getExtension().decryptEncryptedKey(encryptedKey);}
public static KeyProviderCryptoExtension createKeyProviderCryptoExtension(KeyProvider keyProvider) {CryptoExtension cryptoExtension = (keyProvider instanceof CryptoExtension) ? (CryptoExtension) keyProvider : new DefaultCryptoExtension(keyProvider);return new KeyProviderCryptoExtension(keyProvider, cryptoExtension);}public SimulatedFsDatasetVerifier newInstance(DataNode datanode, DataStorage storage, Configuration conf) {return new SimulatedFsDatasetVerifier(storage, conf);}
public boolean isSimulated() {return true;}
public static void setFactory(Configuration conf) {conf.set(DFSConfigKeys.DFS_DATANODE_FSDATASET_FACTORY_KEY, Factory.class.getName());}
public void testDataNodeInitStorage() {// Create configuration to use SimulatedFsDatasetVerifier#Factory.Configuration conf = new HdfsConfiguration();SimulatedFsDatasetVerifier.setFactory(conf);// invoked.MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();cluster.waitActive();cluster.shutdown();}private Path getAttemptOutputDir() {return new Path(JOB_OUTPUT_DIR, conf.get(JobContext.TASK_ATTEMPT_ID));}
public Path getOutputFile() {Path attemptOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING);return lDirAlloc.getLocalPathToRead(attemptOutput.toString(), conf);}
public Path getOutputFileForWrite(long size) {Path attemptOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING);return lDirAlloc.getLocalPathForWrite(attemptOutput.toString(), size, conf);}
public Path getOutputFileForWriteInVolume(Path existing) {Path outputDir = new Path(existing.getParent(), JOB_OUTPUT_DIR);Path attemptOutputDir = new Path(outputDir, conf.get(JobContext.TASK_ATTEMPT_ID));return new Path(attemptOutputDir, MAP_OUTPUT_FILENAME_STRING);}
public Path getOutputIndexFile() {Path attemptIndexOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);return lDirAlloc.getLocalPathToRead(attemptIndexOutput.toString(), conf);}
public Path getOutputIndexFileForWrite(long size) {Path attemptIndexOutput = new Path(getAttemptOutputDir(), MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);return lDirAlloc.getLocalPathForWrite(attemptIndexOutput.toString(), size, conf);}
public Path getOutputIndexFileForWriteInVolume(Path existing) {Path outputDir = new Path(existing.getParent(), JOB_OUTPUT_DIR);Path attemptOutputDir = new Path(outputDir, conf.get(JobContext.TASK_ATTEMPT_ID));return new Path(attemptOutputDir, MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);}
public Path getSpillFile(int spillNumber) {return lDirAlloc.getLocalPathToRead(String.format(SPILL_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), conf);}
public Path getSpillFileForWrite(int spillNumber, long size) {return lDirAlloc.getLocalPathForWrite(String.format(String.format(SPILL_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber)), size, conf);}
public Path getSpillIndexFile(int spillNumber) {return lDirAlloc.getLocalPathToRead(String.format(SPILL_INDEX_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), conf);}
public Path getSpillIndexFileForWrite(int spillNumber, long size) {return lDirAlloc.getLocalPathForWrite(String.format(SPILL_INDEX_FILE_PATTERN, conf.get(JobContext.TASK_ATTEMPT_ID), spillNumber), size, conf);}
public Path getInputFile(int mapId) {throw new UnsupportedOperationException("Incompatible with LocalRunner");}
public Path getInputFileForWrite(org.apache.hadoop.mapreduce.TaskID mapId, long size) {return lDirAlloc.getLocalPathForWrite(String.format(REDUCE_INPUT_FILE_FORMAT_STRING, getAttemptOutputDir().toString(), mapId.getId()), size, conf);}
public void removeAll() {throw new UnsupportedOperationException("Incompatible with LocalRunner");}
public void setConf(Configuration conf) {if (conf instanceof JobConf) {this.conf = (JobConf) conf;} else {this.conf = new JobConf(conf);}}
public Configuration getConf() {return conf;}public void setUp() {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION).build();cluster.waitActive();hdfs = cluster.getFileSystem();}
public void tearDown() {if (cluster != null) {cluster.shutdown();cluster = null;}}
public void testSnapshotfileLength() {hdfs.mkdirs(sub);int bytesRead;byte[] buffer = new byte[BLOCKSIZE * 8];FSDataInputStream fis = null;FileStatus fileStatus = null;// Create and write a file.Path file1 = new Path(sub, file1Name);DFSTestUtil.createFile(hdfs, file1, 0, REPLICATION, SEED);DFSTestUtil.appendFile(hdfs, file1, BLOCKSIZE);// Create a snapshot on the parent directory.hdfs.allowSnapshot(sub);hdfs.createSnapshot(sub, snapshot1);// Write more data to the file.DFSTestUtil.appendFile(hdfs, file1, BLOCKSIZE);// Make sure we can read the entire file via its non-snapshot path.fileStatus = hdfs.getFileStatus(file1);assertThat(fileStatus.getLen(), is((long) BLOCKSIZE * 2));fis = hdfs.open(file1);bytesRead = fis.read(0, buffer, 0, buffer.length);assertThat(bytesRead, is(BLOCKSIZE * 2));fis.close();// Try to open the file via its snapshot path.Path file1snap1 = SnapshotTestHelper.getSnapshotPath(sub, snapshot1, file1Name);fis = hdfs.open(file1snap1);fileStatus = hdfs.getFileStatus(file1snap1);assertThat(fileStatus.getLen(), is((long) BLOCKSIZE));// Make sure we can only read up to the snapshot length.bytesRead = fis.read(0, buffer, 0, buffer.length);assertThat(bytesRead, is(BLOCKSIZE));fis.close();}
public void testSnapshotFileLengthWithCatCommand() {FSDataInputStream fis = null;FileStatus fileStatus = null;int bytesRead;byte[] buffer = new byte[BLOCKSIZE * 8];hdfs.mkdirs(sub);Path file1 = new Path(sub, file1Name);DFSTestUtil.createFile(hdfs, file1, BLOCKSIZE, REPLICATION, SEED);hdfs.allowSnapshot(sub);hdfs.createSnapshot(sub, snapshot1);DFSTestUtil.appendFile(hdfs, file1, BLOCKSIZE);// Make sure we can read the entire file via its non-snapshot path.fileStatus = hdfs.getFileStatus(file1);assertEquals("Unexpected file length", BLOCKSIZE * 2, fileStatus.getLen());fis = hdfs.open(file1);bytesRead = fis.read(buffer, 0, buffer.length);assertEquals("Unexpected # bytes read", BLOCKSIZE * 2, bytesRead);fis.close();Path file1snap1 = SnapshotTestHelper.getSnapshotPath(sub, snapshot1, file1Name);fis = hdfs.open(file1snap1);fileStatus = hdfs.getFileStatus(file1snap1);assertEquals(fileStatus.getLen(), BLOCKSIZE);// Make sure we can only read up to the snapshot length.bytesRead = fis.read(buffer, 0, buffer.length);assertEquals("Unexpected # bytes read", BLOCKSIZE, bytesRead);fis.close();PrintStream outBackup = System.out;PrintStream errBackup = System.err;ByteArrayOutputStream bao = new ByteArrayOutputStream();System.setOut(new PrintStream(bao));System.setErr(new PrintStream(bao));// Make sure we can cat the file upto to snapshot lengthFsShell shell = new FsShell();try {ToolRunner.run(conf, shell, new String[] { "-cat", "/TestSnapshotFileLength/sub1/.snapshot/snapshot1/file1" });assertEquals("Unexpected # bytes from -cat", BLOCKSIZE, bao.size());} finally {System.setOut(outBackup);System.setErr(errBackup);}}public RefreshServiceAclsRequestProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}public static NMClientAsync createNMClientAsync(CallbackHandler callbackHandler) {return new NMClientAsyncImpl(callbackHandler);}
public NMClient getClient() {return client;}
public void setClient(NMClient client) {this.client = client;}
public CallbackHandler getCallbackHandler() {return callbackHandler;}
public void setCallbackHandler(CallbackHandler callbackHandler) {this.callbackHandler = callbackHandler;}public void testQueues() {DatanodeDescriptor fakeDN = DFSTestUtil.getLocalDatanodeDescriptor();DatanodeStorage storage = new DatanodeStorage("STORAGE_ID");DatanodeStorageInfo storageInfo = new DatanodeStorageInfo(fakeDN, storage);msgs.enqueueReportedBlock(storageInfo, block1Gs1, ReplicaState.FINALIZED);msgs.enqueueReportedBlock(storageInfo, block1Gs2, ReplicaState.FINALIZED);assertEquals(2, msgs.count());// Nothing queued yet for block 2assertNull(msgs.takeBlockQueue(block2Gs1));assertEquals(2, msgs.count());Queue<ReportedBlockInfo> q = msgs.takeBlockQueue(block1Gs2DifferentInstance);assertEquals("ReportedBlockInfo [block=blk_1_1, dn=127.0.0.1:50010, reportedState=FINALIZED]," + "ReportedBlockInfo [block=blk_1_2, dn=127.0.0.1:50010, reportedState=FINALIZED]", Joiner.on(",").join(q));assertEquals(0, msgs.count());// Should be null if we pull againassertNull(msgs.takeBlockQueue(block1Gs1));assertEquals(0, msgs.count());}public int compare(EditLogLedgerMetadata o1, EditLogLedgerMetadata o2) {if (o1.firstTxId < o2.firstTxId) {return -1;} else if (o1.firstTxId == o2.firstTxId) {return 0;} else {return 1;}}
String getZkPath() {return zkPath;}
long getFirstTxId() {return firstTxId;}
long getLastTxId() {return lastTxId;}
long getLedgerId() {return ledgerId;}
boolean isInProgress() {return this.inprogress;}
int getDataLayoutVersion() {return this.dataLayoutVersion;}
void finalizeLedger(long newLastTxId) {assert this.lastTxId == HdfsConstants.INVALID_TXID;this.lastTxId = newLastTxId;this.inprogress = false;}
static EditLogLedgerMetadata read(ZooKeeper zkc, String path) {try {byte[] data = zkc.getData(path, false, null);EditLogLedgerProto.Builder builder = EditLogLedgerProto.newBuilder();if (LOG.isDebugEnabled()) {LOG.debug("Reading " + path + " data: " + new String(data, UTF_8));}TextFormat.merge(new String(data, UTF_8), builder);if (!builder.isInitialized()) {throw new IOException("Invalid/Incomplete data in znode");}EditLogLedgerProto ledger = builder.build();int dataLayoutVersion = ledger.getDataLayoutVersion();long ledgerId = ledger.getLedgerId();long firstTxId = ledger.getFirstTxId();if (ledger.hasLastTxId()) {long lastTxId = ledger.getLastTxId();return new EditLogLedgerMetadata(path, dataLayoutVersion, ledgerId, firstTxId, lastTxId);} else {return new EditLogLedgerMetadata(path, dataLayoutVersion, ledgerId, firstTxId);}} catch (KeeperException.NoNodeException nne) {throw nne;} catch (KeeperException ke) {throw new IOException("Error reading from zookeeper", ke);} catch (InterruptedException ie) {throw new IOException("Interrupted reading from zookeeper", ie);}}
void write(ZooKeeper zkc, String path) {this.zkPath = path;EditLogLedgerProto.Builder builder = EditLogLedgerProto.newBuilder();builder.setDataLayoutVersion(dataLayoutVersion).setLedgerId(ledgerId).setFirstTxId(firstTxId);if (!inprogress) {builder.setLastTxId(lastTxId);}try {zkc.create(path, TextFormat.printToString(builder.build()).getBytes(UTF_8), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);} catch (KeeperException.NodeExistsException nee) {throw nee;} catch (KeeperException e) {throw new IOException("Error creating ledger znode", e);} catch (InterruptedException ie) {throw new IOException("Interrupted creating ledger znode", ie);}}
boolean verify(ZooKeeper zkc, String path) {try {EditLogLedgerMetadata other = read(zkc, path);if (LOG.isTraceEnabled()) {LOG.trace("Verifying " + this.toString() + " against " + other);}return other.equals(this);} catch (KeeperException e) {LOG.error("Couldn't verify data in " + path, e);return false;} catch (IOException ie) {LOG.error("Couldn't verify data in " + path, ie);return false;}}
public boolean equals(Object o) {if (!(o instanceof EditLogLedgerMetadata)) {return false;}EditLogLedgerMetadata ol = (EditLogLedgerMetadata) o;return ledgerId == ol.ledgerId && dataLayoutVersion == ol.dataLayoutVersion && firstTxId == ol.firstTxId && lastTxId == ol.lastTxId;}
public int hashCode() {int hash = 1;hash = hash * 31 + (int) ledgerId;hash = hash * 31 + (int) firstTxId;hash = hash * 31 + (int) lastTxId;hash = hash * 31 + dataLayoutVersion;return hash;}
public String toString() {return "[LedgerId:" + ledgerId + ", firstTxId:" + firstTxId + ", lastTxId:" + lastTxId + ", dataLayoutVersion:" + dataLayoutVersion + "]";}public ContainerResourceIncreaseRequestProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public ContainerId getContainerId() {ContainerResourceIncreaseRequestProtoOrBuilder p = viaProto ? proto : builder;if (this.existingContainerId != null) {return this.existingContainerId;}if (p.hasContainerId()) {this.existingContainerId = convertFromProtoFormat(p.getContainerId());}return this.existingContainerId;}
public void setContainerId(ContainerId existingContainerId) {maybeInitBuilder();if (existingContainerId == null) {builder.clearContainerId();}this.existingContainerId = existingContainerId;}
public Resource getCapability() {ContainerResourceIncreaseRequestProtoOrBuilder p = viaProto ? proto : builder;if (this.targetCapability != null) {return this.targetCapability;}if (p.hasCapability()) {this.targetCapability = convertFromProtoFormat(p.getCapability());}return this.targetCapability;}
public void setCapability(Resource targetCapability) {maybeInitBuilder();if (targetCapability == null) {builder.clearCapability();}this.targetCapability = targetCapability;}
private ContainerIdPBImpl convertFromProtoFormat(ContainerIdProto p) {return new ContainerIdPBImpl(p);}
private ContainerIdProto convertToProtoFormat(ContainerId t) {return ((ContainerIdPBImpl) t).getProto();}
private Resource convertFromProtoFormat(ResourceProto p) {return new ResourcePBImpl(p);}
private ResourceProto convertToProtoFormat(Resource t) {return ((ResourcePBImpl) t).getProto();}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ContainerResourceIncreaseRequestProto.newBuilder(proto);}viaProto = false;}
private void mergeLocalToBuilder() {if (this.existingContainerId != null) {builder.setContainerId(convertToProtoFormat(this.existingContainerId));}if (this.targetCapability != null) {builder.setCapability(convertToProtoFormat(this.targetCapability));}}public MyInfo get() {return new MyInfo();}public FileHandle getObjFileHandle() {return objFileHandle;}
public Nfs3FileAttributes getObjAttr() {return objAttr;}
public WccData getDirWcc() {return dirWcc;}
public XDR writeHeaderAndResponse(XDR out, int xid, Verifier verifier) {super.writeHeaderAndResponse(out, xid, verifier);if (getStatus() == Nfs3Status.NFS3_OK) {// Handle followsout.writeBoolean(true);objFileHandle.serialize(out);// Attributes followout.writeBoolean(true);objAttr.serialize(out);}dirWcc.serialize(out);return out;}public void startMiniKdc() {createTestDir();createMiniKdcConf();kdc = new MiniKdc(conf, workDir);kdc.start();}
public void createTestDir() {workDir = new File(System.getProperty("test.dir", "target"));}
public void createMiniKdcConf() {conf = MiniKdc.createConf();}
public void stopMiniKdc() {if (kdc != null) {kdc.stop();}}
public MiniKdc getKdc() {return kdc;}
public File getWorkDir() {return workDir;}
public Properties getConf() {return conf;}public static ResourceRequest newInstance(Priority priority, String hostName, Resource capability, int numContainers) {return newInstance(priority, hostName, capability, numContainers, true);}
public static ResourceRequest newInstance(Priority priority, String hostName, Resource capability, int numContainers, boolean relaxLocality) {ResourceRequest request = Records.newRecord(ResourceRequest.class);request.setPriority(priority);request.setResourceName(hostName);request.setCapability(capability);request.setNumContainers(numContainers);request.setRelaxLocality(relaxLocality);return request;}
public int compare(ResourceRequest r1, ResourceRequest r2) {// Compare priority, host and capabilityint ret = r1.getPriority().compareTo(r2.getPriority());if (ret == 0) {String h1 = r1.getResourceName();String h2 = r2.getResourceName();ret = h1.compareTo(h2);}if (ret == 0) {ret = r1.getCapability().compareTo(r2.getCapability());}return ret;}
public static boolean isAnyLocation(String hostName) {return ANY.equals(hostName);}
public int hashCode() {final int prime = 2153;int result = 2459;Resource capability = getCapability();String hostName = getResourceName();Priority priority = getPriority();result = prime * result + ((capability == null) ? 0 : capability.hashCode());result = prime * result + ((hostName == null) ? 0 : hostName.hashCode());result = prime * result + getNumContainers();result = prime * result + ((priority == null) ? 0 : priority.hashCode());return result;}
public boolean equals(Object obj) {if (this == obj)return true;if (obj == null)return false;if (getClass() != obj.getClass())return false;ResourceRequest other = (ResourceRequest) obj;Resource capability = getCapability();if (capability == null) {if (other.getCapability() != null)return false;} else if (!capability.equals(other.getCapability()))return false;String hostName = getResourceName();if (hostName == null) {if (other.getResourceName() != null)return false;} else if (!hostName.equals(other.getResourceName()))return false;if (getNumContainers() != other.getNumContainers())return false;Priority priority = getPriority();if (priority == null) {if (other.getPriority() != null)return false;} else if (!priority.equals(other.getPriority()))return false;return true;}
public int compareTo(ResourceRequest other) {int priorityComparison = this.getPriority().compareTo(other.getPriority());if (priorityComparison == 0) {int hostNameComparison = this.getResourceName().compareTo(other.getResourceName());if (hostNameComparison == 0) {int capabilityComparison = this.getCapability().compareTo(other.getCapability());if (capabilityComparison == 0) {return this.getNumContainers() - other.getNumContainers();} else {return capabilityComparison;}} else {return hostNameComparison;}} else {return priorityComparison;}}public void testSetrepDecreasing() {TestSetrepIncreasing.setrep(5, 3, false);}public static void setup() {fs = FileSystem.getLocal(new Configuration());fs.delete(testRootTempDir, true);fs.mkdirs(testRootTempDir);}
public static void cleanup() {fs.delete(testRootTempDir, true);}
protected void setup(Context context) {context.setStatus(myStatus);assertEquals(myStatus, context.getStatus());}
public void testContextStatus() {Path test = new Path(testRootTempDir, "testContextStatus");int numMaps = 1;Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, "in"), new Path(test, "out"), numMaps, 0);job.setMapperClass(MyMapper.class);job.waitForCompletion(true);assertTrue("Job failed", job.isSuccessful());TaskReport[] reports = job.getTaskReports(TaskType.MAP);assertEquals(numMaps, reports.length);assertEquals(myStatus, reports[0].getState());// test with default task statusint numReduces = 1;job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, "in"), new Path(test, "out"), numMaps, numReduces);job.setMapperClass(DataCopyMapper.class);job.setReducerClass(DataCopyReducer.class);job.setMapOutputKeyClass(Text.class);job.setMapOutputValueClass(Text.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(Text.class);// fail earlyjob.setMaxMapAttempts(1);job.setMaxReduceAttempts(0);// run the job and wait for completionjob.waitForCompletion(true);assertTrue("Job failed", job.isSuccessful());}
protected void setup(Context context) {// check if the map task attempt progress is 0assertEquals("Invalid progress in map setup", 0.0f, context.getProgress(), 0f);// define the progress boundariesif (context.getNumReduceTasks() == 0) {progressRange = 1f;} else {progressRange = 0.667f;}}
protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper.Context context) {// get the map phase progressfloat mapPhaseProgress = ((float) ++recordCount) / INPUT_LINES;// get the weighted map phase progressfloat weightedMapProgress = progressRange * mapPhaseProgress;// check the map progressassertEquals("Invalid progress in map", weightedMapProgress, context.getProgress(), 0f);context.write(new Text(value.toString() + recordCount), value);}
protected void cleanup(Mapper.Context context) {// check if the attempt progress is at the progress boundary assertEquals("Invalid progress in map cleanup", progressRange, context.getProgress(), 0f);}
public void testMapContextProgress() {int numMaps = 1;Path test = new Path(testRootTempDir, "testMapContextProgress");Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, "in"), new Path(test, "out"), numMaps, 0, INPUT);job.setMapperClass(ProgressCheckerMapper.class);job.setMapOutputKeyClass(Text.class);// fail earlyjob.setMaxMapAttempts(1);job.waitForCompletion(true);assertTrue("Job failed", job.isSuccessful());}
protected void setup(final Reducer.Context context) {// Note that the reduce will read some segments before calling setup()float reducePhaseProgress = ((float) ++recordCount) / INPUT_LINES;float weightedReducePhaseProgress = REDUCE_PROGRESS_RANGE * reducePhaseProgress;// check that the shuffle phase progress is accounted forassertEquals("Invalid progress in reduce setup", SHUFFLE_PROGRESS_RANGE + weightedReducePhaseProgress, context.getProgress(), 0.01f);}
public void reduce(Text key, Iterator<Text> values, Context context) {float reducePhaseProgress = ((float) ++recordCount) / INPUT_LINES;float weightedReducePhaseProgress = REDUCE_PROGRESS_RANGE * reducePhaseProgress;assertEquals("Invalid progress in reduce", SHUFFLE_PROGRESS_RANGE + weightedReducePhaseProgress, context.getProgress(), 0.01f);}
protected void cleanup(Reducer.Context context) {// check if the reduce task has progress of 1 in the endassertEquals("Invalid progress in reduce cleanup", 1.0f, context.getProgress(), 0f);}
public void testReduceContextProgress() {int numTasks = 1;Path test = new Path(testRootTempDir, "testReduceContextProgress");Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, "in"), new Path(test, "out"), numTasks, numTasks, INPUT);job.setMapperClass(ProgressCheckerMapper.class);job.setReducerClass(ProgressCheckerReducer.class);job.setMapOutputKeyClass(Text.class);// fail earlyjob.setMaxMapAttempts(1);job.setMaxReduceAttempts(1);job.waitForCompletion(true);assertTrue("Job failed", job.isSuccessful());}public static OutputStream constructOutputStream(DataOutput out) {if (out instanceof OutputStream) {return (OutputStream) out;} else {return new DataOutputOutputStream(out);}}
public void write(int b) {out.writeByte(b);}
public void write(byte[] b, int off, int len) {out.write(b, off, len);}
public void write(byte[] b) {out.write(b);}public static void setupCluster() {if (DomainSocket.getLoadingFailureReason() != null)return;DFSInputStream.tcpReadsDisabledForTesting = true;sockDir = new TemporarySocketDirectory();HdfsConfiguration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY, new File(sockDir.getDir(), "TestParallelLocalRead.%d.sock").getAbsolutePath());conf.setBoolean(DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_KEY, true);conf.setBoolean(DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_SKIP_CHECKSUM_KEY, false);DomainSocket.disableBindPathValidation();setupCluster(1, conf);}
public void before() {Assume.assumeThat(DomainSocket.getLoadingFailureReason(), equalTo(null));}
public static void teardownCluster() {if (DomainSocket.getLoadingFailureReason() != null)return;sockDir.close();TestParallelReadUtil.teardownCluster();}protected AbstractFSContract createContract(Configuration conf) {return new LocalFSContract(conf);}
public void testContractWorks() {String key = getContract().getConfKey(SUPPORTS_ATOMIC_RENAME);assertNotNull("not set: " + key, getContract().getConf().get(key));assertTrue("not true: " + key, getContract().isSupported(SUPPORTS_ATOMIC_RENAME, false));}
public void testContractResourceOnClasspath() {URL url = this.getClass().getClassLoader().getResource(LocalFSContract.CONTRACT_XML);assertNotNull("could not find contract resource", url);}public void testSocketFactory() {// Create a standard mini-clusterConfiguration sconf = new Configuration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(sconf).numDataNodes(1).build();final int nameNodePort = cluster.getNameNodePort();// Get a reference to its DFS directlyFileSystem fs = cluster.getFileSystem();Assert.assertTrue(fs instanceof DistributedFileSystem);DistributedFileSystem directDfs = (DistributedFileSystem) fs;Configuration cconf = getCustomSocketConfigs(nameNodePort);fs = FileSystem.get(cconf);Assert.assertTrue(fs instanceof DistributedFileSystem);DistributedFileSystem dfs = (DistributedFileSystem) fs;JobClient client = null;MiniMRYarnCluster miniMRYarnCluster = null;try {Path filePath = new Path("/dir");Assert.assertFalse(directDfs.exists(filePath));Assert.assertFalse(dfs.exists(filePath));directDfs.mkdirs(filePath);Assert.assertTrue(directDfs.exists(filePath));Assert.assertTrue(dfs.exists(filePath));fs = FileSystem.get(sconf);JobConf jobConf = new JobConf();FileSystem.setDefaultUri(jobConf, fs.getUri().toString());miniMRYarnCluster = initAndStartMiniMRYarnCluster(jobConf);JobConf jconf = new JobConf(miniMRYarnCluster.getConfig());jconf.set("hadoop.rpc.socket.factory.class.default", "org.apache.hadoop.ipc.DummySocketFactory");jconf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);String rmAddress = jconf.get("yarn.resourcemanager.address");String[] split = rmAddress.split(":");jconf.set("yarn.resourcemanager.address", split[0] + ':' + (Integer.parseInt(split[1]) + 10));client = new JobClient(jconf);JobStatus[] jobs = client.jobsToComplete();Assert.assertTrue(jobs.length == 0);} finally {closeClient(client);closeDfs(dfs);closeDfs(directDfs);stopMiniMRYarnCluster(miniMRYarnCluster);shutdownDFSCluster(cluster);}}
private MiniMRYarnCluster initAndStartMiniMRYarnCluster(JobConf jobConf) {MiniMRYarnCluster miniMRYarnCluster;miniMRYarnCluster = new MiniMRYarnCluster(this.getClass().getName(), 1);miniMRYarnCluster.init(jobConf);miniMRYarnCluster.start();return miniMRYarnCluster;}
private Configuration getCustomSocketConfigs(final int nameNodePort) {// Get another reference via network using a specific socket factoryConfiguration cconf = new Configuration();FileSystem.setDefaultUri(cconf, String.format("hdfs://localhost:%s/", nameNodePort + 10));cconf.set("hadoop.rpc.socket.factory.class.default", "org.apache.hadoop.ipc.DummySocketFactory");cconf.set("hadoop.rpc.socket.factory.class.ClientProtocol", "org.apache.hadoop.ipc.DummySocketFactory");cconf.set("hadoop.rpc.socket.factory.class.JobSubmissionProtocol", "org.apache.hadoop.ipc.DummySocketFactory");return cconf;}
private void shutdownDFSCluster(MiniDFSCluster cluster) {try {if (cluster != null)cluster.shutdown();} catch (Exception ignored) {ignored.printStackTrace();}}
private void stopMiniMRYarnCluster(MiniMRYarnCluster miniMRYarnCluster) {try {if (miniMRYarnCluster != null)miniMRYarnCluster.stop();} catch (Exception ignored) {ignored.printStackTrace();}}
private void closeDfs(DistributedFileSystem dfs) {try {if (dfs != null)dfs.close();} catch (Exception ignored) {ignored.printStackTrace();}}
private void closeClient(JobClient client) {try {if (client != null)client.close();} catch (Exception ignored) {ignored.printStackTrace();}}
public Socket createSocket() {return new Socket() {
@Overridepublic void connect(SocketAddress addr, int timeout) throws IOException {assert (addr instanceof InetSocketAddress);InetSocketAddress iaddr = (InetSocketAddress) addr;SocketAddress newAddr = null;if (iaddr.isUnresolved())newAddr = new InetSocketAddress(iaddr.getHostName(), iaddr.getPort() - 10);elsenewAddr = new InetSocketAddress(iaddr.getAddress(), iaddr.getPort() - 10);System.out.printf("Test socket: rerouting %s to %s\n", iaddr, newAddr);super.connect(newAddr, timeout);}};}
public void connect(SocketAddress addr, int timeout) {assert (addr instanceof InetSocketAddress);InetSocketAddress iaddr = (InetSocketAddress) addr;SocketAddress newAddr = null;if (iaddr.isUnresolved())newAddr = new InetSocketAddress(iaddr.getHostName(), iaddr.getPort() - 10);elsenewAddr = new InetSocketAddress(iaddr.getAddress(), iaddr.getPort() - 10);System.out.printf("Test socket: rerouting %s to %s\n", iaddr, newAddr);super.connect(newAddr, timeout);}
public boolean equals(Object obj) {if (this == obj)return true;if (obj == null)return false;if (!(obj instanceof DummySocketFactory))return false;return true;}public KillTaskRequestProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.taskId != null) {builder.setTaskId(convertToProtoFormat(this.taskId));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = KillTaskRequestProto.newBuilder(proto);}viaProto = false;}
public TaskId getTaskId() {KillTaskRequestProtoOrBuilder p = viaProto ? proto : builder;if (this.taskId != null) {return this.taskId;}if (!p.hasTaskId()) {return null;}this.taskId = convertFromProtoFormat(p.getTaskId());return this.taskId;}
public void setTaskId(TaskId taskId) {maybeInitBuilder();if (taskId == null)builder.clearTaskId();this.taskId = taskId;}
private TaskIdPBImpl convertFromProtoFormat(TaskIdProto p) {return new TaskIdPBImpl(p);}
private TaskIdProto convertToProtoFormat(TaskId t) {return ((TaskIdPBImpl) t).getProto();}public void testHarUriWithHaUriWithNoPort() {Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).nnTopology(MiniDFSNNTopology.simpleHATopology()).build();cluster.transitionToActive(0);HATestUtil.setFailoverConfigurations(cluster, conf);createEmptyHarArchive(HATestUtil.configureFailoverFs(cluster, conf), TEST_HAR_PATH);URI failoverUri = FileSystem.getDefaultUri(conf);Path p = new Path("har://hdfs-" + failoverUri.getAuthority() + TEST_HAR_PATH);p.getFileSystem(conf);} finally {cluster.shutdown();}}
private static void createEmptyHarArchive(FileSystem fs, Path p) {fs.mkdirs(p);OutputStream out = fs.create(new Path(p, "_masterindex"));out.write(Integer.toString(HarFileSystem.VERSION).getBytes());out.close();fs.create(new Path(p, "_index")).close();}private void applyNormalPattern(String modeStr, Matcher matcher) {// Are there multiple permissions stored in one chmod?boolean commaSeperated = false;for (int i = 0; i < 1 || matcher.end() < modeStr.length(); i++) {if (i > 0 && (!commaSeperated || !matcher.find())) {throw new IllegalArgumentException(modeStr);}String str = matcher.group(2);char type = str.charAt(str.length() - 1);boolean user, group, others, stickyBit;user = group = others = stickyBit = false;for (char c : matcher.group(1).toCharArray()) {switch(c) {case 'u':user = true;break;case 'g':group = true;break;case 'o':others = true;break;case 'a':break;default:throw new RuntimeException("Unexpected");}}if (!(user || group || others)) {// same as specifying 'a'user = group = others = true;}short mode = 0;for (char c : matcher.group(3).toCharArray()) {switch(c) {case 'r':mode |= 4;break;case 'w':mode |= 2;break;case 'x':mode |= 1;break;case 'X':mode |= 8;break;case 't':stickyBit = true;break;default:throw new RuntimeException("Unexpected");}}if (user) {userMode = mode;userType = type;}if (group) {groupMode = mode;groupType = type;}if (others) {othersMode = mode;othersType = type;stickyMode = (short) (stickyBit ? 1 : 0);stickyBitType = type;}commaSeperated = matcher.group(4).contains(",");}symbolic = true;}
private void applyOctalPattern(String modeStr, Matcher matcher) {userType = groupType = othersType = '=';// Check if sticky bit is specifiedString sb = matcher.group(1);if (!sb.isEmpty()) {stickyMode = Short.valueOf(sb.substring(0, 1));stickyBitType = '=';}String str = matcher.group(2);userMode = Short.valueOf(str.substring(0, 1));groupMode = Short.valueOf(str.substring(1, 2));othersMode = Short.valueOf(str.substring(2, 3));}
protected int combineModes(int existing, boolean exeOk) {return combineModeSegments(stickyBitType, stickyMode, (existing >>> 9), false) << 9 | combineModeSegments(userType, userMode, (existing >>> 6) & 7, exeOk) << 6 | combineModeSegments(groupType, groupMode, (existing >>> 3) & 7, exeOk) << 3 | combineModeSegments(othersType, othersMode, existing & 7, exeOk);}
protected int combineModeSegments(char type, int mode, int existing, boolean exeOk) {boolean capX = false;if ((mode & 8) != 0) {// convert X to x;capX = true;mode &= ~8;mode |= 1;}switch(type) {case '+':mode = mode | existing;break;case '-':mode = (~mode) & existing;break;case '=':break;default:throw new RuntimeException("Unexpected");}// if X is specified add 'x' only if exeOk or x was already set.if (capX && !exeOk && (mode & 1) != 0 && (existing & 1) == 0) {// remove xmode &= ~1;}return mode;}public int compare(LoggedJob j1, LoggedJob j2) {return (j1.getSubmitTime() < j2.getSubmitTime()) ? -1 : (j1.getSubmitTime() == j2.getSubmitTime()) ? 0 : 1;}
private LoggedJob rawNextJob() {LoggedJob result = reader.getNext();if ((!abortOnUnfixableSkew || skewBufferLength > 0) && result != null) {long thisTime = result.getSubmitTime();if (submitTimesSoFar.contains(thisTime)) {Integer myCount = countedRepeatedSubmitTimesSoFar.get(thisTime);countedRepeatedSubmitTimesSoFar.put(thisTime, myCount == null ? 2 : myCount + 1);} else {submitTimesSoFar.add(thisTime);}if (thisTime < skewMeasurementLatestSubmitTime) {Iterator<Long> endCursor = submitTimesSoFar.descendingIterator();int thisJobNeedsSkew = 0;Long keyNeedingSkew;while (endCursor.hasNext() && (keyNeedingSkew = endCursor.next()) > thisTime) {Integer keyNeedsSkewAmount = countedRepeatedSubmitTimesSoFar.get(keyNeedingSkew);thisJobNeedsSkew += keyNeedsSkewAmount == null ? 1 : keyNeedsSkewAmount;}maxSkewBufferNeeded = Math.max(maxSkewBufferNeeded, thisJobNeedsSkew);}skewMeasurementLatestSubmitTime = Math.max(thisTime, skewMeasurementLatestSubmitTime);}return result;}
LoggedJob nextJob() {LoggedJob newJob = rawNextJob();if (newJob != null) {skewBuffer.add(newJob);}LoggedJob result = skewBuffer.poll();while (result != null && result.getSubmitTime() < returnedLatestSubmitTime) {LOG.error("The current job was submitted earlier than the previous one");LOG.error("Its jobID is " + result.getJobID());LOG.error("Its submit time is " + result.getSubmitTime() + ",but the previous one was " + returnedLatestSubmitTime);if (abortOnUnfixableSkew) {throw new OutOfOrderException("Job submit time is " + result.getSubmitTime() + ",but the previous one was " + returnedLatestSubmitTime);}result = rawNextJob();}if (result != null) {returnedLatestSubmitTime = result.getSubmitTime();}return result;}
private void fillSkewBuffer() {for (int i = 0; i < skewBufferLength; ++i) {LoggedJob newJob = rawNextJob();if (newJob == null) {return;}skewBuffer.add(newJob);}}
int neededSkewBufferSize() {return maxSkewBufferNeeded;}
public void close() {reader.close();}public void testRMStarts() {Configuration conf = new YarnConfiguration();conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, ProportionalCapacityPreemptionPolicy.class.getCanonicalName());ResourceManager rm = new ResourceManager();try {rm.init(conf);} catch (Exception e) {fail("ResourceManager does not start when " + YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS + " is set to true");}}public void testDefaults() {KMSACLs acls = new KMSACLs(new Configuration(false));for (KMSACLs.Type type : KMSACLs.Type.values()) {Assert.assertTrue(acls.hasAccess(type, UserGroupInformation.createRemoteUser("foo")));}}
public void testCustom() {Configuration conf = new Configuration(false);for (KMSACLs.Type type : KMSACLs.Type.values()) {conf.set(type.getConfigKey(), type.toString() + " ");}KMSACLs acls = new KMSACLs(conf);for (KMSACLs.Type type : KMSACLs.Type.values()) {Assert.assertTrue(acls.hasAccess(type, UserGroupInformation.createRemoteUser(type.toString())));Assert.assertFalse(acls.hasAccess(type, UserGroupInformation.createRemoteUser("foo")));}}public void dummy() {}
public Statement apply(final Statement statement, final FrameworkMethod frameworkMethod, final Object o) {return new Statement() {
@Overridepublic void evaluate() throws Throwable {TestException testExceptionAnnotation = frameworkMethod.getAnnotation(TestException.class);try {statement.evaluate();if (testExceptionAnnotation != null) {Class<? extends Throwable> klass = testExceptionAnnotation.exception();fail("Expected Exception: " + klass.getSimpleName());}} catch (Throwable ex) {if (testExceptionAnnotation != null) {Class<? extends Throwable> klass = testExceptionAnnotation.exception();if (klass.isInstance(ex)) {String regExp = testExceptionAnnotation.msgRegExp();Pattern pattern = Pattern.compile(regExp);if (!pattern.matcher(ex.getMessage()).find()) {fail("Expected Exception Message pattern: " + regExp + " got message: " + ex.getMessage());}} else {fail("Expected Exception: " + klass.getSimpleName() + " got: " + ex.getClass().getSimpleName());}} else {throw ex;}}}};}
public void evaluate() {TestException testExceptionAnnotation = frameworkMethod.getAnnotation(TestException.class);try {statement.evaluate();if (testExceptionAnnotation != null) {Class<? extends Throwable> klass = testExceptionAnnotation.exception();fail("Expected Exception: " + klass.getSimpleName());}} catch (Throwable ex) {if (testExceptionAnnotation != null) {Class<? extends Throwable> klass = testExceptionAnnotation.exception();if (klass.isInstance(ex)) {String regExp = testExceptionAnnotation.msgRegExp();Pattern pattern = Pattern.compile(regExp);if (!pattern.matcher(ex.getMessage()).find()) {fail("Expected Exception Message pattern: " + regExp + " got message: " + ex.getMessage());}} else {fail("Expected Exception: " + klass.getSimpleName() + " got: " + ex.getClass().getSimpleName());}} else {throw ex;}}}public int getCode() {return code;}
public static BlockStatus fromCode(int code) {for (BlockStatus bs : BlockStatus.values()) {if (bs.code == code) {return bs;}}return null;}
public Block getBlock() {return this.block;}
public void setBlock(Block blk) {this.block = blk;}
public String getDelHints() {return this.delHints;}
public void setDelHints(String hints) {this.delHints = hints;}
public BlockStatus getStatus() {return status;}
public boolean equals(Object o) {if (!(o instanceof ReceivedDeletedBlockInfo)) {return false;}ReceivedDeletedBlockInfo other = (ReceivedDeletedBlockInfo) o;return this.block.equals(other.getBlock()) && this.status == other.status && this.delHints != null && this.delHints.equals(other.delHints);}
public int hashCode() {assert false : "hashCode not designed";return 0;}
public boolean blockEquals(Block b) {return this.block.equals(b);}
public boolean isDeletedBlock() {return status == BlockStatus.DELETED_BLOCK;}
public String toString() {return block.toString() + ", status: " + status + ", delHint: " + delHints;}protected char getFlag() {return flag;}public static boolean isHealthy(URI uri) {//check schemefinal String scheme = uri.getScheme();if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(scheme)) {throw new IllegalArgumentException("The scheme is not " + HdfsConstants.HDFS_URI_SCHEME + ", uri=" + uri);}final Configuration conf = new Configuration();//disable FileSystem cacheconf.setBoolean(String.format("fs.%s.impl.disable.cache", scheme), true);//disable client retry for rpc connection and rpc callsconf.setBoolean(DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, false);conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 0);DistributedFileSystem fs = null;try {fs = (DistributedFileSystem) FileSystem.get(uri, conf);final boolean safemode = fs.setSafeMode(SafeModeAction.SAFEMODE_GET);if (LOG.isDebugEnabled()) {LOG.debug("Is namenode in safemode? " + safemode + "; uri=" + uri);}fs.close();fs = null;return !safemode;} catch (IOException e) {if (LOG.isDebugEnabled()) {LOG.debug("Got an exception for uri=" + uri, e);}return false;} finally {IOUtils.cleanup(LOG, fs);}}public void init(SubsetConfiguration conf) {String filename = conf.getString(FILENAME_KEY);try {writer = filename == null ? new PrintWriter(System.out) : new PrintWriter(new FileWriter(new File(filename), true));} catch (Exception e) {throw new MetricsException("Error creating " + filename, e);}}
public void putMetrics(MetricsRecord record) {writer.print(record.timestamp());writer.print(" ");writer.print(record.context());writer.print(".");writer.print(record.name());String separator = ": ";for (MetricsTag tag : record.tags()) {writer.print(separator);separator = ", ";writer.print(tag.name());writer.print("=");writer.print(tag.value());}for (AbstractMetric metric : record.metrics()) {writer.print(separator);separator = ", ";writer.print(metric.name());writer.print("=");writer.print(metric.value());}writer.println();}
public void flush() {writer.flush();}
public void close() {writer.close();}public BlockLocation[] getBlockLocations() {return locations;}
public int compareTo(Object o) {return super.compareTo(o);}
public boolean equals(Object o) {return super.equals(o);}
public int hashCode() {return super.hashCode();}public String getContentType() {return JSON_TYPE;}
public String format(LoggingEvent event) {try {return toJson(event);} catch (IOException e) {return "{ \"logfailure\":\"" + e.getClass().toString() + "\"}";}}
public String toJson(LoggingEvent event) {StringWriter writer = new StringWriter();toJson(writer, event);return writer.toString();}
public Writer toJson(final Writer writer, final LoggingEvent event) {ThrowableInformation ti = event.getThrowableInformation();toJson(writer, event.getLoggerName(), event.getTimeStamp(), event.getLevel().toString(), event.getThreadName(), event.getRenderedMessage(), ti);return writer;}
public Writer toJson(final Writer writer, final String loggerName, final long timeStamp, final String level, final String threadName, final String message, final ThrowableInformation ti) {JsonGenerator json = factory.createJsonGenerator(writer);json.writeStartObject();json.writeStringField(NAME, loggerName);json.writeNumberField(TIME, timeStamp);Date date = new Date(timeStamp);json.writeStringField(DATE, dateFormat.format(date));json.writeStringField(LEVEL, level);json.writeStringField(THREAD, threadName);json.writeStringField(MESSAGE, message);if (ti != null) {Throwable thrown = ti.getThrowable();String eclass = (thrown != null) ? thrown.getClass().getName() : "";json.writeStringField(EXCEPTION_CLASS, eclass);String[] stackTrace = ti.getThrowableStrRep();json.writeArrayFieldStart(STACK);for (String row : stackTrace) {json.writeString(row);}json.writeEndArray();}json.writeEndObject();json.flush();json.close();return writer;}
public boolean ignoresThrowable() {return false;}
public void activateOptions() {}
public static ContainerNode parse(String json) {ObjectMapper mapper = new ObjectMapper(factory);JsonNode jsonNode = mapper.readTree(json);if (!(jsonNode instanceof ContainerNode)) {throw new IOException("Wrong JSON data: " + json);}return (ContainerNode) jsonNode;}public void testRelativePathAsURI() {URI u = Util.stringAsURI(RELATIVE_FILE_PATH);LOG.info("Uri: " + u);assertNotNull(u);}
public void testAbsolutePathAsURI() {URI u = null;u = Util.stringAsURI(ABSOLUTE_PATH_WINDOWS);assertNotNull("Uri should not be null for Windows path" + ABSOLUTE_PATH_WINDOWS, u);assertEquals(URI_FILE_SCHEMA, u.getScheme());u = Util.stringAsURI(ABSOLUTE_PATH_UNIX);assertNotNull("Uri should not be null for Unix path" + ABSOLUTE_PATH_UNIX, u);assertEquals(URI_FILE_SCHEMA, u.getScheme());}
public void testURI() {LOG.info("Testing correct Unix URI: " + URI_UNIX);URI u = Util.stringAsURI(URI_UNIX);LOG.info("Uri: " + u);assertNotNull("Uri should not be null at this point", u);assertEquals(URI_FILE_SCHEMA, u.getScheme());assertEquals(URI_PATH_UNIX, u.getPath());LOG.info("Testing correct windows URI: " + URI_WINDOWS);u = Util.stringAsURI(URI_WINDOWS);LOG.info("Uri: " + u);assertNotNull("Uri should not be null at this point", u);assertEquals(URI_FILE_SCHEMA, u.getScheme());assertEquals(URI_PATH_WINDOWS.replace("%20", " "), u.getPath());}protected int getMaxAttempts() {return conf.getInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 4);}
protected TaskAttemptImpl createAttempt() {return new ReduceTaskAttemptImpl(getID(), nextAttemptNumber, eventHandler, jobFile, partition, numMapTasks, conf, taskAttemptListener, jobToken, credentials, clock, appContext);}
public TaskType getType() {return TaskType.REDUCE;}public void write(DataOutput out) {// store versionout.writeByte(getVersion());}
public void readFields(DataInput in) {// read versionbyte version = in.readByte();if (version != getVersion())throw new VersionMismatchException(getVersion(), version);}public String parseParam(String str) {try {if (str != null) {str = str.trim();if (str.length() > 0) {value = parse(str);}}} catch (Exception ex) {throw new IllegalArgumentException(MessageFormat.format("Parameter [{0}], invalid value [{1}], value must be [{2}]", getName(), str, getDomain()));}return value;}
protected String parse(String str) {if (pattern != null) {if (!pattern.matcher(str).matches()) {throw new IllegalArgumentException("Invalid value");}}return str;}
protected String getDomain() {return (pattern == null) ? "a string" : pattern.pattern();}public MiniDFSCluster.Builder getDfsBuilder() {return dfsBuilder;}
public MiniQJMHACluster build() {return new MiniQJMHACluster(this);}
public void startupOption(StartupOption startOpt) {this.startOpt = startOpt;}
public static MiniDFSNNTopology createDefaultTopology(int basePort) {return new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf(NAMESERVICE).addNN(new MiniDFSNNTopology.NNConf("nn1").setIpcPort(basePort).setHttpPort(basePort + 1)).addNN(new MiniDFSNNTopology.NNConf("nn2").setIpcPort(basePort + 2).setHttpPort(basePort + 3)));}
private Configuration initHAConf(URI journalURI, Configuration conf) {conf.set(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY, journalURI.toString());String address1 = "127.0.0.1:" + basePort;String address2 = "127.0.0.1:" + (basePort + 2);conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, NAMESERVICE, NN1), address1);conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, NAMESERVICE, NN2), address2);conf.set(DFSConfigKeys.DFS_NAMESERVICES, NAMESERVICE);conf.set(DFSUtil.addKeySuffixes(DFS_HA_NAMENODES_KEY_PREFIX, NAMESERVICE), NN1 + "," + NN2);conf.set(DFS_CLIENT_FAILOVER_PROXY_PROVIDER_KEY_PREFIX + "." + NAMESERVICE, ConfiguredFailoverProxyProvider.class.getName());conf.set("fs.defaultFS", "hdfs://" + NAMESERVICE);return conf;}
public MiniDFSCluster getDfsCluster() {return cluster;}
public MiniJournalCluster getJournalCluster() {return journalCluster;}
public void shutdown() {cluster.shutdown();journalCluster.shutdown();}public static String getResolvedMRHistoryWebAppURLWithoutScheme(Configuration conf, boolean isSSLEnabled) {InetSocketAddress address = null;if (isSSLEnabled) {address = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_WEBAPP_HTTPS_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_HTTPS_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_HTTPS_PORT);} else {address = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_PORT);}address = NetUtils.getConnectAddress(address);StringBuffer sb = new StringBuffer();InetAddress resolved = address.getAddress();if (resolved == null || resolved.isAnyLocalAddress() || resolved.isLoopbackAddress()) {String lh = address.getHostName();try {lh = InetAddress.getLocalHost().getCanonicalHostName();} catch (UnknownHostException e) {}sb.append(lh);} else {sb.append(address.getHostName());}sb.append(":").append(address.getPort());return sb.toString();}
public void serviceInit(Configuration conf) {conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);if (conf.get(MRJobConfig.MR_AM_STAGING_DIR) == null) {conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(getTestWorkDir(), "apps_staging_dir/").getAbsolutePath());}// By default, VMEM monitoring disabled, PMEM monitoring enabled.if (!conf.getBoolean(MRConfig.MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING, MRConfig.DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING)) {conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);}conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, "000");try {Path stagingPath = FileContext.getFileContext(conf).makeQualified(new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));/*   * Re-configure the staging path on Windows if the file system is localFs.   * We need to use a absolute path that contains the drive letter. The unit   * test could run on a different drive than the AM. We can run into the   * issue that job files are localized to the drive where the test runs on,   * while the AM starts on a different drive and fails to find the job   * metafiles. Using absolute path can avoid this ambiguity.   */if (Path.WINDOWS) {if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)).getAbsolutePath());}}FileContext fc = FileContext.getFileContext(stagingPath.toUri(), conf);if (fc.util().exists(stagingPath)) {LOG.info(stagingPath + " exists! deleting...");fc.delete(stagingPath, true);}LOG.info("mkdir: " + stagingPath);//mkdir the staging directory so that right permissions are set while running as proxy userfc.mkdir(stagingPath, null, true);//mkdir done directory as well String doneDir = JobHistoryUtils.getConfiguredHistoryServerDoneDirPrefix(conf);Path doneDirPath = fc.makeQualified(new Path(doneDir));fc.mkdir(doneDirPath, null, true);} catch (IOException e) {throw new YarnRuntimeException("Could not create staging directory. ", e);}// The default is local because ofconf.set(MRConfig.MASTER_ADDRESS, "test");//configure the shuffle service in NMconf.setStrings(YarnConfiguration.NM_AUX_SERVICES, new String[] { ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID });conf.setClass(String.format(YarnConfiguration.NM_AUX_SERVICE_FMT, ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID), ShuffleHandler.class, Service.class);// Non-standard shuffle portconf.setInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY, 0);conf.setClass(YarnConfiguration.NM_CONTAINER_EXECUTOR, DefaultContainerExecutor.class, ContainerExecutor.class);// for corresponding uberized tests.conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);super.serviceInit(conf);}
public synchronized void serviceStart() {try {if (!getConfig().getBoolean(JHAdminConfig.MR_HISTORY_MINICLUSTER_FIXED_PORTS, JHAdminConfig.DEFAULT_MR_HISTORY_MINICLUSTER_FIXED_PORTS)) {String hostname = MiniYARNCluster.getHostname();// pick free random ports.getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, hostname + ":0");MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), hostname + ":0");getConfig().set(JHAdminConfig.JHS_ADMIN_ADDRESS, hostname + ":0");}historyServer = new JobHistoryServer();historyServer.init(getConfig());new Thread() {
public void run() {historyServer.start();}
;}.start();while (historyServer.getServiceState() == STATE.INITED) {LOG.info("Waiting for HistoryServer to start...");Thread.sleep(1500);}//TODO Add a timeout. State.STOPPED check ?if (historyServer.getServiceState() != STATE.STARTED) {throw new IOException("HistoryServer failed to start");}super.serviceStart();} catch (Throwable t) {throw new YarnRuntimeException(t);}//need to do this because historyServer.init creates a new ConfigurationgetConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));LOG.info("MiniMRYARN ResourceManager address: " + getConfig().get(YarnConfiguration.RM_ADDRESS));LOG.info("MiniMRYARN ResourceManager web address: " + WebAppUtils.getRMWebAppURLWithoutScheme(getConfig()));LOG.info("MiniMRYARN HistoryServer address: " + getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));LOG.info("MiniMRYARN HistoryServer web address: " + getResolvedMRHistoryWebAppURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSHttpPolicy() == HttpConfig.Policy.HTTPS_ONLY));}
public void run() {historyServer.start();}
public synchronized void serviceStop() {if (historyServer != null) {historyServer.stop();}super.serviceStop();}
public JobHistoryServer getHistoryServer() {return this.historyServer;}public void testSingleList() {DatanodeDescriptor dn = new DatanodeDescriptor(new DatanodeID("127.0.0.1", "localhost", "abcd", 5000, 5001, 5002, 5003));CachedBlock[] blocks = new CachedBlock[] { new CachedBlock(0L, (short) 1, true), new CachedBlock(1L, (short) 1, true), new CachedBlock(2L, (short) 1, true) };// check that lists are emptyAssert.assertTrue("expected pending cached list to start off empty.", !dn.getPendingCached().iterator().hasNext());Assert.assertTrue("expected cached list to start off empty.", !dn.getCached().iterator().hasNext());Assert.assertTrue("expected pending uncached list to start off empty.", !dn.getPendingUncached().iterator().hasNext());// add a block to the backAssert.assertTrue(dn.getCached().add(blocks[0]));Assert.assertTrue("expected pending cached list to still be empty.", !dn.getPendingCached().iterator().hasNext());Assert.assertEquals("failed to insert blocks[0]", blocks[0], dn.getCached().iterator().next());Assert.assertTrue("expected pending uncached list to still be empty.", !dn.getPendingUncached().iterator().hasNext());// add another block to the backAssert.assertTrue(dn.getCached().add(blocks[1]));Iterator<CachedBlock> iter = dn.getCached().iterator();Assert.assertEquals(blocks[0], iter.next());Assert.assertEquals(blocks[1], iter.next());Assert.assertTrue(!iter.hasNext());// add a block to the frontAssert.assertTrue(dn.getCached().addFirst(blocks[2]));iter = dn.getCached().iterator();Assert.assertEquals(blocks[2], iter.next());Assert.assertEquals(blocks[0], iter.next());Assert.assertEquals(blocks[1], iter.next());Assert.assertTrue(!iter.hasNext());// remove a block from the middleAssert.assertTrue(dn.getCached().remove(blocks[0]));iter = dn.getCached().iterator();Assert.assertEquals(blocks[2], iter.next());Assert.assertEquals(blocks[1], iter.next());Assert.assertTrue(!iter.hasNext());// remove all blocksdn.getCached().clear();Assert.assertTrue("expected cached list to be empty after clear.", !dn.getPendingCached().iterator().hasNext());}
private void testAddElementsToList(CachedBlocksList list, CachedBlock[] blocks) {Assert.assertTrue("expected list to start off empty.", !list.iterator().hasNext());for (CachedBlock block : blocks) {Assert.assertTrue(list.add(block));}}
private void testRemoveElementsFromList(Random r, CachedBlocksList list, CachedBlock[] blocks) {int i = 0;for (Iterator<CachedBlock> iter = list.iterator(); iter.hasNext(); ) {Assert.assertEquals(blocks[i], iter.next());i++;}if (r.nextBoolean()) {LOG.info("Removing via iterator");for (Iterator<CachedBlock> iter = list.iterator(); iter.hasNext(); ) {iter.next();iter.remove();}} else {LOG.info("Removing in pseudo-random order");CachedBlock[] remainingBlocks = Arrays.copyOf(blocks, blocks.length);for (int removed = 0; removed < remainingBlocks.length; ) {int toRemove = r.nextInt(remainingBlocks.length);if (remainingBlocks[toRemove] != null) {Assert.assertTrue(list.remove(remainingBlocks[toRemove]));remainingBlocks[toRemove] = null;removed++;}}}Assert.assertTrue("expected list to be empty after everything " + "was removed.", !list.iterator().hasNext());}
public void testMultipleLists() {DatanodeDescriptor[] datanodes = new DatanodeDescriptor[] { new DatanodeDescriptor(new DatanodeID("127.0.0.1", "localhost", "abcd", 5000, 5001, 5002, 5003)), new DatanodeDescriptor(new DatanodeID("127.0.1.1", "localhost", "efgh", 6000, 6001, 6002, 6003)) };CachedBlocksList[] lists = new CachedBlocksList[] { datanodes[0].getPendingCached(), datanodes[0].getCached(), datanodes[1].getPendingCached(), datanodes[1].getCached(), datanodes[1].getPendingUncached() };final int NUM_BLOCKS = 8000;CachedBlock[] blocks = new CachedBlock[NUM_BLOCKS];for (int i = 0; i < NUM_BLOCKS; i++) {blocks[i] = new CachedBlock(i, (short) i, true);}Random r = new Random(654);for (CachedBlocksList list : lists) {testAddElementsToList(list, blocks);}for (CachedBlocksList list : lists) {testRemoveElementsFromList(r, list, blocks);}}public String getName() {return name;}public static void verifyHsJobPartial(JSONObject info, Job job) {assertEquals("incorrect number of elements", 12, info.length());// everyone access fieldsverifyHsJobGeneric(job, info.getString("id"), info.getString("user"), info.getString("name"), info.getString("state"), info.getString("queue"), info.getLong("startTime"), info.getLong("finishTime"), info.getInt("mapsTotal"), info.getInt("mapsCompleted"), info.getInt("reducesTotal"), info.getInt("reducesCompleted"));}
public static void verifyHsJob(JSONObject info, Job job) {assertEquals("incorrect number of elements", 25, info.length());// everyone access fieldsverifyHsJobGeneric(job, info.getString("id"), info.getString("user"), info.getString("name"), info.getString("state"), info.getString("queue"), info.getLong("startTime"), info.getLong("finishTime"), info.getInt("mapsTotal"), info.getInt("mapsCompleted"), info.getInt("reducesTotal"), info.getInt("reducesCompleted"));String diagnostics = "";if (info.has("diagnostics")) {diagnostics = info.getString("diagnostics");}// restricted access fields - if security and acls setverifyHsJobGenericSecure(job, info.getBoolean("uberized"), diagnostics, info.getLong("avgMapTime"), info.getLong("avgReduceTime"), info.getLong("avgShuffleTime"), info.getLong("avgMergeTime"), info.getInt("failedReduceAttempts"), info.getInt("killedReduceAttempts"), info.getInt("successfulReduceAttempts"), info.getInt("failedMapAttempts"), info.getInt("killedMapAttempts"), info.getInt("successfulMapAttempts"));}
public static void verifyHsJobGeneric(Job job, String id, String user, String name, String state, String queue, long startTime, long finishTime, int mapsTotal, int mapsCompleted, int reducesTotal, int reducesCompleted) {JobReport report = job.getReport();WebServicesTestUtils.checkStringMatch("id", MRApps.toString(job.getID()), id);WebServicesTestUtils.checkStringMatch("user", job.getUserName().toString(), user);WebServicesTestUtils.checkStringMatch("name", job.getName(), name);WebServicesTestUtils.checkStringMatch("state", job.getState().toString(), state);WebServicesTestUtils.checkStringMatch("queue", job.getQueueName(), queue);assertEquals("startTime incorrect", report.getStartTime(), startTime);assertEquals("finishTime incorrect", report.getFinishTime(), finishTime);assertEquals("mapsTotal incorrect", job.getTotalMaps(), mapsTotal);assertEquals("mapsCompleted incorrect", job.getCompletedMaps(), mapsCompleted);assertEquals("reducesTotal incorrect", job.getTotalReduces(), reducesTotal);assertEquals("reducesCompleted incorrect", job.getCompletedReduces(), reducesCompleted);}
public static void verifyHsJobGenericSecure(Job job, Boolean uberized, String diagnostics, long avgMapTime, long avgReduceTime, long avgShuffleTime, long avgMergeTime, int failedReduceAttempts, int killedReduceAttempts, int successfulReduceAttempts, int failedMapAttempts, int killedMapAttempts, int successfulMapAttempts) {String diagString = "";List<String> diagList = job.getDiagnostics();if (diagList != null && !diagList.isEmpty()) {StringBuffer b = new StringBuffer();for (String diag : diagList) {b.append(diag);}diagString = b.toString();}WebServicesTestUtils.checkStringMatch("diagnostics", diagString, diagnostics);assertEquals("isUber incorrect", job.isUber(), uberized);assertTrue("failedReduceAttempts not >= 0", failedReduceAttempts >= 0);assertTrue("killedReduceAttempts not >= 0", killedReduceAttempts >= 0);assertTrue("successfulReduceAttempts not >= 0", successfulReduceAttempts >= 0);assertTrue("failedMapAttempts not >= 0", failedMapAttempts >= 0);assertTrue("killedMapAttempts not >= 0", killedMapAttempts >= 0);assertTrue("successfulMapAttempts not >= 0", successfulMapAttempts >= 0);assertTrue("avgMapTime not >= 0", avgMapTime >= 0);assertTrue("avgReduceTime not >= 0", avgReduceTime >= 0);assertTrue("avgShuffleTime not >= 0", avgShuffleTime >= 0);assertTrue("avgMergeTime not >= 0", avgMergeTime >= 0);}public void setDisplayName(String name) {}
public synchronized boolean equals(Object genericRight) {if (genericRight instanceof Counter) {synchronized (genericRight) {Counter right = (Counter) genericRight;return getName().equals(right.getName()) && getDisplayName().equals(right.getDisplayName()) && getValue() == right.getValue();}}return false;}
public synchronized int hashCode() {return Objects.hashCode(getName(), getDisplayName(), getValue());}public Container getContainer() {return container;}public static GetClusterMetricsRequest newInstance() {GetClusterMetricsRequest request = Records.newRecord(GetClusterMetricsRequest.class);return request;}public void testGetPassword() {Configuration conf = provisionCredentialsForSSL();// use WebAppUtils as would be used by loadSslConfigurationAssert.assertEquals("keypass", WebAppUtils.getPassword(conf, WebAppUtils.WEB_APP_KEY_PASSWORD_KEY));Assert.assertEquals("storepass", WebAppUtils.getPassword(conf, WebAppUtils.WEB_APP_KEYSTORE_PASSWORD_KEY));Assert.assertEquals("trustpass", WebAppUtils.getPassword(conf, WebAppUtils.WEB_APP_TRUSTSTORE_PASSWORD_KEY));// let's make sure that a password that doesn't exist returns nullAssert.assertEquals(null, WebAppUtils.getPassword(conf, "invalid-alias"));}
public void testLoadSslConfiguration() {Configuration conf = provisionCredentialsForSSL();TestBuilder builder = (TestBuilder) new TestBuilder();builder = (TestBuilder) WebAppUtils.loadSslConfiguration(builder, conf);String keypass = "keypass";String storepass = "storepass";String trustpass = "trustpass";// make sure we get the right passwords in the builderassertEquals(keypass, ((TestBuilder) builder).keypass);assertEquals(storepass, ((TestBuilder) builder).keystorePassword);assertEquals(trustpass, ((TestBuilder) builder).truststorePassword);}
protected Configuration provisionCredentialsForSSL() {File testDir = new File(System.getProperty("test.build.data", "target/test-dir"));Configuration conf = new Configuration();final String ourUrl = JavaKeyStoreProvider.SCHEME_NAME + "://file/" + testDir + "/test.jks";File file = new File(testDir, "test.jks");file.delete();conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, ourUrl);CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);char[] keypass = { 'k', 'e', 'y', 'p', 'a', 's', 's' };char[] storepass = { 's', 't', 'o', 'r', 'e', 'p', 'a', 's', 's' };char[] trustpass = { 't', 'r', 'u', 's', 't', 'p', 'a', 's', 's' };// ensure that we get nulls when the key isn't thereassertEquals(null, provider.getCredentialEntry(WebAppUtils.WEB_APP_KEY_PASSWORD_KEY));assertEquals(null, provider.getCredentialEntry(WebAppUtils.WEB_APP_KEYSTORE_PASSWORD_KEY));assertEquals(null, provider.getCredentialEntry(WebAppUtils.WEB_APP_TRUSTSTORE_PASSWORD_KEY));// create new aliasestry {provider.createCredentialEntry(WebAppUtils.WEB_APP_KEY_PASSWORD_KEY, keypass);provider.createCredentialEntry(WebAppUtils.WEB_APP_KEYSTORE_PASSWORD_KEY, storepass);provider.createCredentialEntry(WebAppUtils.WEB_APP_TRUSTSTORE_PASSWORD_KEY, trustpass);// write out so that it can be found in checksprovider.flush();} catch (Exception e) {e.printStackTrace();throw e;}// make sure we get back the right key directly from apiassertArrayEquals(keypass, provider.getCredentialEntry(WebAppUtils.WEB_APP_KEY_PASSWORD_KEY).getCredential());assertArrayEquals(storepass, provider.getCredentialEntry(WebAppUtils.WEB_APP_KEYSTORE_PASSWORD_KEY).getCredential());assertArrayEquals(trustpass, provider.getCredentialEntry(WebAppUtils.WEB_APP_TRUSTSTORE_PASSWORD_KEY).getCredential());return conf;}
public Builder trustStore(String location, String password, String type) {truststorePassword = password;return super.trustStore(location, password, type);}
public Builder keyStore(String location, String password, String type) {keystorePassword = password;return super.keyStore(location, password, type);}
public Builder keyPassword(String password) {keypass = password;return super.keyPassword(password);}public static boolean injectCriteria(String klassName) {boolean trigger = false;// TODO fix this: make it more sophisticated!!!if (generator.nextFloat() < getProbability(klassName)) {trigger = true;}return trigger;}
protected static float getProbability(final String klass) {String newProbName = FPROB_NAME + klass;String newValue = System.getProperty(newProbName, conf.get(ALL_PROBABILITIES));if (newValue != null && !newValue.equals(conf.get(newProbName)))conf.set(newProbName, newValue);float ret = conf.getFloat(newProbName, conf.getFloat(ALL_PROBABILITIES, DEFAULT_PROB));LOG.debug("Request for " + newProbName + " returns=" + ret);// Make sure that probability level is valid.if (ret < DEFAULT_PROB || ret > MAX_PROB)ret = conf.getFloat(ALL_PROBABILITIES, DEFAULT_PROB);return ret;}public String getName() {return NAME;}
public short getValue(final Configuration conf) {return getValue() != null ? getValue() : (short) conf.getInt(DFS_REPLICATION_KEY, DFS_REPLICATION_DEFAULT);}public int read() {checkStream();return (read(oneByte, 0, oneByte.length) == -1) ? -1 : (oneByte[0] & 0xff);}
public int read(byte[] b, int off, int len) {checkStream();if ((off | len | (off + len) | (b.length - (off + len))) < 0) {throw new IndexOutOfBoundsException();} else if (len == 0) {return 0;}return decompress(b, off, len);}
protected int decompress(byte[] b, int off, int len) {int n = 0;while ((n = decompressor.decompress(b, off, len)) == 0) {if (decompressor.needsDictionary()) {eof = true;return -1;}if (decompressor.finished()) {// concatenated substream/"member".int nRemaining = decompressor.getRemaining();if (nRemaining == 0) {int m = getCompressedData();if (m == -1) {// return success, as if we had never called getCompressedData()eof = true;return -1;}decompressor.reset();decompressor.setInput(buffer, 0, m);lastBytesSent = m;} else {// other engine) and buffers, then "resend" remaining input datadecompressor.reset();int leftoverOffset = lastBytesSent - nRemaining;assert (leftoverOffset >= 0);// this recopies userBuf -> direct buffer if using native libraries:decompressor.setInput(buffer, leftoverOffset, nRemaining);}} else if (decompressor.needsInput()) {int m = getCompressedData();if (m == -1) {throw new EOFException("Unexpected end of input stream");}decompressor.setInput(buffer, 0, m);lastBytesSent = m;}}return n;}
protected int getCompressedData() {checkStream();// note that the _caller_ is now required to call setInput() or throwreturn in.read(buffer, 0, buffer.length);}
protected void checkStream() {if (closed) {throw new IOException("Stream closed");}}
public void resetState() {decompressor.reset();}
public long skip(long n) {// Sanity checksif (n < 0) {throw new IllegalArgumentException("negative skip length");}checkStream();// Read 'n' bytesint skipped = 0;while (skipped < n) {int len = Math.min(((int) n - skipped), skipBytes.length);len = read(skipBytes, 0, len);if (len == -1) {eof = true;break;}skipped += len;}return skipped;}
public int available() {checkStream();return (eof) ? 0 : 1;}
public void close() {if (!closed) {in.close();closed = true;}}
public boolean markSupported() {return false;}
public synchronized void mark(int readlimit) {}
public synchronized void reset() {throw new IOException("mark/reset not supported");}public ApplicationAttemptId getApplicationAttemptId() {if (this.applicationAttemptId != null) {return this.applicationAttemptId;}ApplicationAttemptStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationAttemptId()) {return null;}this.applicationAttemptId = convertFromProtoFormat(p.getApplicationAttemptId());return this.applicationAttemptId;}
public void setApplicationAttemptId(ApplicationAttemptId applicationAttemptId) {maybeInitBuilder();if (applicationAttemptId == null) {builder.clearApplicationAttemptId();}this.applicationAttemptId = applicationAttemptId;}
public String getHost() {ApplicationAttemptStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasHost()) {return null;}return p.getHost();}
public void setHost(String host) {maybeInitBuilder();if (host == null) {builder.clearHost();return;}builder.setHost(host);}
public int getRPCPort() {ApplicationAttemptStartDataProtoOrBuilder p = viaProto ? proto : builder;return p.getRpcPort();}
public void setRPCPort(int rpcPort) {maybeInitBuilder();builder.setRpcPort(rpcPort);}
public ContainerId getMasterContainerId() {if (this.masterContainerId != null) {return this.masterContainerId;}ApplicationAttemptStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationAttemptId()) {return null;}this.masterContainerId = convertFromProtoFormat(p.getMasterContainerId());return this.masterContainerId;}
public void setMasterContainerId(ContainerId masterContainerId) {maybeInitBuilder();if (masterContainerId == null) {builder.clearMasterContainerId();}this.masterContainerId = masterContainerId;}
public ApplicationAttemptStartDataProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (this.applicationAttemptId != null && !((ApplicationAttemptIdPBImpl) this.applicationAttemptId).getProto().equals(builder.getApplicationAttemptId())) {builder.setApplicationAttemptId(convertToProtoFormat(this.applicationAttemptId));}if (this.masterContainerId != null && !((ContainerIdPBImpl) this.masterContainerId).getProto().equals(builder.getMasterContainerId())) {builder.setMasterContainerId(convertToProtoFormat(this.masterContainerId));}}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ApplicationAttemptStartDataProto.newBuilder(proto);}viaProto = false;}
private ApplicationAttemptIdPBImpl convertFromProtoFormat(ApplicationAttemptIdProto applicationAttemptId) {return new ApplicationAttemptIdPBImpl(applicationAttemptId);}
private ApplicationAttemptIdProto convertToProtoFormat(ApplicationAttemptId applicationAttemptId) {return ((ApplicationAttemptIdPBImpl) applicationAttemptId).getProto();}
private ContainerIdPBImpl convertFromProtoFormat(ContainerIdProto containerId) {return new ContainerIdPBImpl(containerId);}
private ContainerIdProto convertToProtoFormat(ContainerId masterContainerId) {return ((ContainerIdPBImpl) masterContainerId).getProto();}public void testRenameFileIntoExistingDirectory() {assumeRenameSupported();Path src = path("/test/olddir/file");createFile(src);Path dst = path("/test/new/newdir");fs.mkdirs(dst);rename(src, dst, true, false, true);Path newFile = path("/test/new/newdir/file");if (!fs.exists(newFile)) {String ls = ls(dst);LOG.info(ls(path("/test/new")));LOG.info(ls(path("/test/hadoop")));fail("did not find " + newFile + " - directory: " + ls);}assertTrue("Destination changed", fs.exists(path("/test/new/newdir/file")));}
public void testRenameFile() {assumeRenameSupported();final Path old = new Path("/test/alice/file");final Path newPath = new Path("/test/bob/file");fs.mkdirs(newPath.getParent());final FSDataOutputStream fsDataOutputStream = fs.create(old);final byte[] message = "Some data".getBytes();fsDataOutputStream.write(message);fsDataOutputStream.close();assertTrue(fs.exists(old));rename(old, newPath, true, false, true);final FSDataInputStream bobStream = fs.open(newPath);final byte[] bytes = new byte[512];final int read = bobStream.read(bytes);bobStream.close();final byte[] buffer = new byte[read];System.arraycopy(bytes, 0, buffer, 0, read);assertEquals(new String(message), new String(buffer));}
public void testRenameDirectory() {assumeRenameSupported();final Path old = new Path("/test/data/logs");final Path newPath = new Path("/test/var/logs");fs.mkdirs(old);fs.mkdirs(newPath.getParent());assertTrue(fs.exists(old));rename(old, newPath, true, false, true);}
public void testRenameTheSameDirectory() {assumeRenameSupported();final Path old = new Path("/test/usr/data");fs.mkdirs(old);rename(old, old, false, true, true);}
public void testRenameDirectoryIntoExistingDirectory() {assumeRenameSupported();Path src = path("/test/olddir/dir");fs.mkdirs(src);createFile(path("/test/olddir/dir/file1"));createFile(path("/test/olddir/dir/subdir/file2"));Path dst = path("/test/new/newdir");fs.mkdirs(dst);//this renames into a childrename(src, dst, true, false, true);assertExists("new dir", path("/test/new/newdir/dir"));assertExists("Renamed nested file1", path("/test/new/newdir/dir/file1"));assertPathDoesNotExist("Nested file1 should have been deleted", path("/test/olddir/dir/file1"));assertExists("Renamed nested subdir", path("/test/new/newdir/dir/subdir/"));assertExists("file under subdir", path("/test/new/newdir/dir/subdir/file2"));assertPathDoesNotExist("Nested /test/hadoop/dir/subdir/file2 still exists", path("/test/olddir/dir/subdir/file2"));}
public void testRenameDirToSelf() {assumeRenameSupported();Path parentdir = path("/test/parentdir");fs.mkdirs(parentdir);Path child = new Path(parentdir, "child");createFile(child);rename(parentdir, parentdir, false, true, true);//verify the child is still thereassertIsFile(child);}
public void testRenameRootDirForbidden() {assumeRenameSupported();rename(path("/"), path("/test/newRootDir"), false, true, false);}
public void testRenameChildDirForbidden() {assumeRenameSupported();Path parentdir = path("/test/parentdir");fs.mkdirs(parentdir);Path childFile = new Path(parentdir, "childfile");createFile(childFile);//verify one level downPath childdir = new Path(parentdir, "childdir");rename(parentdir, childdir, false, true, false);//now another levelfs.mkdirs(childdir);Path childchilddir = new Path(childdir, "childdir");rename(parentdir, childchilddir, false, true, false);}
public void testRenameFileAndVerifyContents() {assumeRenameSupported();final Path filePath = new Path("/test/home/user/documents/file.txt");final Path newFilePath = new Path("/test/home/user/files/file.txt");mkdirs(newFilePath.getParent());int len = 1024;byte[] dataset = dataset(len, 'A', 26);writeDataset(fs, filePath, dataset, len, len, false);rename(filePath, newFilePath, true, false, true);byte[] dest = readDataset(fs, newFilePath, len);compareByteArrays(dataset, dest, len);String reread = readBytesToString(fs, newFilePath, 20);}
public void testMoveFileUnderParent() {if (!renameSupported())return;Path filepath = path("test/file");createFile(filepath);//HDFS expects rename src, src -> truerename(filepath, filepath, true, true, true);//verify the file is still thereassertIsFile(filepath);}
public void testMoveDirUnderParent() {if (!renameSupported()) {return;}Path testdir = path("test/dir");fs.mkdirs(testdir);Path parent = testdir.getParent();//the outcome here is ambiguous, so is not checkedtry {fs.rename(testdir, parent);} catch (SwiftOperationFailedException e) {}assertExists("Source directory has been deleted ", testdir);}
public void testRenameFileToSelf() {if (!renameSupported())return;Path filepath = path("test/file");createFile(filepath);//HDFS expects rename src, src -> truerename(filepath, filepath, true, true, true);//verify the file is still thereassertIsFile(filepath);}
public void testRenamedConsistence() {assumeRenameSupported();describe("verify that overwriting a file with new data doesn't impact" + " the existing content");final Path filePath = new Path("/test/home/user/documents/file.txt");final Path newFilePath = new Path("/test/home/user/files/file.txt");mkdirs(newFilePath.getParent());int len = 1024;byte[] dataset = dataset(len, 'A', 26);byte[] dataset2 = dataset(len, 'a', 26);writeDataset(fs, filePath, dataset, len, len, false);rename(filePath, newFilePath, true, false, true);SwiftTestUtils.writeAndRead(fs, filePath, dataset2, len, len, false, true);byte[] dest = readDataset(fs, newFilePath, len);compareByteArrays(dataset, dest, len);String reread = readBytesToString(fs, newFilePath, 20);}
public void testRenameMissingFile() {assumeRenameSupported();Path path = path("/test/RenameMissingFile");Path path2 = path("/test/RenameMissingFileDest");mkdirs(path("test"));rename(path, path2, false, false, false);}public RegisterNodeManagerResponseProto getProto() {if (rebuild)mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.containerTokenMasterKey != null) {builder.setContainerTokenMasterKey(convertToProtoFormat(this.containerTokenMasterKey));}if (this.nmTokenMasterKey != null) {builder.setNmTokenMasterKey(convertToProtoFormat(this.nmTokenMasterKey));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();rebuild = false;viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = RegisterNodeManagerResponseProto.newBuilder(proto);}viaProto = false;}
public MasterKey getContainerTokenMasterKey() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;if (this.containerTokenMasterKey != null) {return this.containerTokenMasterKey;}if (!p.hasContainerTokenMasterKey()) {return null;}this.containerTokenMasterKey = convertFromProtoFormat(p.getContainerTokenMasterKey());return this.containerTokenMasterKey;}
public void setContainerTokenMasterKey(MasterKey masterKey) {maybeInitBuilder();if (masterKey == null)builder.clearContainerTokenMasterKey();this.containerTokenMasterKey = masterKey;rebuild = true;}
public MasterKey getNMTokenMasterKey() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;if (this.nmTokenMasterKey != null) {return this.nmTokenMasterKey;}if (!p.hasNmTokenMasterKey()) {return null;}this.nmTokenMasterKey = convertFromProtoFormat(p.getNmTokenMasterKey());return this.nmTokenMasterKey;}
public void setNMTokenMasterKey(MasterKey masterKey) {maybeInitBuilder();if (masterKey == null)builder.clearNmTokenMasterKey();this.nmTokenMasterKey = masterKey;rebuild = true;}
public String getDiagnosticsMessage() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasDiagnosticsMessage()) {return null;}return p.getDiagnosticsMessage();}
public void setDiagnosticsMessage(String diagnosticsMessage) {maybeInitBuilder();if (diagnosticsMessage == null) {builder.clearDiagnosticsMessage();return;}builder.setDiagnosticsMessage((diagnosticsMessage));}
public String getRMVersion() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasRmVersion()) {return null;}return p.getRmVersion();}
public void setRMVersion(String rmVersion) {maybeInitBuilder();if (rmVersion == null) {builder.clearRmIdentifier();return;}builder.setRmVersion(rmVersion);}
public NodeAction getNodeAction() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasNodeAction()) {return null;}return convertFromProtoFormat(p.getNodeAction());}
public void setNodeAction(NodeAction nodeAction) {maybeInitBuilder();if (nodeAction == null) {builder.clearNodeAction();} else {builder.setNodeAction(convertToProtoFormat(nodeAction));}rebuild = true;}
public long getRMIdentifier() {RegisterNodeManagerResponseProtoOrBuilder p = viaProto ? proto : builder;return (p.getRmIdentifier());}
public void setRMIdentifier(long rmIdentifier) {maybeInitBuilder();builder.setRmIdentifier(rmIdentifier);}
private NodeAction convertFromProtoFormat(NodeActionProto p) {return NodeAction.valueOf(p.name());}
private NodeActionProto convertToProtoFormat(NodeAction t) {return NodeActionProto.valueOf(t.name());}
private MasterKeyPBImpl convertFromProtoFormat(MasterKeyProto p) {return new MasterKeyPBImpl(p);}
private MasterKeyProto convertToProtoFormat(MasterKey t) {return ((MasterKeyPBImpl) t).getProto();}public static FinishApplicationMasterRequest newInstance(FinalApplicationStatus finalAppStatus, String diagnostics, String url) {FinishApplicationMasterRequest request = Records.newRecord(FinishApplicationMasterRequest.class);request.setFinalApplicationStatus(finalAppStatus);request.setDiagnostics(diagnostics);request.setTrackingUrl(url);return request;}void createFile(int count, String compress) {conf = new Configuration();path = new Path(ROOT, outputFile + "." + compress);fs = path.getFileSystem(conf);FSDataOutputStream out = fs.create(path);Writer writer = new Writer(out, BLOCK_SIZE, compress, comparator, conf);int nx;for (nx = 0; nx < count; nx++) {byte[] key = composeSortedKey(KEY, count, nx).getBytes();byte[] value = (VALUE + nx).getBytes();writer.append(key, value);}writer.close();out.close();}
void readFile() {long fileLength = fs.getFileStatus(path).getLen();int numSplit = 10;long splitSize = fileLength / numSplit + 1;Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);long offset = 0;long rowCount = 0;BytesWritable key, value;for (int i = 0; i < numSplit; ++i, offset += splitSize) {Scanner scanner = reader.createScannerByByteRange(offset, splitSize);int count = 0;key = new BytesWritable();value = new BytesWritable();while (!scanner.atEnd()) {scanner.entry().get(key, value);++count;scanner.advance();}scanner.close();Assert.assertTrue(count > 0);rowCount += count;}Assert.assertEquals(rowCount, reader.getEntryCount());reader.close();}
void readRowSplits(int numSplits) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);long totalRecords = reader.getEntryCount();for (int i = 0; i < numSplits; i++) {long startRec = i * totalRecords / numSplits;long endRec = (i + 1) * totalRecords / numSplits;if (i == numSplits - 1) {endRec = totalRecords;}Scanner scanner = reader.createScannerByRecordNum(startRec, endRec);int count = 0;BytesWritable key = new BytesWritable();BytesWritable value = new BytesWritable();long x = startRec;while (!scanner.atEnd()) {assertEquals("Incorrect RecNum returned by scanner", scanner.getRecordNum(), x);scanner.entry().get(key, value);++count;assertEquals("Incorrect RecNum returned by scanner", scanner.getRecordNum(), x);scanner.advance();++x;}scanner.close();Assert.assertTrue(count == (endRec - startRec));}// make sure specifying range at the end gives zero records.Scanner scanner = reader.createScannerByRecordNum(totalRecords, -1);Assert.assertTrue(scanner.atEnd());}
static String composeSortedKey(String prefix, int total, int value) {return String.format("%s%010d", prefix, value);}
void checkRecNums() {long fileLen = fs.getFileStatus(path).getLen();Reader reader = new Reader(fs.open(path), fileLen, conf);long totalRecs = reader.getEntryCount();long begin = random.nextLong() % (totalRecs / 2);if (begin < 0)begin += (totalRecs / 2);long end = random.nextLong() % (totalRecs / 2);if (end < 0)end += (totalRecs / 2);end += (totalRecs / 2) + 1;assertEquals("RecNum for offset=0 should be 0", 0, reader.getRecordNumNear(0));for (long x : new long[] { fileLen, fileLen + 1, 2 * fileLen }) {assertEquals("RecNum for offset>=fileLen should be total entries", totalRecs, reader.getRecordNumNear(x));}for (long i = 0; i < 100; ++i) {assertEquals("Locaton to RecNum conversion not symmetric", i, reader.getRecordNumByLocation(reader.getLocationByRecordNum(i)));}for (long i = 1; i < 100; ++i) {long x = totalRecs - i;assertEquals("Locaton to RecNum conversion not symmetric", x, reader.getRecordNumByLocation(reader.getLocationByRecordNum(x)));}for (long i = begin; i < end; ++i) {assertEquals("Locaton to RecNum conversion not symmetric", i, reader.getRecordNumByLocation(reader.getLocationByRecordNum(i)));}for (int i = 0; i < 1000; ++i) {long x = random.nextLong() % totalRecs;if (x < 0)x += totalRecs;assertEquals("Locaton to RecNum conversion not symmetric", x, reader.getRecordNumByLocation(reader.getLocationByRecordNum(x)));}}
public void testSplit() {System.out.println("testSplit");createFile(100000, Compression.Algorithm.NONE.getName());checkRecNums();readFile();readRowSplits(10);fs.delete(path, true);createFile(500000, Compression.Algorithm.GZ.getName());checkRecNums();readFile();readRowSplits(83);fs.delete(path, true);}public XDR asReadOnlyWrap() {ByteBuffer b = buf.asReadOnlyBuffer();if (state == State.WRITING) {b.flip();}XDR n = new XDR(b, State.READING);return n;}
public ByteBuffer buffer() {return buf.duplicate();}
public int size() {// two with clear semantics.return state == State.READING ? buf.limit() : buf.position();}
public int readInt() {Preconditions.checkState(state == State.READING);return buf.getInt();}
public void writeInt(int v) {ensureFreeSpace(SIZEOF_INT);buf.putInt(v);}
public boolean readBoolean() {Preconditions.checkState(state == State.READING);return buf.getInt() != 0;}
public void writeBoolean(boolean v) {ensureFreeSpace(SIZEOF_INT);buf.putInt(v ? 1 : 0);}
public long readHyper() {Preconditions.checkState(state == State.READING);return buf.getLong();}
public void writeLongAsHyper(long v) {ensureFreeSpace(SIZEOF_LONG);buf.putLong(v);}
public byte[] readFixedOpaque(int size) {Preconditions.checkState(state == State.READING);byte[] r = new byte[size];buf.get(r);alignPosition();return r;}
public void writeFixedOpaque(byte[] src, int length) {ensureFreeSpace(alignUp(length));buf.put(src, 0, length);writePadding();}
public void writeFixedOpaque(byte[] src) {writeFixedOpaque(src, src.length);}
public byte[] readVariableOpaque() {Preconditions.checkState(state == State.READING);int size = readInt();return readFixedOpaque(size);}
public void writeVariableOpaque(byte[] src) {ensureFreeSpace(SIZEOF_INT + alignUp(src.length));buf.putInt(src.length);writeFixedOpaque(src);}
public String readString() {return new String(readVariableOpaque());}
public void writeString(String s) {writeVariableOpaque(s.getBytes());}
private void writePadding() {Preconditions.checkState(state == State.WRITING);int p = pad(buf.position());ensureFreeSpace(p);buf.put(PADDING_BYTES, 0, p);}
private int alignUp(int length) {return length + pad(length);}
private int pad(int length) {switch(length % 4) {case 1:return 3;case 2:return 2;case 3:return 1;default:return 0;}}
private void alignPosition() {buf.position(alignUp(buf.position()));}
private void ensureFreeSpace(int size) {Preconditions.checkState(state == State.WRITING);if (buf.remaining() < size) {int newCapacity = buf.capacity() * 2;int newRemaining = buf.capacity() + buf.remaining();while (newRemaining < size) {newRemaining += newCapacity;newCapacity *= 2;}ByteBuffer newbuf = ByteBuffer.allocate(newCapacity);buf.flip();newbuf.put(buf);buf = newbuf;}}
public static boolean verifyLength(XDR xdr, int len) {return xdr.buf.remaining() >= len;}
static byte[] recordMark(int size, boolean last) {byte[] b = new byte[SIZEOF_INT];ByteBuffer buf = ByteBuffer.wrap(b);buf.putInt(!last ? size : size | 0x80000000);return b;}
public static ChannelBuffer writeMessageTcp(XDR request, boolean last) {Preconditions.checkState(request.state == XDR.State.WRITING);ByteBuffer b = request.buf.duplicate();b.flip();byte[] fragmentHeader = XDR.recordMark(b.limit(), last);ByteBuffer headerBuf = ByteBuffer.wrap(fragmentHeader);// TODO: Investigate whether making a copy of the buffer is necessary.return ChannelBuffers.copiedBuffer(headerBuf, b);}
public static ChannelBuffer writeMessageUdp(XDR response) {Preconditions.checkState(response.state == XDR.State.READING);// TODO: Investigate whether making a copy of the buffer is necessary.return ChannelBuffers.copiedBuffer(response.buf);}
public static int fragmentSize(byte[] mark) {ByteBuffer b = ByteBuffer.wrap(mark);int n = b.getInt();return n & 0x7fffffff;}
public static boolean isLastFragment(byte[] mark) {ByteBuffer b = ByteBuffer.wrap(mark);int n = b.getInt();return (n & 0x80000000) != 0;}
public byte[] getBytes() {ByteBuffer d = asReadOnlyWrap().buffer();byte[] b = new byte[d.remaining()];d.get(b);return b;}public void testSplitting() {JobConf conf = new JobConf();MiniDFSCluster dfs = null;try {dfs = new MiniDFSCluster.Builder(conf).numDataNodes(4).racks(new String[] { "/rack0", "/rack0", "/rack1", "/rack1" }).hosts(new String[] { "host0", "host1", "host2", "host3" }).build();FileSystem fs = dfs.getFileSystem();Path path = getPath("/foo/bar", fs);Path path2 = getPath("/foo/baz", fs);Path path3 = getPath("/bar/bar", fs);Path path4 = getPath("/bar/baz", fs);final int numSplits = 100;MultipleInputs.addInputPath(conf, path, TextInputFormat.class, MapClass.class);MultipleInputs.addInputPath(conf, path2, TextInputFormat.class, MapClass2.class);MultipleInputs.addInputPath(conf, path3, KeyValueTextInputFormat.class, MapClass.class);MultipleInputs.addInputPath(conf, path4, TextInputFormat.class, MapClass2.class);DelegatingInputFormat inFormat = new DelegatingInputFormat();InputSplit[] splits = inFormat.getSplits(conf, numSplits);int[] bins = new int[3];for (InputSplit split : splits) {assertTrue(split instanceof TaggedInputSplit);final TaggedInputSplit tis = (TaggedInputSplit) split;int index = -1;if (tis.getInputFormatClass().equals(KeyValueTextInputFormat.class)) {// path3index = 0;} else if (tis.getMapperClass().equals(MapClass.class)) {// pathindex = 1;} else {// path2 and path4index = 2;}bins[index]++;}// regardless of the number of paths that use that Mapper/InputFormatfor (int count : bins) {assertEquals(numSplits, count);}assertTrue(true);} finally {if (dfs != null) {dfs.shutdown();}}}
static Path getPath(final String location, final FileSystem fs) {Path path = new Path(location);// create a multi-block file on hdfsDataOutputStream out = fs.create(path, true, 4096, (short) 2, 512, null);for (int i = 0; i < 1000; ++i) {out.writeChars("Hello\n");}out.close();return path;}
public void map(String key, String value, OutputCollector<String, String> output, Reporter reporter) {}
public void configure(JobConf job) {}
public void close() {}public void map(Text key, Text value, Context context) {// hung by this.new Thread() {
@Overridepublic void run() {synchronized (this) {try {wait();} catch (InterruptedException e) {}}}}.start();if (context.getTaskAttemptID().getId() == 0) {System.out.println("Attempt:" + context.getTaskAttemptID() + " Failing mapper throwing exception");throw new IOException("Attempt:" + context.getTaskAttemptID() + " Failing mapper throwing exception");} else {System.out.println("Attempt:" + context.getTaskAttemptID() + " Exiting");System.exit(-1);}}
public void run() {synchronized (this) {try {wait();} catch (InterruptedException e) {}}}public void testFence() {Assume.assumeTrue(isConfigured());Configuration conf = new Configuration();conf.set(SshFenceByTcpPort.CONF_IDENTITIES_KEY, TEST_KEYFILE);SshFenceByTcpPort fence = new SshFenceByTcpPort();fence.setConf(conf);assertTrue(fence.tryFence(TEST_TARGET, null));}
public void testConnectTimeout() {Configuration conf = new Configuration();conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);SshFenceByTcpPort fence = new SshFenceByTcpPort();fence.setConf(conf);assertFalse(fence.tryFence(UNFENCEABLE_TARGET, ""));}
public void testArgsParsing() {Args args = new SshFenceByTcpPort.Args(null);assertEquals(System.getProperty("user.name"), args.user);assertEquals(22, args.sshPort);args = new SshFenceByTcpPort.Args("");assertEquals(System.getProperty("user.name"), args.user);assertEquals(22, args.sshPort);args = new SshFenceByTcpPort.Args("12345");assertEquals("12345", args.user);assertEquals(22, args.sshPort);args = new SshFenceByTcpPort.Args(":12345");assertEquals(System.getProperty("user.name"), args.user);assertEquals(12345, args.sshPort);args = new SshFenceByTcpPort.Args("foo:2222");assertEquals("foo", args.user);assertEquals(2222, args.sshPort);}
public void testBadArgsParsing() {// No port specifiedassertBadArgs(":");// "assertBadArgs("bar.com:");// Port does not parseassertBadArgs(":xx");// "assertBadArgs("bar.com:xx");}
private void assertBadArgs(String argStr) {try {new Args(argStr);fail("Did not fail on bad args: " + argStr);} catch (BadFencingConfigurationException e) {}}
private boolean isConfigured() {return (TEST_FENCING_HOST != null && !TEST_FENCING_HOST.isEmpty()) && (TEST_FENCING_PORT != null && !TEST_FENCING_PORT.isEmpty()) && (TEST_KEYFILE != null && !TEST_KEYFILE.isEmpty());}public LocalResourceRequest getLocalResourceRequest() {return rsrc;}public static GetClusterMetricsResponse newInstance(YarnClusterMetrics metrics) {GetClusterMetricsResponse response = Records.newRecord(GetClusterMetricsResponse.class);response.setClusterMetrics(metrics);return response;}protected AbstractFSContract createContract(Configuration conf) {return new FTPContract(conf);}private void initArgs() {setCmd("test");setArgs("-" + getFlag() + "," + file);}
public void setFile(String file) {this.file = file;}
protected int postCmd(int exit_code) {exit_code = super.postCmd(exit_code);result = exit_code == 0;return exit_code;}
public boolean eval() {initArgs();execute();return result;}// this coordinates with LoggedTaskAttempt.SplitVectorKindint[][] burst() {int[][] result = new int[4][];result[WALLCLOCK_TIME_INDEX] = progressWallclockTime.getValues();result[CPU_TIME_INDEX] = progressCPUTime.getValues();result[VIRTUAL_MEMORY_KBYTES_INDEX] = progressVirtualMemoryKbytes.getValues();result[PHYSICAL_MEMORY_KBYTES_INDEX] = progressPhysicalMemoryKbytes.getValues();return result;}
public static int[] arrayGet(int[][] burstedBlock, int index) {return burstedBlock == null ? NULL_ARRAY : burstedBlock[index];}
public static int[] arrayGetWallclockTime(int[][] burstedBlock) {return arrayGet(burstedBlock, WALLCLOCK_TIME_INDEX);}
public static int[] arrayGetCPUTime(int[][] burstedBlock) {return arrayGet(burstedBlock, CPU_TIME_INDEX);}
public static int[] arrayGetVMemKbytes(int[][] burstedBlock) {return arrayGet(burstedBlock, VIRTUAL_MEMORY_KBYTES_INDEX);}
public static int[] arrayGetPhysMemKbytes(int[][] burstedBlock) {return arrayGet(burstedBlock, PHYSICAL_MEMORY_KBYTES_INDEX);}public void testStandbyIsHot() {Configuration conf = new Configuration();// We read from the standby to watch block locationsHAUtil.setAllowStandbyReads(conf, true);conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(3).build();try {cluster.waitActive();cluster.transitionToActive(0);NameNode nn1 = cluster.getNameNode(0);NameNode nn2 = cluster.getNameNode(1);FileSystem fs = HATestUtil.configureFailoverFs(cluster, conf);Thread.sleep(1000);System.err.println("==================================");DFSTestUtil.writeFile(fs, TEST_FILE_PATH, TEST_FILE_DATA);// Have to force an edit log roll so that the standby catches upnn1.getRpcServer().rollEditLog();System.err.println("==================================");// Block locations should show up on standby.LOG.info("Waiting for block locations to appear on standby node");waitForBlockLocations(cluster, nn2, TEST_FILE, 3);// that the active "trusts" all of the DNscluster.triggerHeartbeats();cluster.triggerBlockReports();// Change replicationLOG.info("Changing replication to 1");fs.setReplication(TEST_FILE_PATH, (short) 1);BlockManagerTestUtil.computeAllPendingWork(nn1.getNamesystem().getBlockManager());waitForBlockLocations(cluster, nn1, TEST_FILE, 1);nn1.getRpcServer().rollEditLog();LOG.info("Waiting for lowered replication to show up on standby");waitForBlockLocations(cluster, nn2, TEST_FILE, 1);// Change back to 3LOG.info("Changing replication to 3");fs.setReplication(TEST_FILE_PATH, (short) 3);BlockManagerTestUtil.computeAllPendingWork(nn1.getNamesystem().getBlockManager());nn1.getRpcServer().rollEditLog();LOG.info("Waiting for higher replication to show up on standby");waitForBlockLocations(cluster, nn2, TEST_FILE, 3);} finally {cluster.shutdown();}}
public void testDatanodeRestarts() {Configuration conf = new Configuration();conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 1024);// We read from the standby to watch block locationsHAUtil.setAllowStandbyReads(conf, true);conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 0);conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(1).build();try {NameNode nn0 = cluster.getNameNode(0);NameNode nn1 = cluster.getNameNode(1);cluster.transitionToActive(0);// Create 5 blocks.DFSTestUtil.createFile(cluster.getFileSystem(0), TEST_FILE_PATH, 5 * 1024, (short) 1, 1L);HATestUtil.waitForStandbyToCatchUp(nn0, nn1);// Stop the DN.DataNode dn = cluster.getDataNodes().get(0);String dnName = dn.getDatanodeId().getXferAddr();DataNodeProperties dnProps = cluster.stopDataNode(0);// Make sure both NNs register it as dead.BlockManagerTestUtil.noticeDeadDatanode(nn0, dnName);BlockManagerTestUtil.noticeDeadDatanode(nn1, dnName);BlockManagerTestUtil.updateState(nn0.getNamesystem().getBlockManager());BlockManagerTestUtil.updateState(nn1.getNamesystem().getBlockManager());assertEquals(5, nn0.getNamesystem().getUnderReplicatedBlocks());// since the SBN doesn't process replication.assertEquals(0, nn1.getNamesystem().getUnderReplicatedBlocks());LocatedBlocks locs = nn1.getRpcServer().getBlockLocations(TEST_FILE, 0, 1);assertEquals("Standby should have registered that the block has no replicas", 0, locs.get(0).getLocations().length);cluster.restartDataNode(dnProps);// Wait for both NNs to re-register the DN.cluster.waitActive(0);cluster.waitActive(1);BlockManagerTestUtil.updateState(nn0.getNamesystem().getBlockManager());BlockManagerTestUtil.updateState(nn1.getNamesystem().getBlockManager());assertEquals(0, nn0.getNamesystem().getUnderReplicatedBlocks());assertEquals(0, nn1.getNamesystem().getUnderReplicatedBlocks());locs = nn1.getRpcServer().getBlockLocations(TEST_FILE, 0, 1);assertEquals("Standby should have registered that the block has replicas again", 1, locs.get(0).getLocations().length);} finally {cluster.shutdown();}}
static void waitForBlockLocations(final MiniDFSCluster cluster, final NameNode nn, final String path, final int expectedReplicas) {GenericTestUtils.waitFor(new Supplier<Boolean>() {
@Overridepublic Boolean get() {try {LocatedBlocks locs = NameNodeAdapter.getBlockLocations(nn, path, 0, 1000);DatanodeInfo[] dnis = locs.getLastLocatedBlock().getLocations();for (DatanodeInfo dni : dnis) {Assert.assertNotNull(dni);}int numReplicas = dnis.length;LOG.info("Got " + numReplicas + " locs: " + locs);if (numReplicas > expectedReplicas) {cluster.triggerDeletionReports();}cluster.triggerHeartbeats();return numReplicas == expectedReplicas;} catch (IOException e) {LOG.warn("No block locations yet: " + e.getMessage());return false;}}}, 500, 20000);}
public Boolean get() {try {LocatedBlocks locs = NameNodeAdapter.getBlockLocations(nn, path, 0, 1000);DatanodeInfo[] dnis = locs.getLastLocatedBlock().getLocations();for (DatanodeInfo dni : dnis) {Assert.assertNotNull(dni);}int numReplicas = dnis.length;LOG.info("Got " + numReplicas + " locs: " + locs);if (numReplicas > expectedReplicas) {cluster.triggerDeletionReports();}cluster.triggerHeartbeats();return numReplicas == expectedReplicas;} catch (IOException e) {LOG.warn("No block locations yet: " + e.getMessage());return false;}}public synchronized void reserveResource(SchedulerApplicationAttempt application, Priority priority, RMContainer container) {// Check if it's already reservedRMContainer reservedContainer = getReservedContainer();if (reservedContainer != null) {// Sanity checkif (!container.getContainer().getNodeId().equals(getNodeID())) {throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());}// Cannot reserve more than one application on a given node!if (!reservedContainer.getContainer().getId().getApplicationAttemptId().equals(container.getContainer().getId().getApplicationAttemptId())) {throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);}LOG.info("Updated reserved container " + container.getContainer().getId() + " on node " + this + " for application " + application);} else {LOG.info("Reserved container " + container.getContainer().getId() + " on node " + this + " for application " + application);}setReservedContainer(container);this.reservedAppSchedulable = (FSAppAttempt) application;}
public synchronized void unreserveResource(SchedulerApplicationAttempt application) {// Cannot unreserve for wrong application...ApplicationAttemptId reservedApplication = getReservedContainer().getContainer().getId().getApplicationAttemptId();if (!reservedApplication.equals(application.getApplicationAttemptId())) {throw new IllegalStateException("Trying to unreserve " + " for application " + application.getApplicationId() + " when currently reserved " + " for application " + reservedApplication.getApplicationId() + " on node " + this);}setReservedContainer(null);this.reservedAppSchedulable = null;}
public synchronized FSAppAttempt getReservedAppSchedulable() {return reservedAppSchedulable;}protected Configuration createConfiguration() {Configuration conf = super.createConfiguration();conf.setInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, 4096);return conf;}
public void teardown() {IOUtils.closeStream(instream);instream = null;super.teardown();}
public void testOpenReadZeroByteFile() {describe("create & read a 0 byte file");Path path = path("zero.txt");touch(getFileSystem(), path);instream = getFileSystem().open(path);assertEquals(0, instream.getPos());//expect initial read to failint result = instream.read();assertMinusOne("initial byte read", result);}
public void testOpenReadDir() {describe("create & read a directory");Path path = path("zero.dir");mkdirs(path);try {instream = getFileSystem().open(path);//at this point we've opened a directoryfail("A directory has been opened for reading");} catch (FileNotFoundException e) {handleExpectedException(e);} catch (IOException e) {handleRelaxedException("opening a directory for reading", "FileNotFoundException", e);}}
public void testOpenReadDirWithChild() {describe("create & read a directory which has a child");Path path = path("zero.dir");mkdirs(path);Path path2 = new Path(path, "child");mkdirs(path2);try {instream = getFileSystem().open(path);//at this point we've opened a directoryfail("A directory has been opened for reading");} catch (FileNotFoundException e) {handleExpectedException(e);} catch (IOException e) {handleRelaxedException("opening a directory for reading", "FileNotFoundException", e);}}
public void testOpenFileTwice() {describe("verify that two opened file streams are independent");Path path = path("testopenfiletwice.txt");byte[] block = dataset(TEST_FILE_LEN, 0, 255);//this file now has a simple rule: offset => valuecreateFile(getFileSystem(), path, false, block);//open firstFSDataInputStream instream1 = getFileSystem().open(path);int c = instream1.read();assertEquals(0, c);FSDataInputStream instream2 = null;try {instream2 = getFileSystem().open(path);assertEquals("first read of instream 2", 0, instream2.read());assertEquals("second read of instream 1", 1, instream1.read());instream1.close();assertEquals("second read of instream 2", 1, instream2.read());//close instream1 againinstream1.close();} finally {IOUtils.closeStream(instream1);IOUtils.closeStream(instream2);}}
public void testSequentialRead() {describe("verify that sequential read() operations return values");Path path = path("testsequentialread.txt");int len = 4;int base = 0x40;// 64byte[] block = dataset(len, base, base + len);//this file now has a simple rule: offset => (value | 0x40)createFile(getFileSystem(), path, false, block);//open firstinstream = getFileSystem().open(path);assertEquals(base, instream.read());assertEquals(base + 1, instream.read());assertEquals(base + 2, instream.read());assertEquals(base + 3, instream.read());// and now, failuresassertEquals(-1, instream.read());assertEquals(-1, instream.read());instream.close();}public Object getDatum() {if (datum == null) {datum = new MapAttemptFinished();datum.taskid = new Utf8(attemptId.getTaskID().toString());datum.attemptId = new Utf8(attemptId.toString());datum.taskType = new Utf8(taskType.name());datum.taskStatus = new Utf8(taskStatus);datum.mapFinishTime = mapFinishTime;datum.finishTime = finishTime;datum.hostname = new Utf8(hostname);datum.port = port;if (rackName != null) {datum.rackname = new Utf8(rackName);}datum.state = new Utf8(state);datum.counters = EventWriter.toAvro(counters);datum.clockSplits = AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetWallclockTime(allSplits));datum.cpuUsages = AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetCPUTime(allSplits));datum.vMemKbytes = AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetVMemKbytes(allSplits));datum.physMemKbytes = AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetPhysMemKbytes(allSplits));}return datum;}
public void setDatum(Object oDatum) {this.datum = (MapAttemptFinished) oDatum;this.attemptId = TaskAttemptID.forName(datum.attemptId.toString());this.taskType = TaskType.valueOf(datum.taskType.toString());this.taskStatus = datum.taskStatus.toString();this.mapFinishTime = datum.mapFinishTime;this.finishTime = datum.finishTime;this.hostname = datum.hostname.toString();this.rackName = datum.rackname.toString();this.port = datum.port;this.state = datum.state.toString();this.counters = EventReader.fromAvro(datum.counters);this.clockSplits = AvroArrayUtils.fromAvro(datum.clockSplits);this.cpuUsages = AvroArrayUtils.fromAvro(datum.cpuUsages);this.vMemKbytes = AvroArrayUtils.fromAvro(datum.vMemKbytes);this.physMemKbytes = AvroArrayUtils.fromAvro(datum.physMemKbytes);}
public TaskID getTaskId() {return attemptId.getTaskID();}
public TaskAttemptID getAttemptId() {return attemptId;}
public TaskType getTaskType() {return taskType;}
public String getTaskStatus() {return taskStatus.toString();}
public long getMapFinishTime() {return mapFinishTime;}
public long getFinishTime() {return finishTime;}
public String getHostname() {return hostname.toString();}
public int getPort() {return port;}
public String getRackName() {return rackName == null ? null : rackName.toString();}
public String getState() {return state.toString();}
Counters getCounters() {return counters;}
public EventType getEventType() {return EventType.MAP_ATTEMPT_FINISHED;}
public int[] getClockSplits() {return clockSplits;}
public int[] getCpuUsages() {return cpuUsages;}
public int[] getVMemKbytes() {return vMemKbytes;}
public int[] getPhysMemKbytes() {return physMemKbytes;}public static void delete(FileSystem fs, String name) {Path dir = new Path(name);Path data = new Path(dir, MapFile.DATA_FILE_NAME);Path index = new Path(dir, MapFile.INDEX_FILE_NAME);Path bloom = new Path(dir, BLOOM_FILE_NAME);fs.delete(data, true);fs.delete(index, true);fs.delete(bloom, true);fs.delete(dir, true);}
private static byte[] byteArrayForBloomKey(DataOutputBuffer buf) {int cleanLength = buf.getLength();byte[] ba = buf.getData();if (cleanLength != ba.length) {ba = new byte[cleanLength];System.arraycopy(buf.getData(), 0, ba, 0, cleanLength);}return ba;}
private synchronized void initBloomFilter(Configuration conf) {numKeys = conf.getInt("io.mapfile.bloom.size", 1024 * 1024);// Our desired error rate is by default 0.005, i.e. 0.5%float errorRate = conf.getFloat("io.mapfile.bloom.error.rate", 0.005f);vectorSize = (int) Math.ceil((double) (-HASH_COUNT * numKeys) / Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT)));bloomFilter = new DynamicBloomFilter(vectorSize, HASH_COUNT, Hash.getHashType(conf), numKeys);}
public synchronized void append(WritableComparable key, Writable val) {super.append(key, val);buf.reset();key.write(buf);bloomKey.set(byteArrayForBloomKey(buf), 1.0);bloomFilter.add(bloomKey);}
public synchronized void close() {super.close();DataOutputStream out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);try {bloomFilter.write(out);out.flush();out.close();out = null;} finally {IOUtils.closeStream(out);}}
private void initBloomFilter(Path dirName, Configuration conf) {DataInputStream in = null;try {FileSystem fs = dirName.getFileSystem(conf);in = fs.open(new Path(dirName, BLOOM_FILE_NAME));bloomFilter = new DynamicBloomFilter();bloomFilter.readFields(in);in.close();in = null;} catch (IOException ioe) {LOG.warn("Can't open BloomFilter: " + ioe + " - fallback to MapFile.");bloomFilter = null;} finally {IOUtils.closeStream(in);}}
public boolean probablyHasKey(WritableComparable key) {if (bloomFilter == null) {return true;}buf.reset();key.write(buf);bloomKey.set(byteArrayForBloomKey(buf), 1.0);return bloomFilter.membershipTest(bloomKey);}
public synchronized Writable get(WritableComparable key, Writable val) {if (!probablyHasKey(key)) {return null;}return super.get(key, val);}
public Filter getBloomFilter() {return bloomFilter;}private static void readJunk(DataInput in, Random r, long seed, int iter) {r.setSeed(seed);for (int i = 0; i < iter; ++i) {switch(r.nextInt(7)) {case 0:assertEquals((byte) (r.nextInt() & 0xFF), in.readByte());break;case 1:assertEquals((short) (r.nextInt() & 0xFFFF), in.readShort());break;case 2:assertEquals(r.nextInt(), in.readInt());break;case 3:assertEquals(r.nextLong(), in.readLong());break;case 4:assertEquals(Double.doubleToLongBits(r.nextDouble()), Double.doubleToLongBits(in.readDouble()));break;case 5:assertEquals(Float.floatToIntBits(r.nextFloat()), Float.floatToIntBits(in.readFloat()));break;case 6:int len = r.nextInt(1024);byte[] vb = new byte[len];r.nextBytes(vb);byte[] b = new byte[len];in.readFully(b, 0, len);assertArrayEquals(vb, b);break;}}}
private static void writeJunk(DataOutput out, Random r, long seed, int iter) {r.setSeed(seed);for (int i = 0; i < iter; ++i) {switch(r.nextInt(7)) {case 0:out.writeByte(r.nextInt());break;case 1:out.writeShort((short) (r.nextInt() & 0xFFFF));break;case 2:out.writeInt(r.nextInt());break;case 3:out.writeLong(r.nextLong());break;case 4:out.writeDouble(r.nextDouble());break;case 5:out.writeFloat(r.nextFloat());break;case 6:byte[] b = new byte[r.nextInt(1024)];r.nextBytes(b);out.write(b);break;}}}
public void testBaseBuffers() {DataOutputBuffer dob = new DataOutputBuffer();Random r = new Random();long seed = r.nextLong();r.setSeed(seed);System.out.println("SEED: " + seed);writeJunk(dob, r, seed, 1000);DataInputBuffer dib = new DataInputBuffer();dib.reset(dob.getData(), 0, dob.getLength());readJunk(dib, r, seed, 1000);dob.reset();writeJunk(dob, r, seed, 1000);dib.reset(dob.getData(), 0, dob.getLength());readJunk(dib, r, seed, 1000);}
public void testByteBuffers() {DataOutputByteBuffer dob = new DataOutputByteBuffer();Random r = new Random();long seed = r.nextLong();r.setSeed(seed);System.out.println("SEED: " + seed);writeJunk(dob, r, seed, 1000);DataInputByteBuffer dib = new DataInputByteBuffer();dib.reset(dob.getData());readJunk(dib, r, seed, 1000);dob.reset();writeJunk(dob, r, seed, 1000);dib.reset(dob.getData());readJunk(dib, r, seed, 1000);}
private static byte[] toBytes(ByteBuffer[] bufs, int len) {byte[] ret = new byte[len];int pos = 0;for (int i = 0; i < bufs.length; ++i) {int rem = bufs[i].remaining();bufs[i].get(ret, pos, rem);pos += rem;}return ret;}
public void testDataOutputByteBufferCompatibility() {DataOutputBuffer dob = new DataOutputBuffer();DataOutputByteBuffer dobb = new DataOutputByteBuffer();Random r = new Random();long seed = r.nextLong();r.setSeed(seed);System.out.println("SEED: " + seed);writeJunk(dob, r, seed, 1000);writeJunk(dobb, r, seed, 1000);byte[] check = toBytes(dobb.getData(), dobb.getLength());assertEquals(check.length, dob.getLength());assertArrayEquals(check, Arrays.copyOf(dob.getData(), dob.getLength()));dob.reset();dobb.reset();writeJunk(dob, r, seed, 3000);writeJunk(dobb, r, seed, 3000);check = toBytes(dobb.getData(), dobb.getLength());assertEquals(check.length, dob.getLength());assertArrayEquals(check, Arrays.copyOf(dob.getData(), dob.getLength()));dob.reset();dobb.reset();writeJunk(dob, r, seed, 1000);writeJunk(dobb, r, seed, 1000);check = toBytes(dobb.getData(), dobb.getLength());assertEquals("Failed Checking length = " + check.length, check.length, dob.getLength());assertArrayEquals(check, Arrays.copyOf(dob.getData(), dob.getLength()));}
public void TestDataInputByteBufferCompatibility() {DataOutputBuffer dob = new DataOutputBuffer();Random r = new Random();long seed = r.nextLong();r.setSeed(seed);System.out.println("SEED: " + seed);writeJunk(dob, r, seed, 1000);ByteBuffer buf = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());DataInputByteBuffer dib = new DataInputByteBuffer();dib.reset(buf);readJunk(dib, r, seed, 1000);}
public void TestDataOutputByteBufferCompatibility() {DataOutputByteBuffer dob = new DataOutputByteBuffer();Random r = new Random();long seed = r.nextLong();r.setSeed(seed);System.out.println("SEED: " + seed);writeJunk(dob, r, seed, 1000);ByteBuffer buf = ByteBuffer.allocate(dob.getLength());for (ByteBuffer b : dob.getData()) {buf.put(b);}buf.flip();DataInputBuffer dib = new DataInputBuffer();dib.reset(buf.array(), 0, buf.remaining());readJunk(dib, r, seed, 1000);}public Token getDelegationToken() {CancelDelegationTokenRequestProtoOrBuilder p = viaProto ? proto : builder;if (this.token != null) {return this.token;}this.token = convertFromProtoFormat(p.getToken());return this.token;}
public void setDelegationToken(Token token) {maybeInitBuilder();if (token == null)builder.clearToken();this.token = token;}
public CancelDelegationTokenRequestProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (token != null) {builder.setToken(convertToProtoFormat(this.token));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = CancelDelegationTokenRequestProto.newBuilder(proto);}viaProto = false;}
private TokenPBImpl convertFromProtoFormat(TokenProto p) {return new TokenPBImpl(p);}
private TokenProto convertToProtoFormat(Token t) {return ((TokenPBImpl) t).getProto();}public void enrollAttempt(TaskAttemptStatus status, long timestamp) {}
public long attemptEnrolledTime(TaskAttemptId attemptID) {return Long.MAX_VALUE;}
public void updateAttempt(TaskAttemptStatus status, long timestamp) {}
public void contextualize(Configuration conf, AppContext context) {}
public long thresholdRuntime(TaskId id) {return Long.MAX_VALUE;}
public long estimatedRuntime(TaskAttemptId id) {return -1L;}
public long estimatedNewAttemptRuntime(TaskId id) {return -1L;}
public long runtimeEstimateVariance(TaskAttemptId id) {return -1L;}public InitReplicaRecoveryResponseProto initReplicaRecovery(RpcController unused, InitReplicaRecoveryRequestProto request) {RecoveringBlock b = PBHelper.convert(request.getBlock());ReplicaRecoveryInfo r;try {r = impl.initReplicaRecovery(b);} catch (IOException e) {throw new ServiceException(e);}if (r == null) {return InitReplicaRecoveryResponseProto.newBuilder().setReplicaFound(false).build();} else {return InitReplicaRecoveryResponseProto.newBuilder().setReplicaFound(true).setBlock(PBHelper.convert(r)).setState(PBHelper.convert(r.getOriginalReplicaState())).build();}}
public UpdateReplicaUnderRecoveryResponseProto updateReplicaUnderRecovery(RpcController unused, UpdateReplicaUnderRecoveryRequestProto request) {final String storageID;try {storageID = impl.updateReplicaUnderRecovery(PBHelper.convert(request.getBlock()), request.getRecoveryId(), request.getNewLength());} catch (IOException e) {throw new ServiceException(e);}return UpdateReplicaUnderRecoveryResponseProto.newBuilder().setStorageUuid(storageID).build();}public GetDiagnosticsRequestProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.taskAttemptId != null) {builder.setTaskAttemptId(convertToProtoFormat(this.taskAttemptId));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = GetDiagnosticsRequestProto.newBuilder(proto);}viaProto = false;}
public TaskAttemptId getTaskAttemptId() {GetDiagnosticsRequestProtoOrBuilder p = viaProto ? proto : builder;if (this.taskAttemptId != null) {return this.taskAttemptId;}if (!p.hasTaskAttemptId()) {return null;}this.taskAttemptId = convertFromProtoFormat(p.getTaskAttemptId());return this.taskAttemptId;}
public void setTaskAttemptId(TaskAttemptId taskAttemptId) {maybeInitBuilder();if (taskAttemptId == null)builder.clearTaskAttemptId();this.taskAttemptId = taskAttemptId;}
private TaskAttemptIdPBImpl convertFromProtoFormat(TaskAttemptIdProto p) {return new TaskAttemptIdPBImpl(p);}
private TaskAttemptIdProto convertToProtoFormat(TaskAttemptId t) {return ((TaskAttemptIdPBImpl) t).getProto();}public static synchronized NfsExports getInstance(Configuration conf) {if (exports == null) {String matchHosts = conf.get(CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY, CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT);int cacheSize = conf.getInt(Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_KEY, Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_DEFAULT);long expirationPeriodNano = conf.getLong(Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY, Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_DEFAULT) * 1000 * 1000;try {exports = new NfsExports(cacheSize, expirationPeriodNano, matchHosts);} catch (IllegalArgumentException e) {LOG.error("Invalid NFS Exports provided: ", e);return exports;}}return exports;}
public int hashCode() {return hostAddr.hashCode();}
public boolean equals(Object obj) {if (this == obj) {return true;}if (obj instanceof AccessCacheEntry) {AccessCacheEntry entry = (AccessCacheEntry) obj;return this.hostAddr.equals(entry.hostAddr);}return false;}
public void setNext(LinkedElement next) {this.next = next;}
public LinkedElement getNext() {return this.next;}
public void setExpirationTime(long timeNano) {}
public long getExpirationTime() {return this.expirationTime;}
public String[] getHostGroupList() {int listSize = mMatches.size();String[] hostGroups = new String[listSize];for (int i = 0; i < mMatches.size(); i++) {hostGroups[i] = mMatches.get(i).getHostGroup();}return hostGroups;}
public AccessPrivilege getAccessPrivilege(InetAddress addr) {return getAccessPrivilege(addr.getHostAddress(), addr.getCanonicalHostName());}
AccessPrivilege getAccessPrivilege(String address, String hostname) {long now = System.nanoTime();AccessCacheEntry newEntry = new AccessCacheEntry(address, AccessPrivilege.NONE, now + this.cacheExpirationPeriod);// check if there is a cache entry for the given addressAccessCacheEntry cachedEntry = accessCache.get(newEntry);if (cachedEntry != null && now < cachedEntry.expirationTime) {// get a non-expired cache entry, use itreturn cachedEntry.access;} else {for (Match match : mMatches) {if (match.isIncluded(address, hostname)) {if (match.accessPrivilege == AccessPrivilege.READ_ONLY) {newEntry.access = AccessPrivilege.READ_ONLY;break;} else if (match.accessPrivilege == AccessPrivilege.READ_WRITE) {newEntry.access = AccessPrivilege.READ_WRITE;}}}accessCache.put(newEntry);return newEntry.access;}}
public boolean isIncluded(String address, String hostname) {return true;}
public String getHostGroup() {return "*";}
public boolean isIncluded(String address, String hostname) {if (subnetInfo.isInRange(address)) {if (LOG.isDebugEnabled()) {LOG.debug("CIDRNMatcher low = " + subnetInfo.getLowAddress() + ", high = " + subnetInfo.getHighAddress() + ", allowing client '" + address + "', '" + hostname + "'");}return true;}if (LOG.isDebugEnabled()) {LOG.debug("CIDRNMatcher low = " + subnetInfo.getLowAddress() + ", high = " + subnetInfo.getHighAddress() + ", denying client '" + address + "', '" + hostname + "'");}return false;}
public String getHostGroup() {return subnetInfo.getAddress() + "/" + subnetInfo.getNetmask();}
public boolean isIncluded(String address, String hostname) {if (ipOrHost.equalsIgnoreCase(address) || ipOrHost.equalsIgnoreCase(hostname)) {if (LOG.isDebugEnabled()) {LOG.debug("ExactMatcher '" + ipOrHost + "', allowing client " + "'" + address + "', '" + hostname + "'");}return true;}if (LOG.isDebugEnabled()) {LOG.debug("ExactMatcher '" + ipOrHost + "', denying client " + "'" + address + "', '" + hostname + "'");}return false;}
public String getHostGroup() {return ipOrHost;}
public boolean isIncluded(String address, String hostname) {if (pattern.matcher(address).matches() || pattern.matcher(hostname).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("RegexMatcher '" + pattern.pattern() + "', allowing client '" + address + "', '" + hostname + "'");}return true;}if (LOG.isDebugEnabled()) {LOG.debug("RegexMatcher '" + pattern.pattern() + "', denying client '" + address + "', '" + hostname + "'");}return false;}
public String getHostGroup() {return pattern.toString();}
private static Match getMatch(String line) {String[] parts = line.split("\\s+");final String host;AccessPrivilege privilege = AccessPrivilege.READ_ONLY;switch(parts.length) {case 1:host = parts[0].toLowerCase().trim();break;case 2:host = parts[0].toLowerCase().trim();String option = parts[1].trim();if ("rw".equalsIgnoreCase(option)) {privilege = AccessPrivilege.READ_WRITE;}break;default:throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");}if (host.equals("*")) {if (LOG.isDebugEnabled()) {LOG.debug("Using match all for '" + host + "' and " + privilege);}return new AnonymousMatch(privilege);} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and " + privilege);}return new CIDRMatch(privilege, new SubnetUtils(host).getInfo());} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and " + privilege);}String[] pair = host.split("/");return new CIDRMatch(privilege, new SubnetUtils(pair[0], pair[1]).getInfo());} else if (host.contains("*") || host.contains("?") || host.contains("[") || host.contains("]")) {if (LOG.isDebugEnabled()) {LOG.debug("Using Regex match for '" + host + "' and " + privilege);}return new RegexMatch(privilege, host);} else if (HOSTNAME_FORMAT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using exact match for '" + host + "' and " + privilege);}return new ExactMatch(privilege, host);} else {throw new IllegalArgumentException("Invalid hostname provided '" + host + "'");}}public void setup() {testRootDir.mkdirs();}
public void tearDown() {if (testRootDir.exists()) {FileContext.getLocalFSFileContext().delete(new Path(testRootDir.getAbsolutePath()), true);}}
private Configuration getConfForNodeHealthScript() {Configuration conf = new Configuration();conf.set(YarnConfiguration.NM_HEALTH_CHECK_SCRIPT_PATH, nodeHealthscriptFile.getAbsolutePath());conf.setLong(YarnConfiguration.NM_HEALTH_CHECK_INTERVAL_MS, 500);conf.setLong(YarnConfiguration.NM_HEALTH_CHECK_SCRIPT_TIMEOUT_MS, 1000);return conf;}
private void writeNodeHealthScriptFile(String scriptStr, boolean setExecutable) {PrintWriter pw = null;try {FileUtil.setWritable(nodeHealthscriptFile, true);FileUtil.setReadable(nodeHealthscriptFile, true);pw = new PrintWriter(new FileOutputStream(nodeHealthscriptFile));pw.println(scriptStr);pw.flush();} finally {pw.close();}FileUtil.setExecutable(nodeHealthscriptFile, setExecutable);}
public void testNodeHealthScriptShouldRun() {// node health script path.Assert.assertFalse("By default Health script should not have started", NodeHealthScriptRunner.shouldRun(new Configuration()));Configuration conf = getConfForNodeHealthScript();// existsAssert.assertFalse("Node health script should start", NodeHealthScriptRunner.shouldRun(conf));// Create script path.conf.writeXml(new FileOutputStream(nodeHealthConfigFile));conf.addResource(nodeHealthConfigFile.getName());writeNodeHealthScriptFile("", false);// executable.Assert.assertFalse("Node health script should start", NodeHealthScriptRunner.shouldRun(conf));writeNodeHealthScriptFile("", true);Assert.assertTrue("Node health script should start", NodeHealthScriptRunner.shouldRun(conf));}
private void setHealthStatus(NodeHealthStatus healthStatus, boolean isHealthy, String healthReport, long lastHealthReportTime) {healthStatus.setHealthReport(healthReport);healthStatus.setIsNodeHealthy(isHealthy);healthStatus.setLastHealthReportTime(lastHealthReportTime);}
public void testNodeHealthScript() {RecordFactory factory = RecordFactoryProvider.getRecordFactory(null);NodeHealthStatus healthStatus = factory.newRecordInstance(NodeHealthStatus.class);String errorScript = "echo ERROR\n echo \"Tracker not healthy\"";String normalScript = "echo \"I am all fine\"";String timeOutScript = Shell.WINDOWS ? "@echo off\nping -n 4 127.0.0.1 >nul\necho \"I am fine\"" : "sleep 4\necho \"I am fine\"";Configuration conf = getConfForNodeHealthScript();conf.writeXml(new FileOutputStream(nodeHealthConfigFile));conf.addResource(nodeHealthConfigFile.getName());writeNodeHealthScriptFile(normalScript, true);NodeHealthCheckerService nodeHealthChecker = new NodeHealthCheckerService();nodeHealthChecker.init(conf);NodeHealthScriptRunner nodeHealthScriptRunner = nodeHealthChecker.getNodeHealthScriptRunner();TimerTask timerTask = nodeHealthScriptRunner.getTimerTask();timerTask.run();setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());LOG.info("Checking initial healthy condition");// Check proper report conditions.Assert.assertTrue("Node health status reported unhealthy", healthStatus.getIsNodeHealthy());Assert.assertTrue("Node health status reported unhealthy", healthStatus.getHealthReport().equals(nodeHealthChecker.getHealthReport()));// Healthy to unhealthy transitionwriteNodeHealthScriptFile(errorScript, true);// Run timertimerTask.run();// update health statussetHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());LOG.info("Checking Healthy--->Unhealthy");Assert.assertFalse("Node health status reported healthy", healthStatus.getIsNodeHealthy());Assert.assertTrue("Node health status reported healthy", healthStatus.getHealthReport().equals(nodeHealthChecker.getHealthReport()));// Check unhealthy to healthy transitions.writeNodeHealthScriptFile(normalScript, true);timerTask.run();setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());LOG.info("Checking UnHealthy--->healthy");// Check proper report conditions.Assert.assertTrue("Node health status reported unhealthy", healthStatus.getIsNodeHealthy());Assert.assertTrue("Node health status reported unhealthy", healthStatus.getHealthReport().equals(nodeHealthChecker.getHealthReport()));// Healthy to timeout transition.writeNodeHealthScriptFile(timeOutScript, true);timerTask.run();setHealthStatus(healthStatus, nodeHealthChecker.isHealthy(), nodeHealthChecker.getHealthReport(), nodeHealthChecker.getLastHealthReportTime());LOG.info("Checking Healthy--->timeout");Assert.assertFalse("Node health status reported healthy even after timeout", healthStatus.getIsNodeHealthy());Assert.assertTrue("Node script time out message not propogated", healthStatus.getHealthReport().equals(NodeHealthScriptRunner.NODE_HEALTH_SCRIPT_TIMED_OUT_MSG + NodeHealthCheckerService.SEPARATOR + nodeHealthChecker.getDiskHandler().getDisksHealthReport()));}public static AHSClient createAHSClient() {AHSClient client = new AHSClientImpl();return client;}public DelegationTokenIdentifier createIdentifier() {return new DelegationTokenIdentifier();}public void testNumberOfBlocks() {Configuration conf = new Configuration();conf.set(NativeAzureFileSystem.AZURE_BLOCK_SIZE_PROPERTY_NAME, "500");AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.createMock(conf);FileSystem fs = testAccount.getFileSystem();Path testFile = createTestFile(fs, 1200);FileStatus stat = fs.getFileStatus(testFile);assertEquals(500, stat.getBlockSize());testAccount.cleanup();}
public void testBlockLocationsTypical() {BlockLocation[] locations = getBlockLocationsOutput(210, 50, 0, 210);assertEquals(5, locations.length);assertEquals("localhost", locations[0].getHosts()[0]);assertEquals(50, locations[0].getLength());assertEquals(10, locations[4].getLength());assertEquals(100, locations[2].getOffset());}
public void testBlockLocationsEmptyFile() {BlockLocation[] locations = getBlockLocationsOutput(0, 50, 0, 0);assertEquals(0, locations.length);}
public void testBlockLocationsSmallFile() {BlockLocation[] locations = getBlockLocationsOutput(1, 50, 0, 1);assertEquals(1, locations.length);assertEquals(1, locations[0].getLength());}
public void testBlockLocationsExactBlockSizeMultiple() {BlockLocation[] locations = getBlockLocationsOutput(200, 50, 0, 200);assertEquals(4, locations.length);assertEquals(150, locations[3].getOffset());assertEquals(50, locations[3].getLength());}
public void testBlockLocationsSubsetOfFile() {BlockLocation[] locations = getBlockLocationsOutput(205, 10, 15, 35);assertEquals(4, locations.length);assertEquals(10, locations[0].getLength());assertEquals(15, locations[0].getOffset());assertEquals(5, locations[3].getLength());assertEquals(45, locations[3].getOffset());}
public void testBlockLocationsOutOfRangeSubsetOfFile() {BlockLocation[] locations = getBlockLocationsOutput(205, 10, 300, 10);assertEquals(0, locations.length);}
public void testBlockLocationsEmptySubsetOfFile() {BlockLocation[] locations = getBlockLocationsOutput(205, 10, 0, 0);assertEquals(0, locations.length);}
public void testBlockLocationsDifferentLocationHost() {BlockLocation[] locations = getBlockLocationsOutput(100, 10, 0, 100, "myblobhost");assertEquals(10, locations.length);assertEquals("myblobhost", locations[0].getHosts()[0]);}
private static BlockLocation[] getBlockLocationsOutput(int fileSize, int blockSize, long start, long len) {return getBlockLocationsOutput(fileSize, blockSize, start, len, null);}
private static BlockLocation[] getBlockLocationsOutput(int fileSize, int blockSize, long start, long len, String blockLocationHost) {Configuration conf = new Configuration();conf.set(NativeAzureFileSystem.AZURE_BLOCK_SIZE_PROPERTY_NAME, "" + blockSize);if (blockLocationHost != null) {conf.set(NativeAzureFileSystem.AZURE_BLOCK_LOCATION_HOST_PROPERTY_NAME, blockLocationHost);}AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.createMock(conf);FileSystem fs = testAccount.getFileSystem();Path testFile = createTestFile(fs, fileSize);FileStatus stat = fs.getFileStatus(testFile);BlockLocation[] locations = fs.getFileBlockLocations(stat, start, len);testAccount.cleanup();return locations;}
private static Path createTestFile(FileSystem fs, int size) {Path testFile = new Path("/testFile");OutputStream outputStream = fs.create(testFile);outputStream.write(new byte[size]);outputStream.close();return testFile;}public GetQueueInfoResponseProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
public QueueInfo getQueueInfo() {if (this.queueInfo != null) {return this.queueInfo;}GetQueueInfoResponseProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasQueueInfo()) {return null;}this.queueInfo = convertFromProtoFormat(p.getQueueInfo());return this.queueInfo;}
public void setQueueInfo(QueueInfo queueInfo) {maybeInitBuilder();if (queueInfo == null) {builder.clearQueueInfo();}this.queueInfo = queueInfo;}
private void mergeLocalToBuilder() {if (this.queueInfo != null) {builder.setQueueInfo(convertToProtoFormat(this.queueInfo));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = GetQueueInfoResponseProto.newBuilder(proto);}viaProto = false;}
private QueueInfo convertFromProtoFormat(QueueInfoProto queueInfo) {return new QueueInfoPBImpl(queueInfo);}
private QueueInfoProto convertToProtoFormat(QueueInfo queueInfo) {return ((QueueInfoPBImpl) queueInfo).getProto();}public void setup() {Configuration conf = new YarnConfiguration();cluster = MiniMRClientClusterFactory.create(this.getClass(), 2, conf);cluster.start();}
public void cleanup() {if (cluster != null) {cluster.stop();cluster = null;}}
public void testLargeSort() {String[] args = new String[0];int[] ioSortMbs = { 128, 256, 1536 };for (int ioSortMb : ioSortMbs) {Configuration conf = new Configuration(cluster.getConfig());conf.setInt(MRJobConfig.IO_SORT_MB, ioSortMb);conf.setInt(LargeSorter.NUM_MAP_TASKS, 1);conf.setInt(LargeSorter.MBS_PER_MAP, ioSortMb);assertEquals("Large sort failed for " + ioSortMb, 0, ToolRunner.run(conf, new LargeSorter(), args));}}protected void serviceInit(Configuration conf) {super.serviceInit(conf);rmPollInterval = conf.getInt(MRJobConfig.MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS, MRJobConfig.DEFAULT_MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS);}
protected void serviceStart() {scheduler = createSchedulerProxy();JobID id = TypeConverter.fromYarn(this.applicationId);JobId jobId = TypeConverter.toYarn(id);job = context.getJob(jobId);register();startAllocatorThread();super.serviceStart();}
protected AppContext getContext() {return context;}
protected Job getJob() {return job;}
protected float getApplicationProgress() {// aggregate progress.return this.job.getProgress();}
protected void register() {//RegisterInetSocketAddress serviceAddr = null;if (clientService != null) {serviceAddr = clientService.getBindAddress();}try {RegisterApplicationMasterRequest request = recordFactory.newRecordInstance(RegisterApplicationMasterRequest.class);if (serviceAddr != null) {request.setHost(serviceAddr.getHostName());request.setRpcPort(serviceAddr.getPort());request.setTrackingUrl(MRWebAppUtil.getAMWebappScheme(getConfig()) + serviceAddr.getHostName() + ":" + clientService.getHttpPort());}RegisterApplicationMasterResponse response = scheduler.registerApplicationMaster(request);isApplicationMasterRegistered = true;maxContainerCapability = response.getMaximumResourceCapability();this.context.getClusterInfo().setMaxContainerCapability(maxContainerCapability);if (UserGroupInformation.isSecurityEnabled()) {setClientToAMToken(response.getClientToAMTokenMasterKey());}this.applicationACLs = response.getApplicationACLs();LOG.info("maxContainerCapability: " + maxContainerCapability.getMemory());String queue = response.getQueue();LOG.info("queue: " + queue);job.setQueueName(queue);} catch (Exception are) {LOG.error("Exception while registering", are);throw new YarnRuntimeException(are);}}
private void setClientToAMToken(ByteBuffer clientToAMTokenMasterKey) {byte[] key = clientToAMTokenMasterKey.array();context.getClientToAMTokenSecretManager().setMasterKey(key);}
protected void unregister() {try {doUnregistration();} catch (Exception are) {LOG.error("Exception while unregistering ", are);RunningAppContext raContext = (RunningAppContext) context;raContext.resetIsLastAMRetry();}}
protected void doUnregistration() {FinalApplicationStatus finishState = FinalApplicationStatus.UNDEFINED;JobImpl jobImpl = (JobImpl) job;if (jobImpl.getInternalState() == JobStateInternal.SUCCEEDED) {finishState = FinalApplicationStatus.SUCCEEDED;} else if (jobImpl.getInternalState() == JobStateInternal.KILLED || (jobImpl.getInternalState() == JobStateInternal.RUNNING && isSignalled)) {finishState = FinalApplicationStatus.KILLED;} else if (jobImpl.getInternalState() == JobStateInternal.FAILED || jobImpl.getInternalState() == JobStateInternal.ERROR) {finishState = FinalApplicationStatus.FAILED;}StringBuffer sb = new StringBuffer();for (String s : job.getDiagnostics()) {sb.append(s).append("\n");}LOG.info("Setting job diagnostics to " + sb.toString());String historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(getConfig(), context.getApplicationID());LOG.info("History url is " + historyUrl);FinishApplicationMasterRequest request = FinishApplicationMasterRequest.newInstance(finishState, sb.toString(), historyUrl);try {while (true) {FinishApplicationMasterResponse response = scheduler.finishApplicationMaster(request);if (response.getIsUnregistered()) {// final states.RunningAppContext raContext = (RunningAppContext) context;raContext.markSuccessfulUnregistration();break;}LOG.info("Waiting for application to be successfully unregistered.");Thread.sleep(rmPollInterval);}} catch (ApplicationMasterNotRegisteredException e) {register();doUnregistration();}}
protected Resource getMaxContainerCapability() {return maxContainerCapability;}
protected void serviceStop() {if (stopped.getAndSet(true)) {// return if already stoppedreturn;}if (allocatorThread != null) {allocatorThread.interrupt();try {allocatorThread.join();} catch (InterruptedException ie) {LOG.warn("InterruptedException while stopping", ie);}}if (isApplicationMasterRegistered && shouldUnregister) {unregister();}super.serviceStop();}
protected void startAllocatorThread() {allocatorThread = new Thread(new Runnable() {
@Overridepublic void run() {while (!stopped.get() && !Thread.currentThread().isInterrupted()) {try {Thread.sleep(rmPollInterval);try {heartbeat();} catch (YarnRuntimeException e) {LOG.error("Error communicating with RM: " + e.getMessage(), e);return;} catch (Exception e) {LOG.error("ERROR IN CONTACTING RM. ", e);continue;}lastHeartbeatTime = context.getClock().getTime();executeHeartbeatCallbacks();} catch (InterruptedException e) {if (!stopped.get()) {LOG.warn("Allocated thread interrupted. Returning.");}return;}}}});allocatorThread.setName("RMCommunicator Allocator");allocatorThread.start();}
public void run() {while (!stopped.get() && !Thread.currentThread().isInterrupted()) {try {Thread.sleep(rmPollInterval);try {heartbeat();} catch (YarnRuntimeException e) {LOG.error("Error communicating with RM: " + e.getMessage(), e);return;} catch (Exception e) {LOG.error("ERROR IN CONTACTING RM. ", e);continue;}lastHeartbeatTime = context.getClock().getTime();executeHeartbeatCallbacks();} catch (InterruptedException e) {if (!stopped.get()) {LOG.warn("Allocated thread interrupted. Returning.");}return;}}}
protected ApplicationMasterProtocol createSchedulerProxy() {final Configuration conf = getConfig();try {return ClientRMProxy.createRMProxy(conf, ApplicationMasterProtocol.class);} catch (IOException e) {throw new YarnRuntimeException(e);}}
private void executeHeartbeatCallbacks() {Runnable callback = null;while ((callback = heartbeatCallbacks.poll()) != null) {callback.run();}}
public long getLastHeartbeatTime() {return lastHeartbeatTime;}
public void runOnNextHeartbeat(Runnable callback) {heartbeatCallbacks.add(callback);}
public void setShouldUnregister(boolean shouldUnregister) {this.shouldUnregister = shouldUnregister;LOG.info("RMCommunicator notified that shouldUnregistered is: " + shouldUnregister);}
public void setSignalled(boolean isSignalled) {this.isSignalled = isSignalled;LOG.info("RMCommunicator notified that iSignalled is: " + isSignalled);}
protected boolean isApplicationMasterRegistered() {return isApplicationMasterRegistered;}public void init(FilterConfig filterConfig) {String configPrefix = filterConfig.getInitParameter(CONFIG_PREFIX);configPrefix = (configPrefix != null) ? configPrefix + "." : "";config = getConfiguration(configPrefix, filterConfig);String authHandlerName = config.getProperty(AUTH_TYPE, null);String authHandlerClassName;if (authHandlerName == null) {throw new ServletException("Authentication type must be specified: " + PseudoAuthenticationHandler.TYPE + "|" + KerberosAuthenticationHandler.TYPE + "|<class>");}if (authHandlerName.toLowerCase(Locale.ENGLISH).equals(PseudoAuthenticationHandler.TYPE)) {authHandlerClassName = PseudoAuthenticationHandler.class.getName();} else if (authHandlerName.toLowerCase(Locale.ENGLISH).equals(KerberosAuthenticationHandler.TYPE)) {authHandlerClassName = KerberosAuthenticationHandler.class.getName();} else {authHandlerClassName = authHandlerName;}try {Class<?> klass = Thread.currentThread().getContextClassLoader().loadClass(authHandlerClassName);authHandler = (AuthenticationHandler) klass.newInstance();authHandler.init(config);} catch (ClassNotFoundException ex) {throw new ServletException(ex);} catch (InstantiationException ex) {throw new ServletException(ex);} catch (IllegalAccessException ex) {throw new ServletException(ex);}validity = Long.parseLong(config.getProperty(AUTH_TOKEN_VALIDITY, "36000")) * //10 hours1000;secretProvider = (SignerSecretProvider) filterConfig.getServletContext().getAttribute(SIGNATURE_PROVIDER_ATTRIBUTE);if (secretProvider == null) {String signerSecretProviderClassName = config.getProperty(configPrefix + SIGNER_SECRET_PROVIDER_CLASS, null);if (signerSecretProviderClassName == null) {String signatureSecret = config.getProperty(configPrefix + SIGNATURE_SECRET, null);if (signatureSecret != null) {secretProvider = new StringSignerSecretProvider(signatureSecret);} else {secretProvider = new RandomSignerSecretProvider();randomSecret = true;}} else {try {Class<?> klass = Thread.currentThread().getContextClassLoader().loadClass(signerSecretProviderClassName);secretProvider = (SignerSecretProvider) klass.newInstance();customSecretProvider = true;} catch (ClassNotFoundException ex) {throw new ServletException(ex);} catch (InstantiationException ex) {throw new ServletException(ex);} catch (IllegalAccessException ex) {throw new ServletException(ex);}}try {secretProvider.init(config, validity);} catch (Exception ex) {throw new ServletException(ex);}} else {customSecretProvider = true;}signer = new Signer(secretProvider);cookieDomain = config.getProperty(COOKIE_DOMAIN, null);cookiePath = config.getProperty(COOKIE_PATH, null);}
protected Properties getConfiguration() {return config;}
protected AuthenticationHandler getAuthenticationHandler() {return authHandler;}
protected boolean isRandomSecret() {return randomSecret;}
protected boolean isCustomSignerSecretProvider() {return customSecretProvider;}
protected long getValidity() {return validity / 1000;}
protected String getCookieDomain() {return cookieDomain;}
protected String getCookiePath() {return cookiePath;}
public void destroy() {if (authHandler != null) {authHandler.destroy();authHandler = null;}if (secretProvider != null) {secretProvider.destroy();}}
protected Properties getConfiguration(String configPrefix, FilterConfig filterConfig) {Properties props = new Properties();Enumeration<?> names = filterConfig.getInitParameterNames();while (names.hasMoreElements()) {String name = (String) names.nextElement();if (name.startsWith(configPrefix)) {String value = filterConfig.getInitParameter(name);props.put(name.substring(configPrefix.length()), value);}}return props;}
protected String getRequestURL(HttpServletRequest request) {StringBuffer sb = request.getRequestURL();if (request.getQueryString() != null) {sb.append("?").append(request.getQueryString());}return sb.toString();}
protected AuthenticationToken getToken(HttpServletRequest request) {AuthenticationToken token = null;String tokenStr = null;Cookie[] cookies = request.getCookies();if (cookies != null) {for (Cookie cookie : cookies) {if (cookie.getName().equals(AuthenticatedURL.AUTH_COOKIE)) {tokenStr = cookie.getValue();try {tokenStr = signer.verifyAndExtract(tokenStr);} catch (SignerException ex) {throw new AuthenticationException(ex);}break;}}}if (tokenStr != null) {token = AuthenticationToken.parse(tokenStr);if (!token.getType().equals(authHandler.getType())) {throw new AuthenticationException("Invalid AuthenticationToken type");}if (token.isExpired()) {throw new AuthenticationException("AuthenticationToken expired");}}return token;}
public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) {boolean unauthorizedResponse = true;int errCode = HttpServletResponse.SC_UNAUTHORIZED;AuthenticationException authenticationEx = null;HttpServletRequest httpRequest = (HttpServletRequest) request;HttpServletResponse httpResponse = (HttpServletResponse) response;boolean isHttps = "https".equals(httpRequest.getScheme());try {boolean newToken = false;AuthenticationToken token;try {token = getToken(httpRequest);} catch (AuthenticationException ex) {LOG.warn("AuthenticationToken ignored: " + ex.getMessage());authenticationEx = ex;token = null;}if (authHandler.managementOperation(token, httpRequest, httpResponse)) {if (token == null) {if (LOG.isDebugEnabled()) {LOG.debug("Request [{}] triggering authentication", getRequestURL(httpRequest));}token = authHandler.authenticate(httpRequest, httpResponse);if (token != null && token.getExpires() != 0 && token != AuthenticationToken.ANONYMOUS) {token.setExpires(System.currentTimeMillis() + getValidity() * 1000);}newToken = true;}if (token != null) {unauthorizedResponse = false;if (LOG.isDebugEnabled()) {LOG.debug("Request [{}] user [{}] authenticated", getRequestURL(httpRequest), token.getUserName());}final AuthenticationToken authToken = token;httpRequest = new HttpServletRequestWrapper(httpRequest) {
@Overridepublic String getAuthType() {return authToken.getType();}
@Overridepublic String getRemoteUser() {return authToken.getUserName();}
@Overridepublic Principal getUserPrincipal() {return (authToken != AuthenticationToken.ANONYMOUS) ? authToken : null;}};if (newToken && !token.isExpired() && token != AuthenticationToken.ANONYMOUS) {String signedToken = signer.sign(token.toString());createAuthCookie(httpResponse, signedToken, getCookieDomain(), getCookiePath(), token.getExpires(), isHttps);}doFilter(filterChain, httpRequest, httpResponse);}} else {unauthorizedResponse = false;}} catch (AuthenticationException ex) {errCode = HttpServletResponse.SC_FORBIDDEN;authenticationEx = ex;LOG.warn("Authentication exception: " + ex.getMessage(), ex);}if (unauthorizedResponse) {if (!httpResponse.isCommitted()) {createAuthCookie(httpResponse, "", getCookieDomain(), getCookiePath(), 0, isHttps);if (authenticationEx == null) {httpResponse.sendError(errCode, "Authentication required");} else {httpResponse.sendError(errCode, authenticationEx.getMessage());}}}}
public String getAuthType() {return authToken.getType();}
public String getRemoteUser() {return authToken.getUserName();}
public Principal getUserPrincipal() {return (authToken != AuthenticationToken.ANONYMOUS) ? authToken : null;}
protected void doFilter(FilterChain filterChain, HttpServletRequest request, HttpServletResponse response) {filterChain.doFilter(request, response);}
public static void createAuthCookie(HttpServletResponse resp, String token, String domain, String path, long expires, boolean isSecure) {StringBuilder sb = new StringBuilder(AuthenticatedURL.AUTH_COOKIE).append("=");if (token != null && token.length() > 0) {sb.append("\"").append(token).append("\"");}sb.append("; Version=1");if (path != null) {sb.append("; Path=").append(path);}if (domain != null) {sb.append("; Domain=").append(domain);}if (expires >= 0) {Date date = new Date(expires);SimpleDateFormat df = new SimpleDateFormat("EEE, " + "dd-MMM-yyyy HH:mm:ss zzz");df.setTimeZone(TimeZone.getTimeZone("GMT"));sb.append("; Expires=").append(df.format(date));}if (isSecure) {sb.append("; Secure");}sb.append("; HttpOnly");resp.addHeader("Set-Cookie", sb.toString());}public void startUpCluster() {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPL_FACTOR).build();fs = (DistributedFileSystem) cluster.getFileSystem();}
public synchronized void shutDownCluster() {if (cluster != null)cluster.shutdown();}
public void pipeline_04() {final String METHOD_NAME = GenericTestUtils.getMethodName();if (LOG.isDebugEnabled()) {LOG.debug("Running " + METHOD_NAME);}final PipelinesTestUtil.PipelinesTest pipst = (PipelinesTestUtil.PipelinesTest) PipelinesTestUtil.initTest();pipst.fiCallSetNumBytes.set(new PipelinesTestUtil.ReceivedCheckAction(METHOD_NAME));pipst.fiCallSetBytesAcked.set(new PipelinesTestUtil.AckedCheckAction(METHOD_NAME));Path filePath = new Path("/" + METHOD_NAME + ".dat");FSDataOutputStream fsOut = fs.create(filePath);TestPipelines.writeData(fsOut, 2);fs.close();}
public void pipeline_05() {final String METHOD_NAME = GenericTestUtils.getMethodName();if (LOG.isDebugEnabled()) {LOG.debug("Running " + METHOD_NAME);}final PipelinesTestUtil.PipelinesTest pipst = (PipelinesTestUtil.PipelinesTest) PipelinesTestUtil.initTest();pipst.fiCallSetNumBytes.set(new PipelinesTestUtil.ReceivedCheckAction(METHOD_NAME));pipst.fiCallSetBytesAcked.set(new PipelinesTestUtil.AckedCheckAction(METHOD_NAME));Path filePath = new Path("/" + METHOD_NAME + ".dat");FSDataOutputStream fsOut = fs.create(filePath);for (int i = 0; i < 17; i++) {TestPipelines.writeData(fsOut, 23);}fs.close();}
public void pipeline_06() {final String METHOD_NAME = GenericTestUtils.getMethodName();final int MAX_PACKETS = 80;if (LOG.isDebugEnabled()) {LOG.debug("Running " + METHOD_NAME);}final PipelinesTestUtil.PipelinesTest pipst = (PipelinesTestUtil.PipelinesTest) PipelinesTestUtil.initTest();// This is ack. suspend testpipst.setSuspend(true);Path filePath = new Path("/" + METHOD_NAME + ".dat");FSDataOutputStream fsOut = fs.create(filePath);int cnt = 0;try {// because what it's gonna do has BLOCKING effect on datanodes QueueChecker cq = new QueueChecker(pipst, MAX_PACKETS);cq.start();// The actual logic is expressed in DFSClient#computePacketChunkSizeint bytesToSend = 700;while (cnt < 100 && pipst.getSuspend()) {if (LOG.isDebugEnabled()) {LOG.debug("_06(): " + cnt++ + " sending another " + bytesToSend + " bytes");}TestPipelines.writeData(fsOut, bytesToSend);}} catch (Exception e) {LOG.warn("Getting unexpected exception: ", e);}if (LOG.isDebugEnabled()) {LOG.debug("Last queued packet number " + pipst.getLastQueued());}assertTrue("Shouldn't be able to send more than 81 packet", pipst.getLastQueued() <= 81);}
public void run() {while (!done) {if (LOG.isDebugEnabled()) {LOG.debug("_06: checking for the limit " + test.getLastQueued() + " and " + MAX);}if (test.getLastQueued() >= MAX) {if (LOG.isDebugEnabled()) {LOG.debug("FI: Resume packets acking");}//Do not suspend ack sending any moretest.setSuspend(false);done = true;}if (!done)try {if (LOG.isDebugEnabled()) {LOG.debug("_06: MAX isn't reached yet. Current=" + test.getLastQueued());}sleep(100);} catch (InterruptedException e) {}}assertTrue("Shouldn't be able to send more than 81 packet", test.getLastQueued() <= 81);try {if (LOG.isDebugEnabled()) {LOG.debug("_06: shutting down the cluster");}// Which is where the waiting is happening.if (cluster != null)shutDownCluster();} catch (Exception e) {e.printStackTrace();}if (LOG.isDebugEnabled()) {LOG.debug("End QueueChecker thread");}}
private static void setConfiguration() {conf = new Configuration();int customPerChecksumSize = 700;int customBlockSize = customPerChecksumSize * 3;conf.setInt(DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY, 100);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, customPerChecksumSize);conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, customBlockSize);conf.setInt(DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY, customBlockSize / 2);conf.setInt(DFSConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY, 0);}
private static void initLoggers() {((Log4JLogger) NameNode.stateChangeLog).getLogger().setLevel(Level.ALL);((Log4JLogger) LogFactory.getLog(FSNamesystem.class)).getLogger().setLevel(Level.ALL);((Log4JLogger) DataNode.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) TestFiPipelines.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) DFSClient.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) FiTestUtil.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) BlockReceiverAspects.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) DFSClientAspects.LOG).getLogger().setLevel(Level.ALL);}protected void setUp() {super.setUp();conf = createJobConf();fs = getFileSystem();}
protected void tearDown() {fs.delete(new Path(TEST_ROOT_DIR), true);super.tearDown();}
public void cleanupJob(JobContext context) {System.err.println("---- HERE ----");Path outputPath = FileOutputFormat.getOutputPath(context);FileSystem fs = outputPath.getFileSystem(context.getConfiguration());fs.create(new Path(outputPath, CUSTOM_CLEANUP_FILE_NAME)).close();}
public void abortJob(JobContext context, JobStatus.State state) {Path outputPath = FileOutputFormat.getOutputPath(context);FileSystem fs = outputPath.getFileSystem(context.getConfiguration());String fileName = (state.equals(JobStatus.State.FAILED)) ? ABORT_FAILED_FILE_NAME : ABORT_KILLED_FILE_NAME;fs.create(new Path(outputPath, fileName)).close();}
private Path getNewOutputDir() {return new Path(TEST_ROOT_DIR, "output-" + outDirs++);}
public synchronized OutputCommitter getOutputCommitter(TaskAttemptContext context) {if (committer == null) {Path output = getOutputPath(context);committer = new CommitterWithCustomAbort(output, context);}return committer;}
public synchronized OutputCommitter getOutputCommitter(TaskAttemptContext context) {if (committer == null) {Path output = getOutputPath(context);committer = new CommitterWithCustomDeprecatedCleanup(output, context);}return committer;}
private void testSuccessfulJob(String filename, Class<? extends OutputFormat> output, String[] exclude) {Path outDir = getNewOutputDir();Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 0);job.setOutputFormatClass(output);assertTrue("Job failed!", job.waitForCompletion(true));Path testFile = new Path(outDir, filename);assertTrue("Done file missing for job " + job.getJobID(), fs.exists(testFile));// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for successful job " + job.getJobID(), fs.exists(file));}}
private void testFailedJob(String fileName, Class<? extends OutputFormat> output, String[] exclude) {Path outDir = getNewOutputDir();Job job = MapReduceTestUtil.createFailJob(conf, outDir, inDir);job.setOutputFormatClass(output);assertFalse("Job did not fail!", job.waitForCompletion(true));if (fileName != null) {Path testFile = new Path(outDir, fileName);assertTrue("File " + testFile + " missing for failed job " + job.getJobID(), fs.exists(testFile));}// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for failed job " + job.getJobID(), fs.exists(file));}}
private void testKilledJob(String fileName, Class<? extends OutputFormat> output, String[] exclude) {Path outDir = getNewOutputDir();Job job = MapReduceTestUtil.createKillJob(conf, outDir, inDir);job.setOutputFormatClass(output);job.submit();// wait for the setup to be completedwhile (job.setupProgress() != 1.0f) {UtilsForTests.waitFor(100);}// kill the jobjob.killJob();assertFalse("Job did not get kill", job.waitForCompletion(true));if (fileName != null) {Path testFile = new Path(outDir, fileName);assertTrue("File " + testFile + " missing for job " + job.getJobID(), fs.exists(testFile));}// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for killed job " + job.getJobID(), fs.exists(file));}}
public void testDefaultCleanupAndAbort() {// check with a successful jobtestSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, TextOutputFormat.class, new String[] {});// check with a failed jobtestFailedJob(null, TextOutputFormat.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });// check default abort job killtestKilledJob(null, TextOutputFormat.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });}
public void testCustomAbort() {// check with a successful jobtestSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { ABORT_FAILED_FILE_NAME, ABORT_KILLED_FILE_NAME });// check with a failed jobtestFailedJob(ABORT_FAILED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_KILLED_FILE_NAME });// check with a killed jobtestKilledJob(ABORT_KILLED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_FAILED_FILE_NAME });}
public void testCustomCleanup() {// check with a successful jobtestSuccessfulJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] {});// check with a failed jobtestFailedJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });// check with a killed jobtestKilledJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });}public void testGoodClassOrNull() {String NAME = "ClassWithNoPackage";ClassLoader cl = TestClassWithNoPackage.class.getClassLoader();String JAR = JarFinder.getJar(cl.loadClass(NAME));// Add testjob jar file to classpath.Configuration conf = new Configuration();conf.setClassLoader(new URLClassLoader(new URL[] { new URL("file", null, JAR) }, null));// Get class with no package name.String defaultPackage = this.getClass().getPackage().getName();Class c = StreamUtil.goodClassOrNull(conf, NAME, defaultPackage);assertNotNull("Class " + NAME + " not found!", c);}
public static void main(String[] args) {new TestClassWithNoPackage().testGoodClassOrNull();}public void setConf(Configuration conf) {this.conf = conf;final Class<? extends Random> klass = conf.getClass(HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY, OsSecureRandom.class, Random.class);try {random = ReflectionUtils.newInstance(klass, conf);} catch (Exception e) {LOG.info("Unable to use " + klass.getName() + ".  Falling back to " + "Java SecureRandom.", e);this.random = new SecureRandom();}}
protected void finalize() {try {Closeable r = (Closeable) this.random;r.close();} catch (ClassCastException e) {}super.finalize();}
public Configuration getConf() {return conf;}
public Encryptor createEncryptor() {return new OpensslAesCtrCipher(OpensslCipher.ENCRYPT_MODE);}
public Decryptor createDecryptor() {return new OpensslAesCtrCipher(OpensslCipher.DECRYPT_MODE);}
public void generateSecureRandom(byte[] bytes) {random.nextBytes(bytes);}
public void init(byte[] key, byte[] iv) {Preconditions.checkNotNull(key);Preconditions.checkNotNull(iv);contextReset = false;cipher.init(mode, key, iv);}
public void encrypt(ByteBuffer inBuffer, ByteBuffer outBuffer) {process(inBuffer, outBuffer);}
public void decrypt(ByteBuffer inBuffer, ByteBuffer outBuffer) {process(inBuffer, outBuffer);}
private void process(ByteBuffer inBuffer, ByteBuffer outBuffer) {try {int inputSize = inBuffer.remaining();// OpensslCipher#update will maintain crypto context.int n = cipher.update(inBuffer, outBuffer);if (n < inputSize) {/**   * Typically code will not get here. OpensslCipher#update will* consume all input data and put result in outBuffer.* OpensslCipher#doFinal will reset the crypto context.   */contextReset = true;cipher.doFinal(outBuffer);}} catch (Exception e) {throw new IOException(e);}}
public boolean isContextReset() {return contextReset;}public static long now() {return System.currentTimeMillis();}
public static long monotonicNow() {final long NANOSECONDS_PER_MILLISECOND = 1000000;return System.nanoTime() / NANOSECONDS_PER_MILLISECOND;}static FsPermission checkPermission(FileSystem fs, String path, FsPermission expected) {FileStatus s = fs.getFileStatus(new Path(path));LOG.info(s.getPath() + ": " + s.isDirectory() + " " + s.getPermission() + ":" + s.getOwner() + ":" + s.getGroup());if (expected != null) {assertEquals(expected, s.getPermission());assertEquals(expected.toShort(), s.getPermission().toShort());}return s.getPermission();}
public void testBackwardCompatibility() {// FSPermission.setUMask() APIFsPermission perm = new FsPermission((short) 18);Configuration conf = new Configuration();FsPermission.setUMask(conf, perm);assertEquals(18, FsPermission.getUMask(conf).toShort());// umask value should be handledperm = new FsPermission((short) 18);conf = new Configuration();conf.set(FsPermission.DEPRECATED_UMASK_LABEL, "18");assertEquals(18, FsPermission.getUMask(conf).toShort());// Test 3 - old configuration key overrides the new oneconf = new Configuration();conf.set(FsPermission.DEPRECATED_UMASK_LABEL, "18");conf.set(FsPermission.UMASK_LABEL, "000");assertEquals(18, FsPermission.getUMask(conf).toShort());// Test 4 - new configuration key is handledconf = new Configuration();conf.set(FsPermission.UMASK_LABEL, "022");assertEquals(18, FsPermission.getUMask(conf).toShort());}
public void testCreate() {Configuration conf = new HdfsConfiguration();conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);conf.set(FsPermission.UMASK_LABEL, "000");MiniDFSCluster cluster = null;FileSystem fs = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();cluster.waitActive();fs = FileSystem.get(conf);FsPermission rootPerm = checkPermission(fs, "/", null);FsPermission inheritPerm = FsPermission.createImmutable((short) (rootPerm.toShort() | 0300));FsPermission dirPerm = new FsPermission((short) 0777);fs.mkdirs(new Path("/a1/a2/a3"), dirPerm);checkPermission(fs, "/a1", dirPerm);checkPermission(fs, "/a1/a2", dirPerm);checkPermission(fs, "/a1/a2/a3", dirPerm);dirPerm = new FsPermission((short) 0123);FsPermission permission = FsPermission.createImmutable((short) (dirPerm.toShort() | 0300));fs.mkdirs(new Path("/aa/1/aa/2/aa/3"), dirPerm);checkPermission(fs, "/aa/1", permission);checkPermission(fs, "/aa/1/aa/2", permission);checkPermission(fs, "/aa/1/aa/2/aa/3", dirPerm);FsPermission filePerm = new FsPermission((short) 0444);Path p = new Path("/b1/b2/b3.txt");FSDataOutputStream out = fs.create(p, filePerm, true, conf.getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), fs.getDefaultReplication(p), fs.getDefaultBlockSize(p), null);out.write(123);out.close();checkPermission(fs, "/b1", inheritPerm);checkPermission(fs, "/b1/b2", inheritPerm);checkPermission(fs, "/b1/b2/b3.txt", filePerm);conf.set(FsPermission.UMASK_LABEL, "022");permission = FsPermission.createImmutable((short) 0666);FileSystem.mkdirs(fs, new Path("/c1"), new FsPermission(permission));FileSystem.create(fs, new Path("/c1/c2.txt"), new FsPermission(permission));checkPermission(fs, "/c1", permission);checkPermission(fs, "/c1/c2.txt", permission);} finally {try {if (fs != null)fs.close();} catch (Exception e) {LOG.error(StringUtils.stringifyException(e));}try {if (cluster != null)cluster.shutdown();} catch (Exception e) {LOG.error(StringUtils.stringifyException(e));}}}
public void testFilePermission() {final Configuration conf = new HdfsConfiguration();conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();cluster.waitActive();try {FileSystem nnfs = FileSystem.get(conf);// test permissions on files that do not existassertFalse(nnfs.exists(CHILD_FILE1));try {nnfs.setOwner(CHILD_FILE1, "foo", "bar");assertTrue(false);} catch (java.io.FileNotFoundException e) {LOG.info("GOOD: got " + e);}try {nnfs.setPermission(CHILD_FILE1, new FsPermission((short) 0777));assertTrue(false);} catch (java.io.FileNotFoundException e) {LOG.info("GOOD: got " + e);}// permission umask applied)FSDataOutputStream out = nnfs.create(CHILD_FILE1, new FsPermission((short) 0777), true, 1024, (short) 1, 1024, null);FileStatus status = nnfs.getFileStatus(CHILD_FILE1);// FS_PERMISSIONS_UMASK_DEFAULT is 0022assertTrue(status.getPermission().toString().equals("rwxr-xr-x"));nnfs.delete(CHILD_FILE1, false);// following dir/file creations are legalnnfs.mkdirs(CHILD_DIR1);out = nnfs.create(CHILD_FILE1);status = nnfs.getFileStatus(CHILD_FILE1);assertTrue(status.getPermission().toString().equals("rw-r--r--"));byte data[] = new byte[FILE_LEN];RAN.nextBytes(data);out.write(data);out.close();nnfs.setPermission(CHILD_FILE1, new FsPermission("700"));status = nnfs.getFileStatus(CHILD_FILE1);assertTrue(status.getPermission().toString().equals("rwx------"));// following read is legalbyte dataIn[] = new byte[FILE_LEN];FSDataInputStream fin = nnfs.open(CHILD_FILE1);int bytesRead = fin.read(dataIn);assertTrue(bytesRead == FILE_LEN);for (int i = 0; i < FILE_LEN; i++) {assertEquals(data[i], dataIn[i]);}// test execution bit support for filesnnfs.setPermission(CHILD_FILE1, new FsPermission("755"));status = nnfs.getFileStatus(CHILD_FILE1);assertTrue(status.getPermission().toString().equals("rwxr-xr-x"));nnfs.setPermission(CHILD_FILE1, new FsPermission("744"));status = nnfs.getFileStatus(CHILD_FILE1);assertTrue(status.getPermission().toString().equals("rwxr--r--"));nnfs.setPermission(CHILD_FILE1, new FsPermission("700"));// test illegal file/dir creationUserGroupInformation userGroupInfo = UserGroupInformation.createUserForTesting(USER_NAME, GROUP_NAMES);FileSystem userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);// this user does not throw an exception.userfs.mkdirs(CHILD_DIR1);// illegal mkdirassertTrue(!canMkdirs(userfs, CHILD_DIR2));// illegal file creationassertTrue(!canCreate(userfs, CHILD_FILE2));// illegal file openassertTrue(!canOpen(userfs, CHILD_FILE1));nnfs.setPermission(ROOT_PATH, new FsPermission((short) 0755));nnfs.setPermission(CHILD_DIR1, new FsPermission("777"));nnfs.setPermission(new Path("/"), new FsPermission((short) 0777));final Path RENAME_PATH = new Path("/foo/bar");userfs.mkdirs(RENAME_PATH);assertTrue(canRename(userfs, RENAME_PATH, CHILD_DIR1));} finally {cluster.shutdown();}}
static boolean canMkdirs(FileSystem fs, Path p) {try {fs.mkdirs(p);return true;} catch (AccessControlException e) {Path parent = p.getParent();assertTrue(parent.isUriPathAbsolute());assertTrue(e.getMessage().contains(parent.toString()));return false;}}
static boolean canCreate(FileSystem fs, Path p) {try {fs.create(p);return true;} catch (AccessControlException e) {Path parent = p.getParent();assertTrue(parent.isUriPathAbsolute());assertTrue(e.getMessage().contains(parent.toString()));return false;}}
static boolean canOpen(FileSystem fs, Path p) {try {fs.open(p);return true;} catch (AccessControlException e) {assertTrue(p.isUriPathAbsolute());assertTrue(e.getMessage().contains(p.toString()));return false;}}
static boolean canRename(FileSystem fs, Path src, Path dst) {try {fs.rename(src, dst);return true;} catch (AccessControlException e) {Path parent = dst.getParent();assertTrue(parent.isUriPathAbsolute());assertTrue(e.getMessage().contains(parent.toString()));return false;}}public void testOutputOnlyKeys() {args.add("-jobconf");args.add("stream.reduce.input" + "=keyonlytext");args.add("-jobconf");args.add("stream.reduce.output" + "=keyonlytext");super.testCommandLine();}
public String getExpectedOutput() {return outputExpect.replaceAll("\t", "");}
public void testCommandLine() {}public synchronized ApplicationResourceUsageReportProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (this.usedResources != null && !((ResourcePBImpl) this.usedResources).getProto().equals(builder.getUsedResources())) {builder.setUsedResources(convertToProtoFormat(this.usedResources));}if (this.reservedResources != null && !((ResourcePBImpl) this.reservedResources).getProto().equals(builder.getReservedResources())) {builder.setReservedResources(convertToProtoFormat(this.reservedResources));}if (this.neededResources != null && !((ResourcePBImpl) this.neededResources).getProto().equals(builder.getNeededResources())) {builder.setNeededResources(convertToProtoFormat(this.neededResources));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private synchronized void maybeInitBuilder() {if (viaProto || builder == null) {builder = ApplicationResourceUsageReportProto.newBuilder(proto);}viaProto = false;}
public synchronized int getNumUsedContainers() {ApplicationResourceUsageReportProtoOrBuilder p = viaProto ? proto : builder;return (p.getNumUsedContainers());}
public synchronized void setNumUsedContainers(int num_containers) {maybeInitBuilder();builder.setNumUsedContainers((num_containers));}
public synchronized int getNumReservedContainers() {ApplicationResourceUsageReportProtoOrBuilder p = viaProto ? proto : builder;return (p.getNumReservedContainers());}
public synchronized void setNumReservedContainers(int num_reserved_containers) {maybeInitBuilder();builder.setNumReservedContainers((num_reserved_containers));}
public synchronized Resource getUsedResources() {ApplicationResourceUsageReportProtoOrBuilder p = viaProto ? proto : builder;if (this.usedResources != null) {return this.usedResources;}if (!p.hasUsedResources()) {return null;}this.usedResources = convertFromProtoFormat(p.getUsedResources());return this.usedResources;}
public synchronized void setUsedResources(Resource resources) {maybeInitBuilder();if (resources == null)builder.clearUsedResources();this.usedResources = resources;}
public synchronized Resource getReservedResources() {ApplicationResourceUsageReportProtoOrBuilder p = viaProto ? proto : builder;if (this.reservedResources != null) {return this.reservedResources;}if (!p.hasReservedResources()) {return null;}this.reservedResources = convertFromProtoFormat(p.getReservedResources());return this.reservedResources;}
public synchronized void setReservedResources(Resource reserved_resources) {maybeInitBuilder();if (reserved_resources == null)builder.clearReservedResources();this.reservedResources = reserved_resources;}
public synchronized Resource getNeededResources() {ApplicationResourceUsageReportProtoOrBuilder p = viaProto ? proto : builder;if (this.neededResources != null) {return this.neededResources;}if (!p.hasNeededResources()) {return null;}this.neededResources = convertFromProtoFormat(p.getNeededResources());return this.neededResources;}
public synchronized void setNeededResources(Resource reserved_resources) {maybeInitBuilder();if (reserved_resources == null)builder.clearNeededResources();this.neededResources = reserved_resources;}
private ResourcePBImpl convertFromProtoFormat(ResourceProto p) {return new ResourcePBImpl(p);}
private ResourceProto convertToProtoFormat(Resource t) {return ((ResourcePBImpl) t).getProto();}public static void testSetUp() {conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 2);cluster = new MiniDFSCluster.Builder(conf).build();fs = cluster.getFileSystem();fc = FileContext.getFileContext(cluster.getURI(0), conf);dfsClient = new DFSClient(NameNode.getAddress(conf), conf);file1 = new Path("filestatus.dat");DFSTestUtil.createFile(fs, file1, fileSize, fileSize, blockSize, (short) 1, seed);}
public static void testTearDown() {fs.close();cluster.shutdown();}
private void checkFile(FileSystem fileSys, Path name, int repl) {DFSTestUtil.waitReplication(fileSys, name, (short) repl);}
public void testGetFileInfo() {// Check that / existsPath path = new Path("/");assertTrue("/ should be a directory", fs.getFileStatus(path).isDirectory());// Make sure getFileInfo returns null for files which do not existHdfsFileStatus fileInfo = dfsClient.getFileInfo("/noSuchFile");assertEquals("Non-existant file should result in null", null, fileInfo);Path path1 = new Path("/name1");Path path2 = new Path("/name1/name2");assertTrue(fs.mkdirs(path1));FSDataOutputStream out = fs.create(path2, false);out.close();fileInfo = dfsClient.getFileInfo(path1.toString());assertEquals(1, fileInfo.getChildrenNum());fileInfo = dfsClient.getFileInfo(path2.toString());assertEquals(0, fileInfo.getChildrenNum());// Test getFileInfo throws the right exception given a non-absolute path.try {dfsClient.getFileInfo("non-absolute");fail("getFileInfo for a non-absolute path did not throw IOException");} catch (RemoteException re) {assertTrue("Wrong exception for invalid file name", re.toString().contains("Invalid file name"));}}
public void testGetFileStatusOnFile() {checkFile(fs, file1, 1);// test getFileStatus on a fileFileStatus status = fs.getFileStatus(file1);assertFalse(file1 + " should be a file", status.isDirectory());assertEquals(blockSize, status.getBlockSize());assertEquals(1, status.getReplication());assertEquals(fileSize, status.getLen());assertEquals(file1.makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString(), status.getPath().toString());}
public void testListStatusOnFile() {FileStatus[] stats = fs.listStatus(file1);assertEquals(1, stats.length);FileStatus status = stats[0];assertFalse(file1 + " should be a file", status.isDirectory());assertEquals(blockSize, status.getBlockSize());assertEquals(1, status.getReplication());assertEquals(fileSize, status.getLen());assertEquals(file1.makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString(), status.getPath().toString());RemoteIterator<FileStatus> itor = fc.listStatus(file1);status = itor.next();assertEquals(stats[0], status);assertFalse(file1 + " should be a file", status.isDirectory());}
public void testGetFileStatusOnNonExistantFileDir() {Path dir = new Path("/test/mkdirs");try {fs.listStatus(dir);fail("listStatus of non-existent path should fail");} catch (FileNotFoundException fe) {assertEquals("File " + dir + " does not exist.", fe.getMessage());}try {fc.listStatus(dir);fail("listStatus of non-existent path should fail");} catch (FileNotFoundException fe) {assertEquals("File " + dir + " does not exist.", fe.getMessage());}try {fs.getFileStatus(dir);fail("getFileStatus of non-existent path should fail");} catch (FileNotFoundException fe) {assertTrue("Exception doesn't indicate non-existant path", fe.getMessage().startsWith("File does not exist"));}}
public void testGetFileStatusOnDir() {Path dir = new Path("/test/mkdirs");assertTrue("mkdir failed", fs.mkdirs(dir));assertTrue("mkdir failed", fs.exists(dir));FileStatus status = fs.getFileStatus(dir);assertTrue(dir + " should be a directory", status.isDirectory());assertTrue(dir + " should be zero size ", status.getLen() == 0);assertEquals(dir.makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString(), status.getPath().toString());// test listStatus on an empty directoryFileStatus[] stats = fs.listStatus(dir);assertEquals(dir + " should be empty", 0, stats.length);assertEquals(dir + " should be zero size ", 0, fs.getContentSummary(dir).getLength());RemoteIterator<FileStatus> itor = fc.listStatus(dir);assertFalse(dir + " should be empty", itor.hasNext());// create another file that is smaller than a block.Path file2 = new Path(dir, "filestatus2.dat");DFSTestUtil.createFile(fs, file2, blockSize / 4, blockSize / 4, blockSize, (short) 1, seed);checkFile(fs, file2, 1);// verify file attributesstatus = fs.getFileStatus(file2);assertEquals(blockSize, status.getBlockSize());assertEquals(1, status.getReplication());file2 = fs.makeQualified(file2);assertEquals(file2.toString(), status.getPath().toString());// Create another file in the same directoryPath file3 = new Path(dir, "filestatus3.dat");DFSTestUtil.createFile(fs, file3, blockSize / 4, blockSize / 4, blockSize, (short) 1, seed);checkFile(fs, file3, 1);file3 = fs.makeQualified(file3);// of the two filesfinal int expected = blockSize / 2;assertEquals(dir + " size should be " + expected, expected, fs.getContentSummary(dir).getLength());// Test listStatus on a non-empty directorystats = fs.listStatus(dir);assertEquals(dir + " should have two entries", 2, stats.length);assertEquals(file2.toString(), stats[0].getPath().toString());assertEquals(file3.toString(), stats[1].getPath().toString());itor = fc.listStatus(dir);assertEquals(file2.toString(), itor.next().getPath().toString());assertEquals(file3.toString(), itor.next().getPath().toString());assertFalse("Unexpected addtional file", itor.hasNext());// Test iterative listing. Now dir has 2 entries, create one more.Path dir3 = fs.makeQualified(new Path(dir, "dir3"));fs.mkdirs(dir3);dir3 = fs.makeQualified(dir3);stats = fs.listStatus(dir);assertEquals(dir + " should have three entries", 3, stats.length);assertEquals(dir3.toString(), stats[0].getPath().toString());assertEquals(file2.toString(), stats[1].getPath().toString());assertEquals(file3.toString(), stats[2].getPath().toString());itor = fc.listStatus(dir);assertEquals(dir3.toString(), itor.next().getPath().toString());assertEquals(file2.toString(), itor.next().getPath().toString());assertEquals(file3.toString(), itor.next().getPath().toString());assertFalse("Unexpected addtional file", itor.hasNext());// Now dir has 3 entries, create two morePath dir4 = fs.makeQualified(new Path(dir, "dir4"));fs.mkdirs(dir4);dir4 = fs.makeQualified(dir4);Path dir5 = fs.makeQualified(new Path(dir, "dir5"));fs.mkdirs(dir5);dir5 = fs.makeQualified(dir5);stats = fs.listStatus(dir);assertEquals(dir + " should have five entries", 5, stats.length);assertEquals(dir3.toString(), stats[0].getPath().toString());assertEquals(dir4.toString(), stats[1].getPath().toString());assertEquals(dir5.toString(), stats[2].getPath().toString());assertEquals(file2.toString(), stats[3].getPath().toString());assertEquals(file3.toString(), stats[4].getPath().toString());itor = fc.listStatus(dir);assertEquals(dir3.toString(), itor.next().getPath().toString());assertEquals(dir4.toString(), itor.next().getPath().toString());assertEquals(dir5.toString(), itor.next().getPath().toString());assertEquals(file2.toString(), itor.next().getPath().toString());assertEquals(file3.toString(), itor.next().getPath().toString());assertFalse(itor.hasNext());fs.delete(dir, true);}public Thread createReaderThread() {return new ReplayReaderThread("ReplayJobFactory");}
public void update(Statistics.ClusterStats item) {}
public void run() {try {startFlag.await();if (Thread.currentThread().isInterrupted()) {return;}final long initTime = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);LOG.info("START REPLAY @ " + initTime);long first = -1;long last = -1;while (!Thread.currentThread().isInterrupted()) {try {final JobStory job = getNextJobFiltered();if (null == job) {return;}if (first < 0) {first = job.getSubmissionTime();}final long current = job.getSubmissionTime();if (current < last) {LOG.warn("Job " + job.getJobID() + " out of order");continue;}last = current;submitter.add(jobCreator.createGridmixJob(conf, initTime + Math.round(rateFactor * (current - first)), job, scratch, userResolver.getTargetUgi(UserGroupInformation.createRemoteUser(job.getUser())), sequence.getAndIncrement()));} catch (IOException e) {error = e;return;}}} catch (InterruptedException e) {} finally {IOUtils.cleanup(null, jobProducer);}}
public void start() {this.rThread.start();}public void cleanup() {cluster.shutdown();}
private void waitForDatanodeState(String nodeID, boolean alive, int waitTime) {long stopTime = Time.now() + waitTime;FSNamesystem namesystem = cluster.getNamesystem();String state = alive ? "alive" : "dead";while (Time.now() < stopTime) {final DatanodeDescriptor dd = BlockManagerTestUtil.getDatanode(namesystem, nodeID);if (dd.isAlive == alive) {LOG.info("datanode " + nodeID + " is " + state);return;}LOG.info("Waiting for datanode " + nodeID + " to become " + state);Thread.sleep(1000);}throw new TimeoutException("Timedout waiting for datanode reach state " + state);}
public void testDeadDatanode() {Configuration conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, 500);conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1L);cluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();String poolId = cluster.getNamesystem().getBlockPoolId();DataNode dn = cluster.getDataNodes().get(0);DatanodeRegistration reg = DataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(0), poolId);waitForDatanodeState(reg.getDatanodeUuid(), true, 20000);dn.shutdown();waitForDatanodeState(reg.getDatanodeUuid(), false, 20000);DatanodeProtocol dnp = cluster.getNameNodeRpc();ReceivedDeletedBlockInfo[] blocks = { new ReceivedDeletedBlockInfo(new Block(0), ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK, null) };StorageReceivedDeletedBlocks[] storageBlocks = { new StorageReceivedDeletedBlocks(reg.getDatanodeUuid(), blocks) };try {dnp.blockReceivedAndDeleted(reg, poolId, storageBlocks);fail("Expected IOException is not thrown");} catch (IOException ex) {}// Ensure blockReport from dead datanode is rejected with IOExceptionStorageBlockReport[] report = { new StorageBlockReport(new DatanodeStorage(reg.getDatanodeUuid()), new long[] { 0L, 0L, 0L }) };try {dnp.blockReport(reg, poolId, report);fail("Expected IOException is not thrown");} catch (IOException ex) {}// that asks datanode to register againStorageReport[] rep = { new StorageReport(new DatanodeStorage(reg.getDatanodeUuid()), false, 0, 0, 0, 0) };DatanodeCommand[] cmd = dnp.sendHeartbeat(reg, rep, 0L, 0L, 0, 0, 0).getCommands();assertEquals(1, cmd.length);assertEquals(cmd[0].getAction(), RegisterCommand.REGISTER.getAction());}public HTML<WebAppProxyServlet._> html() {return new HTML<WebAppProxyServlet._>("html", null, EnumSet.of(EOpt.ENDTAG));}
private static void notFound(HttpServletResponse resp, String message) {resp.setStatus(HttpServletResponse.SC_NOT_FOUND);resp.setContentType(MimeType.HTML);Page p = new Page(resp.getWriter());p.html().h1(message)._();}
private static void warnUserPage(HttpServletResponse resp, String link, String user, ApplicationId id) {//having first visited this page then this page will still be displayed resp.addCookie(makeCheckCookie(id, false));resp.setContentType(MimeType.HTML);Page p = new Page(resp.getWriter());p.html().h1("WARNING: The following page may not be safe!").h3()._("click ").a(link, "here")._(" to continue to an Application Master web interface owned by ", user)._()._();}
private static void proxyLink(HttpServletRequest req, HttpServletResponse resp, URI link, Cookie c, String proxyHost) {org.apache.commons.httpclient.URI uri = new org.apache.commons.httpclient.URI(link.toString(), false);HttpClientParams params = new HttpClientParams();params.setCookiePolicy(CookiePolicy.BROWSER_COMPATIBILITY);params.setBooleanParameter(HttpClientParams.ALLOW_CIRCULAR_REDIRECTS, true);HttpClient client = new HttpClient(params);// similar could cause issues otherwise.HostConfiguration config = new HostConfiguration();InetAddress localAddress = InetAddress.getByName(proxyHost);if (LOG.isDebugEnabled()) {LOG.debug("local InetAddress for proxy host: " + localAddress.toString());}config.setLocalAddress(localAddress);HttpMethod method = new GetMethod(uri.getEscapedURI());@SuppressWarnings("unchecked") Enumeration<String> names = req.getHeaderNames();while (names.hasMoreElements()) {String name = names.nextElement();if (passThroughHeaders.contains(name)) {String value = req.getHeader(name);LOG.debug("REQ HEADER: " + name + " : " + value);method.setRequestHeader(name, value);}}String user = req.getRemoteUser();if (user != null && !user.isEmpty()) {method.setRequestHeader("Cookie", PROXY_USER_COOKIE_NAME + "=" + URLEncoder.encode(user, "ASCII"));}OutputStream out = resp.getOutputStream();try {resp.setStatus(client.executeMethod(config, method));for (Header header : method.getResponseHeaders()) {resp.setHeader(header.getName(), header.getValue());}if (c != null) {resp.addCookie(c);}InputStream in = method.getResponseBodyAsStream();if (in != null) {IOUtils.copyBytes(in, out, 4096, true);}} finally {method.releaseConnection();}}
private static String getCheckCookieName(ApplicationId id) {return "checked_" + id;}
private static Cookie makeCheckCookie(ApplicationId id, boolean isSet) {Cookie c = new Cookie(getCheckCookieName(id), String.valueOf(isSet));c.setPath(ProxyUriUtils.getPath(id));//2 hours in secondsc.setMaxAge(60 * 60 * 2);return c;}
private boolean isSecurityEnabled() {Boolean b = (Boolean) getServletContext().getAttribute(WebAppProxy.IS_SECURITY_ENABLED_ATTRIBUTE);if (b != null)return b;return false;}
private ApplicationReport getApplicationReport(ApplicationId id) {return ((AppReportFetcher) getServletContext().getAttribute(WebAppProxy.FETCHER_ATTRIBUTE)).getApplicationReport(id);}
private String getProxyHost() {return ((String) getServletContext().getAttribute(WebAppProxy.PROXY_HOST_ATTRIBUTE));}
protected void doGet(HttpServletRequest req, HttpServletResponse resp) {try {String userApprovedParamS = req.getParameter(ProxyUriUtils.PROXY_APPROVAL_PARAM);boolean userWasWarned = false;boolean userApproved = (userApprovedParamS != null && Boolean.valueOf(userApprovedParamS));boolean securityEnabled = isSecurityEnabled();final String remoteUser = req.getRemoteUser();final String pathInfo = req.getPathInfo();String parts[] = pathInfo.split("/", 3);if (parts.length < 2) {LOG.warn(remoteUser + " Gave an invalid proxy path " + pathInfo);notFound(resp, "Your path appears to be formatted incorrectly.");return;}//parts[0] is empty because path info always starts with a /String appId = parts[1];String rest = parts.length > 2 ? parts[2] : "";ApplicationId id = Apps.toAppID(appId);if (id == null) {LOG.warn(req.getRemoteUser() + " Attempting to access " + appId + " that is invalid");notFound(resp, appId + " appears to be formatted incorrectly.");return;}if (securityEnabled) {String cookieName = getCheckCookieName(id);Cookie[] cookies = req.getCookies();if (cookies != null) {for (Cookie c : cookies) {if (cookieName.equals(c.getName())) {userWasWarned = true;userApproved = userApproved || Boolean.valueOf(c.getValue());break;}}}}boolean checkUser = securityEnabled && (!userWasWarned || !userApproved);ApplicationReport applicationReport = null;try {applicationReport = getApplicationReport(id);} catch (ApplicationNotFoundException e) {applicationReport = null;}if (applicationReport == null) {LOG.warn(req.getRemoteUser() + " Attempting to access " + id + " that was not found");URI toFetch = ProxyUriUtils.getUriFromTrackingPlugins(id, this.trackingUriPlugins);if (toFetch != null) {resp.sendRedirect(resp.encodeRedirectURL(toFetch.toString()));return;}notFound(resp, "Application " + appId + " could not be found, " + "please try the history server");return;}String original = applicationReport.getOriginalTrackingUrl();URI trackingUri = null;// fallback to ResourceManager's app page if no tracking URI providedif (original == null || original.equals("N/A")) {resp.sendRedirect(resp.encodeRedirectURL(StringHelper.pjoin(rmAppPageUrlBase, id.toString())));return;} else {if (ProxyUriUtils.getSchemeFromUrl(original).isEmpty()) {trackingUri = ProxyUriUtils.getUriFromAMUrl(WebAppUtils.getHttpSchemePrefix(conf), original);} else {trackingUri = new URI(original);}}String runningUser = applicationReport.getUser();if (checkUser && !runningUser.equals(remoteUser)) {LOG.info("Asking " + remoteUser + " if they want to connect to the " + "app master GUI of " + appId + " owned by " + runningUser);warnUserPage(resp, ProxyUriUtils.getPathAndQuery(id, rest, req.getQueryString(), true), runningUser, id);return;}URI toFetch = new URI(trackingUri.getScheme(), trackingUri.getAuthority(), StringHelper.ujoin(trackingUri.getPath(), rest), req.getQueryString(), null);LOG.info(req.getRemoteUser() + " is accessing unchecked " + toFetch + " which is the app master GUI of " + appId + " owned by " + runningUser);switch(applicationReport.getYarnApplicationState()) {case KILLED:case FINISHED:case FAILED:resp.sendRedirect(resp.encodeRedirectURL(toFetch.toString()));return;}Cookie c = null;if (userWasWarned && userApproved) {c = makeCheckCookie(id, true);}proxyLink(req, resp, toFetch, c, getProxyHost());} catch (URISyntaxException e) {throw new IOException(e);} catch (YarnException e) {throw new IOException(e);}}public static String send4LetterWord(String host, int port, String cmd) {Socket sock = new Socket(host, port);BufferedReader reader = null;try {OutputStream outstream = sock.getOutputStream();outstream.write(cmd.getBytes());outstream.flush();// this replicates NC - close the output stream before readingsock.shutdownOutput();reader = new BufferedReader(new InputStreamReader(sock.getInputStream()));StringBuilder sb = new StringBuilder();String line;while ((line = reader.readLine()) != null) {sb.append(line + "\n");}return sb.toString();} finally {sock.close();if (reader != null) {reader.close();}}}
public static boolean waitForServerDown(String hp, long timeout) {long start = System.currentTimeMillis();while (true) {try {String host = hp.split(":")[0];int port = Integer.parseInt(hp.split(":")[1]);send4LetterWord(host, port, "stat");} catch (IOException e) {return true;}if (System.currentTimeMillis() > start + timeout) {break;}try {Thread.sleep(250);} catch (InterruptedException e) {}}return false;}
public static boolean waitForServerUp(String hp, long timeout) {long start = System.currentTimeMillis();while (true) {try {String host = hp.split(":")[0];int port = Integer.parseInt(hp.split(":")[1]);// if there are multiple hostports, just take the first oneString result = send4LetterWord(host, port, "stat");if (result.startsWith("Zookeeper version:")) {return true;}} catch (IOException e) {}if (System.currentTimeMillis() > start + timeout) {break;}try {Thread.sleep(250);} catch (InterruptedException e) {}}return false;}
public static File createTmpDir(File parentDir) {File tmpFile = File.createTempFile("test", ".junit", parentDir);// a tmpDir with a duplicate nameFile tmpDir = new File(tmpFile + ".dir");Assert.assertFalse(tmpDir.exists());Assert.assertTrue(tmpDir.mkdirs());return tmpDir;}
public void setUp() {System.setProperty("zookeeper.preAllocSize", "100");FileTxnLog.setPreallocSize(100 * 1024);if (!BASETEST.exists()) {BASETEST.mkdirs();}File dataDir = createTmpDir(BASETEST);zks = new ZooKeeperServer(dataDir, dataDir, 3000);final int PORT = Integer.parseInt(hostPort.split(":")[1]);if (factory == null) {factory = new NIOServerCnxnFactory();factory.configure(new InetSocketAddress(PORT), maxCnxns);}factory.startup(zks);Assert.assertTrue("waiting for server up", waitForServerUp("127.0.0.1:" + PORT, CONNECTION_TIMEOUT));}
public void tearDown() {if (zks != null) {ZKDatabase zkDb = zks.getZKDatabase();factory.shutdown();try {zkDb.close();} catch (IOException ie) {}final int PORT = Integer.parseInt(hostPort.split(":")[1]);Assert.assertTrue("waiting for server down", waitForServerDown("127.0.0.1:" + PORT, CONNECTION_TIMEOUT));}}
public void testzkClient() {test("/some/test");}
private void test(String testClient) {ZKClient client = new ZKClient(hostPort);client.registerService("/nodemanager", "hostPort");client.unregisterService("/nodemanager");}protected byte[] generateNewSecret() {return Long.toString(rand.nextLong()).getBytes();}public final String reconfigureProperty(String property, String newVal) {if (isPropertyReconfigurable(property)) {LOG.info("changing property " + property + " to " + newVal);String oldVal;synchronized (getConf()) {oldVal = getConf().get(property);reconfigurePropertyImpl(property, newVal);if (newVal != null) {getConf().set(property, newVal);} else {getConf().unset(property);}}return oldVal;} else {throw new ReconfigurationException(property, newVal, getConf().get(property));}}
public boolean isPropertyReconfigurable(String property) {return getReconfigurableProperties().contains(property);}public static HttpServer2 createTestServer() {prepareTestWebapp();return createServer(TEST);}
public static HttpServer2 createTestServer(Configuration conf) {prepareTestWebapp();return createServer(TEST, conf);}
public static HttpServer2 createTestServer(Configuration conf, AccessControlList adminsAcl) {prepareTestWebapp();return createServer(TEST, conf, adminsAcl);}
public static HttpServer2 createTestServer(Configuration conf, String[] pathSpecs) {prepareTestWebapp();return createServer(TEST, conf, pathSpecs);}
protected static void prepareTestWebapp() {String webapps = System.getProperty(TEST_BUILD_WEBAPPS, BUILD_WEBAPPS_DIR);File testWebappDir = new File(webapps + File.separatorChar + TEST);try {if (!testWebappDir.exists()) {fail("Test webapp dir " + testWebappDir.getCanonicalPath() + " missing");}} catch (IOException e) {}}
public static HttpServer2 createServer(String host, int port) {prepareTestWebapp();return new HttpServer2.Builder().setName(TEST).addEndpoint(URI.create("http://" + host + ":" + port)).setFindPort(true).build();}
public static HttpServer2 createServer(String webapp) {return localServerBuilder(webapp).setFindPort(true).build();}
public static HttpServer2 createServer(String webapp, Configuration conf) {return localServerBuilder(webapp).setFindPort(true).setConf(conf).build();}
public static HttpServer2 createServer(String webapp, Configuration conf, AccessControlList adminsAcl) {return localServerBuilder(webapp).setFindPort(true).setConf(conf).setACL(adminsAcl).build();}
private static Builder localServerBuilder(String webapp) {return new HttpServer2.Builder().setName(webapp).addEndpoint(URI.create("http://localhost:0"));}
public static HttpServer2 createServer(String webapp, Configuration conf, String[] pathSpecs) {return localServerBuilder(webapp).setFindPort(true).setConf(conf).setPathSpec(pathSpecs).build();}
public static HttpServer2 createAndStartTestServer() {HttpServer2 server = createTestServer();server.start();return server;}
public static void stop(HttpServer2 server) {if (server != null) {server.stop();}}
public static URL getServerURL(HttpServer2 server) {assertNotNull("No server", server);return new URL("http://" + NetUtils.getHostPortString(server.getConnectorAddress(0)));}
protected static String readOutput(URL url) {StringBuilder out = new StringBuilder();InputStream in = url.openConnection().getInputStream();byte[] buffer = new byte[64 * 1024];int len = in.read(buffer);while (len > 0) {out.append(new String(buffer, 0, len));len = in.read(buffer);}return out.toString();}public static ApplicationAttemptFinishData newInstance(ApplicationAttemptId appAttemptId, String diagnosticsInfo, String trackingURL, FinalApplicationStatus finalApplicationStatus, YarnApplicationAttemptState yarnApplicationAttemptState) {ApplicationAttemptFinishData appAttemptFD = Records.newRecord(ApplicationAttemptFinishData.class);appAttemptFD.setApplicationAttemptId(appAttemptId);appAttemptFD.setDiagnosticsInfo(diagnosticsInfo);appAttemptFD.setTrackingURL(trackingURL);appAttemptFD.setFinalApplicationStatus(finalApplicationStatus);appAttemptFD.setYarnApplicationAttemptState(yarnApplicationAttemptState);return appAttemptFD;}public KillTaskResponseProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = KillTaskResponseProto.newBuilder(proto);}viaProto = false;}public String getType() {return type;}
public void setType(String type) {this.type = type;}
public Object getContent() {return content;}
public void setContent(Object content) {this.content = content;}public static String getFileIdPath(FileHandle handle) {return getFileIdPath(handle.getFileId());}
public static String getFileIdPath(long fileId) {return INODEID_PATH_PREFIX + fileId;}
public static HdfsFileStatus getFileStatus(DFSClient client, String fileIdPath) {return client.getFileLinkInfo(fileIdPath);}
public static Nfs3FileAttributes getNfs3FileAttrFromFileStatus(HdfsFileStatus fs, IdUserGroup iug) {/** * Some 32bit Linux client has problem with 64bit fileId: it seems the 32bit * client takes only the lower 32bit of the fileId and treats it as signed * int. When the 32th bit is 1, the client considers it invalid. */NfsFileType fileType = fs.isDir() ? NfsFileType.NFSDIR : NfsFileType.NFSREG;fileType = fs.isSymlink() ? NfsFileType.NFSLNK : fileType;return new Nfs3FileAttributes(fileType, fs.getChildrenNum(), fs.getPermission().toShort(), iug.getUidAllowingUnknown(fs.getOwner()), iug.getGidAllowingUnknown(fs.getGroup()), fs.getLen(), 0, /* fsid */fs.getFileId(), fs.getModificationTime(), fs.getAccessTime());}
public static Nfs3FileAttributes getFileAttr(DFSClient client, String fileIdPath, IdUserGroup iug) {HdfsFileStatus fs = getFileStatus(client, fileIdPath);return fs == null ? null : getNfs3FileAttrFromFileStatus(fs, iug);}
public static WccAttr getWccAttr(DFSClient client, String fileIdPath) {HdfsFileStatus fstat = getFileStatus(client, fileIdPath);if (fstat == null) {return null;}long size = fstat.isDir() ? Nfs3FileAttributes.getDirSize(fstat.getChildrenNum()) : fstat.getLen();return new WccAttr(size, new NfsTime(fstat.getModificationTime()), new NfsTime(fstat.getModificationTime()));}
public static WccAttr getWccAttr(Nfs3FileAttributes attr) {return attr == null ? new WccAttr() : new WccAttr(attr.getSize(), attr.getMtime(), attr.getCtime());}
public static WccData createWccData(final WccAttr preOpAttr, DFSClient dfsClient, final String fileIdPath, final IdUserGroup iug) {Nfs3FileAttributes postOpDirAttr = getFileAttr(dfsClient, fileIdPath, iug);return new WccData(preOpAttr, postOpDirAttr);}
public static void writeChannel(Channel channel, XDR out, int xid) {if (channel == null) {RpcProgramNfs3.LOG.info("Null channel should only happen in tests. Do nothing.");return;}if (RpcProgramNfs3.LOG.isDebugEnabled()) {RpcProgramNfs3.LOG.debug(WRITE_RPC_END + xid);}ChannelBuffer outBuf = XDR.writeMessageTcp(out, true);channel.write(outBuf);}
public static void writeChannelCommit(Channel channel, XDR out, int xid) {if (RpcProgramNfs3.LOG.isDebugEnabled()) {RpcProgramNfs3.LOG.debug("Commit done:" + xid);}ChannelBuffer outBuf = XDR.writeMessageTcp(out, true);channel.write(outBuf);}
private static boolean isSet(int access, int bits) {return (access & bits) == bits;}
public static int getAccessRights(int mode, int type) {int rtn = 0;if (isSet(mode, Nfs3Constant.ACCESS_MODE_READ)) {rtn |= Nfs3Constant.ACCESS3_READ;// LOOKUP is only meaningful for dirif (type == NfsFileType.NFSDIR.toValue()) {rtn |= Nfs3Constant.ACCESS3_LOOKUP;}}if (isSet(mode, Nfs3Constant.ACCESS_MODE_WRITE)) {rtn |= Nfs3Constant.ACCESS3_MODIFY;rtn |= Nfs3Constant.ACCESS3_EXTEND;// parent dir op permissionrtn |= Nfs3Constant.ACCESS3_DELETE;}if (isSet(mode, Nfs3Constant.ACCESS_MODE_EXECUTE)) {if (type == NfsFileType.NFSREG.toValue()) {rtn |= Nfs3Constant.ACCESS3_EXECUTE;} else {rtn |= Nfs3Constant.ACCESS3_LOOKUP;}}return rtn;}
public static int getAccessRightsForUserGroup(int uid, int gid, int[] auxGids, Nfs3FileAttributes attr) {int mode = attr.getMode();if (uid == attr.getUid()) {return getAccessRights(mode >> 6, attr.getType());}if (gid == attr.getGid()) {return getAccessRights(mode >> 3, attr.getType());}// Check for membership in auxiliary groupsif (auxGids != null) {for (int auxGid : auxGids) {if (attr.getGid() == auxGid) {return getAccessRights(mode >> 3, attr.getType());}}}return getAccessRights(mode, attr.getType());}
public static long bytesToLong(byte[] data) {long n = 0xffL & data[0];for (int i = 1; i < 8; i++) {n = (n << 8) | (0xffL & data[i]);}return n;}
public static byte[] longToByte(long v) {byte[] data = new byte[8];data[0] = (byte) (v >>> 56);data[1] = (byte) (v >>> 48);data[2] = (byte) (v >>> 40);data[3] = (byte) (v >>> 32);data[4] = (byte) (v >>> 24);data[5] = (byte) (v >>> 16);data[6] = (byte) (v >>> 8);data[7] = (byte) (v >>> 0);return data;}protected String _getVersion() {return info.getProperty("version", "Unknown");}
protected String _getRevision() {return info.getProperty("revision", "Unknown");}
protected String _getBranch() {return info.getProperty("branch", "Unknown");}
protected String _getDate() {return info.getProperty("date", "Unknown");}
protected String _getUser() {return info.getProperty("user", "Unknown");}
protected String _getUrl() {return info.getProperty("url", "Unknown");}
protected String _getSrcChecksum() {return info.getProperty("srcChecksum", "Unknown");}
protected String _getBuildVersion() {return getVersion() + " from " + _getRevision() + " by " + _getUser() + " source checksum " + _getSrcChecksum();}
protected String _getProtocVersion() {return info.getProperty("protocVersion", "Unknown");}
public static String getVersion() {return COMMON_VERSION_INFO._getVersion();}
public static String getRevision() {return COMMON_VERSION_INFO._getRevision();}
public static String getBranch() {return COMMON_VERSION_INFO._getBranch();}
public static String getDate() {return COMMON_VERSION_INFO._getDate();}
public static String getUser() {return COMMON_VERSION_INFO._getUser();}
public static String getUrl() {return COMMON_VERSION_INFO._getUrl();}
public static String getSrcChecksum() {return COMMON_VERSION_INFO._getSrcChecksum();}
public static String getBuildVersion() {return COMMON_VERSION_INFO._getBuildVersion();}
public static String getProtocVersion() {return COMMON_VERSION_INFO._getProtocVersion();}
public static void main(String[] args) {LOG.debug("version: " + getVersion());System.out.println("Hadoop " + getVersion());System.out.println("Subversion " + getUrl() + " -r " + getRevision());System.out.println("Compiled by " + getUser() + " on " + getDate());System.out.println("Compiled with protoc " + getProtocVersion());System.out.println("From source with checksum " + getSrcChecksum());System.out.println("This command was run using " + ClassUtil.findContainingJar(VersionInfo.class));}public static RefreshQueuesResponse newInstance() {RefreshQueuesResponse response = Records.newRecord(RefreshQueuesResponse.class);return response;}public static NodeManagerMetrics create() {return create(DefaultMetricsSystem.instance());}
static NodeManagerMetrics create(MetricsSystem ms) {JvmMetrics.create("NodeManager", null, ms);return ms.register(new NodeManagerMetrics());}
public void launchedContainer() {containersLaunched.incr();}
public void completedContainer() {containersCompleted.incr();}
public void failedContainer() {containersFailed.incr();}
public void killedContainer() {containersKilled.incr();}
public void initingContainer() {containersIniting.incr();}
public void endInitingContainer() {containersIniting.decr();}
public void runningContainer() {containersRunning.incr();}
public void endRunningContainer() {containersRunning.decr();}
public void allocateContainer(Resource res) {allocatedContainers.incr();allocatedGB.incr(res.getMemory() / 1024);availableGB.decr(res.getMemory() / 1024);allocatedVCores.incr(res.getVirtualCores());availableVCores.decr(res.getVirtualCores());}
public void releaseContainer(Resource res) {allocatedContainers.decr();allocatedGB.decr(res.getMemory() / 1024);availableGB.incr(res.getMemory() / 1024);allocatedVCores.decr(res.getVirtualCores());availableVCores.incr(res.getVirtualCores());}
public void addResource(Resource res) {availableGB.incr(res.getMemory() / 1024);availableVCores.incr(res.getVirtualCores());}
public int getRunningContainers() {return containersRunning.value();}public void run() {publishMetricsFromQueue();}
boolean putMetrics(MetricsBuffer buffer, long logicalTime) {if (logicalTime % period == 0) {LOG.debug("enqueue, logicalTime=" + logicalTime);if (queue.enqueue(buffer))return true;dropped.incr();return false;}// OKreturn true;}
public boolean putMetricsImmediate(MetricsBuffer buffer) {WaitableMetricsBuffer waitableBuffer = new WaitableMetricsBuffer(buffer);if (!queue.enqueue(waitableBuffer)) {LOG.warn(name + " has a full queue and can't consume the given metrics.");dropped.incr();return false;}if (!waitableBuffer.waitTillNotified(oobPutTimeout)) {LOG.warn(name + " couldn't fulfill an immediate putMetrics request in time." + " Abandoning.");return false;}return true;}
void publishMetricsFromQueue() {int retryDelay = firstRetryDelay;int n = retryCount;// millisint minDelay = Math.min(500, retryDelay * 1000);Random rng = new Random(System.nanoTime());while (!stopping) {try {queue.consumeAll(this);retryDelay = firstRetryDelay;n = retryCount;inError = false;} catch (InterruptedException e) {LOG.info(name + " thread interrupted.");} catch (Exception e) {if (n > 0) {int retryWindow = Math.max(0, 1000 / 2 * retryDelay - minDelay);int awhile = rng.nextInt(retryWindow) + minDelay;if (!inError) {LOG.error("Got sink exception, retry in " + awhile + "ms", e);}retryDelay *= retryBackoff;try {Thread.sleep(awhile);} catch (InterruptedException e2) {LOG.info(name + " thread interrupted while waiting for retry", e2);}--n;} else {if (!inError) {LOG.error("Got sink exception and over retry limit, " + "suppressing further error messages", e);}queue.clear();inError = true;}}}}
public void consume(MetricsBuffer buffer) {long ts = 0;for (MetricsBuffer.Entry entry : buffer) {if (sourceFilter == null || sourceFilter.accepts(entry.name())) {for (MetricsRecordImpl record : entry.records()) {if ((context == null || context.equals(record.context())) && (recordFilter == null || recordFilter.accepts(record))) {if (LOG.isDebugEnabled()) {LOG.debug("Pushing record " + entry.name() + "." + record.context() + "." + record.name() + " to " + name);}sink.putMetrics(metricFilter == null ? record : new MetricsRecordFiltered(record, metricFilter));if (ts == 0)ts = record.timestamp();}}}}if (ts > 0) {sink.flush();latency.add(Time.now() - ts);}if (buffer instanceof WaitableMetricsBuffer) {((WaitableMetricsBuffer) buffer).notifyAnyWaiters();}LOG.debug("Done");}
void start() {sinkThread.start();LOG.info("Sink " + name + " started");}
void stop() {stopping = true;sinkThread.interrupt();try {sinkThread.join();} catch (InterruptedException e) {LOG.warn("Stop interrupted", e);}if (sink instanceof Closeable) {IOUtils.cleanup(LOG, (Closeable) sink);}}
String name() {return name;}
String description() {return description;}
void snapshot(MetricsRecordBuilder rb, boolean all) {registry.snapshot(rb, all);}
MetricsSink sink() {return sink;}
public boolean waitTillNotified(long millisecondsToWait) {try {return notificationSemaphore.tryAcquire(millisecondsToWait, TimeUnit.MILLISECONDS);} catch (InterruptedException e) {return false;}}
public void notifyAnyWaiters() {notificationSemaphore.release();}public void index() {setTitle("Applications");}
public void about() {setTitle("About the Cluster");render(AboutPage.class);}
public void app() {render(AppPage.class);}
public void nodes() {render(NodesPage.class);}
public void scheduler() {// limit applications to those in states relevant to schedulingset(YarnWebParams.APP_STATE, StringHelper.cjoin(YarnApplicationState.NEW.toString(), YarnApplicationState.NEW_SAVING.toString(), YarnApplicationState.SUBMITTED.toString(), YarnApplicationState.ACCEPTED.toString(), YarnApplicationState.RUNNING.toString()));ResourceManager rm = getInstance(ResourceManager.class);ResourceScheduler rs = rm.getResourceScheduler();if (rs == null || rs instanceof CapacityScheduler) {setTitle("Capacity Scheduler");render(CapacitySchedulerPage.class);return;}if (rs instanceof FairScheduler) {setTitle("Fair Scheduler");render(FairSchedulerPage.class);return;}setTitle("Default Scheduler");render(DefaultSchedulerPage.class);}
public void queue() {setTitle(join("Queue ", get(QUEUE_NAME, "unknown")));}
public void submit() {setTitle("Application Submission Not Allowed");}protected int[] getValues() {return values;}
protected void initializeInterval() {state.currentAccumulation = 0.0D;}
protected void extend(double newProgress, int newValue) {if (state == null || newProgress < state.oldProgress) {return;}// This correctness of this code depends on 100% * count = count.int oldIndex = (int) (state.oldProgress * count);int newIndex = (int) (newProgress * count);int originalOldValue = state.oldValue;double fullValueDistance = (double) newValue - state.oldValue;double fullProgressDistance = newProgress - state.oldProgress;double originalOldProgress = state.oldProgress;//  crosses a boundary.for (int closee = oldIndex; closee < newIndex; ++closee) {double interpolationProgress = (double) (closee + 1) / count;// In floats, x * y / y might not equal y.interpolationProgress = Math.min(interpolationProgress, newProgress);double progressLength = (interpolationProgress - originalOldProgress);double interpolationProportion = progressLength / fullProgressDistance;double interpolationValueDistance = fullValueDistance * interpolationProportion;// estimates the value at the next [interpolated] subsegment boundaryint interpolationValue = (int) interpolationValueDistance + originalOldValue;extendInternal(interpolationProgress, interpolationValue);advanceState(interpolationProgress, interpolationValue);values[closee] = (int) state.currentAccumulation;initializeInterval();}extendInternal(newProgress, newValue);advanceState(newProgress, newValue);if (newIndex == count) {state = null;}}
protected void advanceState(double newProgress, int newValue) {state.oldValue = newValue;state.oldProgress = newProgress;}
int getCount() {return count;}
int get(int index) {return values[index];}public void setConf(Configuration conf) {this.conf = conf;}
public Configuration getConf() {return conf;}
public CompressionOutputStream createOutputStream(OutputStream out) {return null;}
public Class<? extends Compressor> getCompressorType() {return null;}
public Compressor createCompressor() {return null;}
public CompressionInputStream createInputStream(InputStream in, Decompressor decompressor) {return null;}
public CompressionInputStream createInputStream(InputStream in) {return null;}
public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) {return null;}
public Class<? extends Decompressor> getDecompressorType() {return null;}
public Decompressor createDecompressor() {return null;}
public String getDefaultExtension() {return ".base";}
public String getDefaultExtension() {return "bar";}
public String getDefaultExtension() {return ".foo.bar";}
public String getDefaultExtension() {return ".foo";}
public String getDefaultExtension() {return ".gz";}
private static CompressionCodecFactory setClasses(Class[] classes) {Configuration conf = new Configuration();CompressionCodecFactory.setCodecClasses(conf, Arrays.asList(classes));return new CompressionCodecFactory(conf);}
private static void checkCodec(String msg, Class expected, CompressionCodec actual) {assertEquals(msg + " unexpected codec found", expected.getName(), actual.getClass().getName());}
public static void testFinding() {CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration());CompressionCodec codec = factory.getCodec(new Path("/tmp/foo.bar"));assertEquals("default factory foo codec", null, codec);codec = factory.getCodecByClassName(BarCodec.class.getCanonicalName());assertEquals("default factory foo codec", null, codec);codec = factory.getCodec(new Path("/tmp/foo.gz"));checkCodec("default factory for .gz", GzipCodec.class, codec);codec = factory.getCodecByClassName(GzipCodec.class.getCanonicalName());checkCodec("default factory for gzip codec", GzipCodec.class, codec);codec = factory.getCodecByName("gzip");checkCodec("default factory for gzip codec", GzipCodec.class, codec);codec = factory.getCodecByName("GZIP");checkCodec("default factory for gzip codec", GzipCodec.class, codec);codec = factory.getCodecByName("GZIPCodec");checkCodec("default factory for gzip codec", GzipCodec.class, codec);codec = factory.getCodecByName("gzipcodec");checkCodec("default factory for gzip codec", GzipCodec.class, codec);Class klass = factory.getCodecClassByName("gzipcodec");assertEquals(GzipCodec.class, klass);codec = factory.getCodec(new Path("/tmp/foo.bz2"));checkCodec("default factory for .bz2", BZip2Codec.class, codec);codec = factory.getCodecByClassName(BZip2Codec.class.getCanonicalName());checkCodec("default factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodecByName("bzip2");checkCodec("default factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodecByName("bzip2codec");checkCodec("default factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodecByName("BZIP2");checkCodec("default factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodecByName("BZIP2CODEC");checkCodec("default factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodecByClassName(DeflateCodec.class.getCanonicalName());checkCodec("default factory for deflate codec", DeflateCodec.class, codec);codec = factory.getCodecByName("deflate");checkCodec("default factory for deflate codec", DeflateCodec.class, codec);codec = factory.getCodecByName("deflatecodec");checkCodec("default factory for deflate codec", DeflateCodec.class, codec);codec = factory.getCodecByName("DEFLATE");checkCodec("default factory for deflate codec", DeflateCodec.class, codec);codec = factory.getCodecByName("DEFLATECODEC");checkCodec("default factory for deflate codec", DeflateCodec.class, codec);factory = setClasses(new Class[0]);// gz, bz2, snappy, lz4 are picked up by service loader, but bar isn'tcodec = factory.getCodec(new Path("/tmp/foo.bar"));assertEquals("empty factory bar codec", null, codec);codec = factory.getCodecByClassName(BarCodec.class.getCanonicalName());assertEquals("empty factory bar codec", null, codec);codec = factory.getCodec(new Path("/tmp/foo.gz"));checkCodec("empty factory gz codec", GzipCodec.class, codec);codec = factory.getCodecByClassName(GzipCodec.class.getCanonicalName());checkCodec("empty factory gz codec", GzipCodec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.bz2"));checkCodec("empty factory for .bz2", BZip2Codec.class, codec);codec = factory.getCodecByClassName(BZip2Codec.class.getCanonicalName());checkCodec("empty factory for bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.snappy"));checkCodec("empty factory snappy codec", SnappyCodec.class, codec);codec = factory.getCodecByClassName(SnappyCodec.class.getCanonicalName());checkCodec("empty factory snappy codec", SnappyCodec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.lz4"));checkCodec("empty factory lz4 codec", Lz4Codec.class, codec);codec = factory.getCodecByClassName(Lz4Codec.class.getCanonicalName());checkCodec("empty factory lz4 codec", Lz4Codec.class, codec);factory = setClasses(new Class[] { BarCodec.class, FooCodec.class, FooBarCodec.class });codec = factory.getCodec(new Path("/tmp/.foo.bar.gz"));checkCodec("full factory gz codec", GzipCodec.class, codec);codec = factory.getCodecByClassName(GzipCodec.class.getCanonicalName());checkCodec("full codec gz codec", GzipCodec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.bz2"));checkCodec("full factory for .bz2", BZip2Codec.class, codec);codec = factory.getCodecByClassName(BZip2Codec.class.getCanonicalName());checkCodec("full codec bzip2 codec", BZip2Codec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.bar"));checkCodec("full factory bar codec", BarCodec.class, codec);codec = factory.getCodecByClassName(BarCodec.class.getCanonicalName());checkCodec("full factory bar codec", BarCodec.class, codec);codec = factory.getCodecByName("bar");checkCodec("full factory bar codec", BarCodec.class, codec);codec = factory.getCodecByName("BAR");checkCodec("full factory bar codec", BarCodec.class, codec);codec = factory.getCodec(new Path("/tmp/foo/baz.foo.bar"));checkCodec("full factory foo bar codec", FooBarCodec.class, codec);codec = factory.getCodecByClassName(FooBarCodec.class.getCanonicalName());checkCodec("full factory foo bar codec", FooBarCodec.class, codec);codec = factory.getCodecByName("foobar");checkCodec("full factory foo bar codec", FooBarCodec.class, codec);codec = factory.getCodecByName("FOOBAR");checkCodec("full factory foo bar codec", FooBarCodec.class, codec);codec = factory.getCodec(new Path("/tmp/foo.foo"));checkCodec("full factory foo codec", FooCodec.class, codec);codec = factory.getCodecByClassName(FooCodec.class.getCanonicalName());checkCodec("full factory foo codec", FooCodec.class, codec);codec = factory.getCodecByName("foo");checkCodec("full factory foo codec", FooCodec.class, codec);codec = factory.getCodecByName("FOO");checkCodec("full factory foo codec", FooCodec.class, codec);factory = setClasses(new Class[] { NewGzipCodec.class });codec = factory.getCodec(new Path("/tmp/foo.gz"));checkCodec("overridden factory for .gz", NewGzipCodec.class, codec);codec = factory.getCodecByClassName(NewGzipCodec.class.getCanonicalName());checkCodec("overridden factory for gzip codec", NewGzipCodec.class, codec);Configuration conf = new Configuration();conf.set("io.compression.codecs", "   org.apache.hadoop.io.compress.GzipCodec   , " + "org.apache.hadoop.io.compress.DefaultCodec  , " + " org.apache.hadoop.io.compress.BZip2Codec   ");try {CompressionCodecFactory.getCodecClasses(conf);} catch (IllegalArgumentException e) {fail("IllegalArgumentException is unexpected");}}private static void createFile(Suite s, int index, long len) {final FileSystem fs = s.cluster.getFileSystem(index);DFSTestUtil.createFile(fs, FILE_PATH, len, s.replication, RANDOM.nextLong());DFSTestUtil.waitReplication(fs, FILE_PATH, s.replication);}
private static ExtendedBlock[][] generateBlocks(Suite s, long size) {final ExtendedBlock[][] blocks = new ExtendedBlock[s.clients.length][];for (int n = 0; n < s.clients.length; n++) {final long fileLen = size / s.replication;createFile(s, n, fileLen);final List<LocatedBlock> locatedBlocks = s.clients[n].getBlockLocations(FILE_NAME, 0, fileLen).getLocatedBlocks();final int numOfBlocks = locatedBlocks.size();blocks[n] = new ExtendedBlock[numOfBlocks];for (int i = 0; i < numOfBlocks; i++) {final ExtendedBlock b = locatedBlocks.get(i).getBlock();blocks[n][i] = new ExtendedBlock(b.getBlockPoolId(), b.getBlockId(), b.getNumBytes(), b.getGenerationStamp());}}return blocks;}
static void wait(final ClientProtocol[] clients, long expectedUsedSpace, long expectedTotalSpace) {LOG.info("WAIT expectedUsedSpace=" + expectedUsedSpace + ", expectedTotalSpace=" + expectedTotalSpace);for (int n = 0; n < clients.length; n++) {int i = 0;for (boolean done = false; !done; ) {final long[] s = clients[n].getStats();done = s[0] == expectedTotalSpace && s[1] == expectedUsedSpace;if (!done) {sleep(100L);if (++i % 100 == 0) {LOG.warn("WAIT i=" + i + ", s=[" + s[0] + ", " + s[1] + "]");}}}}}
static void runBalancer(Suite s, final long totalUsed, final long totalCapacity) {final double avg = totalUsed * 100.0 / totalCapacity;LOG.info("BALANCER 0: totalUsed=" + totalUsed + ", totalCapacity=" + totalCapacity + ", avg=" + avg);wait(s.clients, totalUsed, totalCapacity);LOG.info("BALANCER 1");// start rebalancingfinal Collection<URI> namenodes = DFSUtil.getNsServiceRpcUris(s.conf);final int r = Balancer.run(namenodes, Balancer.Parameters.DEFAULT, s.conf);Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), r);LOG.info("BALANCER 2");wait(s.clients, totalUsed, totalCapacity);LOG.info("BALANCER 3");int i = 0;for (boolean balanced = false; !balanced; i++) {final long[] used = new long[s.cluster.getDataNodes().size()];final long[] cap = new long[used.length];for (int n = 0; n < s.clients.length; n++) {final DatanodeInfo[] datanodes = s.clients[n].getDatanodeReport(DatanodeReportType.ALL);Assert.assertEquals(datanodes.length, used.length);for (int d = 0; d < datanodes.length; d++) {if (n == 0) {used[d] = datanodes[d].getDfsUsed();cap[d] = datanodes[d].getCapacity();if (i % 100 == 0) {LOG.warn("datanodes[" + d + "]: getDfsUsed()=" + datanodes[d].getDfsUsed() + ", getCapacity()=" + datanodes[d].getCapacity());}} else {Assert.assertEquals(used[d], datanodes[d].getDfsUsed());Assert.assertEquals(cap[d], datanodes[d].getCapacity());}}}balanced = true;for (int d = 0; d < used.length; d++) {final double p = used[d] * 100.0 / cap[d];balanced = p <= avg + Balancer.Parameters.DEFAULT.threshold;if (!balanced) {if (i % 100 == 0) {LOG.warn("datanodes " + d + " is not yet balanced: " + "used=" + used[d] + ", cap=" + cap[d] + ", avg=" + avg);LOG.warn("TestBalancer.sum(used)=" + TestBalancer.sum(used) + ", TestBalancer.sum(cap)=" + TestBalancer.sum(cap));}sleep(100);break;}}}LOG.info("BALANCER 6");}
private static void sleep(long ms) {try {Thread.sleep(ms);} catch (InterruptedException e) {LOG.error(e);}}
private static Configuration createConf() {final Configuration conf = new HdfsConfiguration();TestBalancer.initConf(conf);return conf;}
private void unevenDistribution(final int nNameNodes, long distributionPerNN[], long capacities[], String[] racks, Configuration conf) {LOG.info("UNEVEN 0");final int nDataNodes = distributionPerNN.length;if (capacities.length != nDataNodes || racks.length != nDataNodes) {throw new IllegalArgumentException("Array length is not the same");}// calculate total space that need to be filledfinal long usedSpacePerNN = TestBalancer.sum(distributionPerNN);// fill the clusterfinal ExtendedBlock[][] blocks;{LOG.info("UNEVEN 1");final MiniDFSCluster cluster = new MiniDFSCluster.Builder(new Configuration(conf)).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).numDataNodes(nDataNodes).racks(racks).simulatedCapacities(capacities).build();LOG.info("UNEVEN 2");try {cluster.waitActive();DFSTestUtil.setFederatedConfiguration(cluster, conf);LOG.info("UNEVEN 3");final Suite s = new Suite(cluster, nNameNodes, nDataNodes, conf);blocks = generateBlocks(s, usedSpacePerNN);LOG.info("UNEVEN 4");} finally {cluster.shutdown();}}conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY, "0.0f");{LOG.info("UNEVEN 10");final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)).numDataNodes(nDataNodes).racks(racks).simulatedCapacities(capacities).format(false).build();LOG.info("UNEVEN 11");try {cluster.waitActive();LOG.info("UNEVEN 12");final Suite s = new Suite(cluster, nNameNodes, nDataNodes, conf);for (int n = 0; n < nNameNodes; n++) {// redistribute blocksfinal Block[][] blocksDN = TestBalancer.distributeBlocks(blocks[n], s.replication, distributionPerNN);for (int d = 0; d < blocksDN.length; d++) cluster.injectBlocks(n, d, Arrays.asList(blocksDN[d]));LOG.info("UNEVEN 13: n=" + n);}final long totalCapacity = TestBalancer.sum(capacities);final long totalUsed = nNameNodes * usedSpacePerNN;LOG.info("UNEVEN 14");runBalancer(s, totalUsed, totalCapacity);LOG.info("UNEVEN 15");} finally {cluster.shutdown();}LOG.info("UNEVEN 16");}}
private void runTest(final int nNameNodes, long[] capacities, String[] racks, long newCapacity, String newRack, Configuration conf) {final int nDataNodes = capacities.length;LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes=" + nDataNodes);Assert.assertEquals(nDataNodes, racks.length);LOG.info("RUN_TEST -1");final MiniDFSCluster cluster = new MiniDFSCluster.Builder(new Configuration(conf)).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)).numDataNodes(nDataNodes).racks(racks).simulatedCapacities(capacities).build();LOG.info("RUN_TEST 0");DFSTestUtil.setFederatedConfiguration(cluster, conf);try {cluster.waitActive();LOG.info("RUN_TEST 1");final Suite s = new Suite(cluster, nNameNodes, nDataNodes, conf);long totalCapacity = TestBalancer.sum(capacities);LOG.info("RUN_TEST 2");// fill up the cluster to be 30% fullfinal long totalUsed = totalCapacity * 3 / 10;final long size = (totalUsed / nNameNodes) / s.replication;for (int n = 0; n < nNameNodes; n++) {createFile(s, n, size);}LOG.info("RUN_TEST 3");// start up an empty node with the same capacity and on the same rackcluster.startDataNodes(conf, 1, true, null, new String[] { newRack }, new long[] { newCapacity });totalCapacity += newCapacity;LOG.info("RUN_TEST 4");// run RUN_TEST and validate resultsrunBalancer(s, totalUsed, totalCapacity);LOG.info("RUN_TEST 5");} finally {cluster.shutdown();}LOG.info("RUN_TEST 6");}
public void testBalancer() {final Configuration conf = createConf();runTest(2, new long[] { CAPACITY }, new String[] { RACK0 }, CAPACITY / 2, RACK0, conf);}
public void testUnevenDistribution() {final Configuration conf = createConf();unevenDistribution(2, new long[] { 30 * CAPACITY / 100, 5 * CAPACITY / 100 }, new long[] { CAPACITY, CAPACITY }, new String[] { RACK0, RACK1 }, conf);}public Object getDatum() {if (datum == null) {datum = new JobFinished();datum.jobid = new Utf8(jobId.toString());datum.finishTime = finishTime;datum.finishedMaps = finishedMaps;datum.finishedReduces = finishedReduces;datum.failedMaps = failedMaps;datum.failedReduces = failedReduces;datum.mapCounters = EventWriter.toAvro(mapCounters, "MAP_COUNTERS");datum.reduceCounters = EventWriter.toAvro(reduceCounters, "REDUCE_COUNTERS");datum.totalCounters = EventWriter.toAvro(totalCounters, "TOTAL_COUNTERS");}return datum;}
public void setDatum(Object oDatum) {this.datum = (JobFinished) oDatum;this.jobId = JobID.forName(datum.jobid.toString());this.finishTime = datum.finishTime;this.finishedMaps = datum.finishedMaps;this.finishedReduces = datum.finishedReduces;this.failedMaps = datum.failedMaps;this.failedReduces = datum.failedReduces;this.mapCounters = EventReader.fromAvro(datum.mapCounters);this.reduceCounters = EventReader.fromAvro(datum.reduceCounters);this.totalCounters = EventReader.fromAvro(datum.totalCounters);}
public EventType getEventType() {return EventType.JOB_FINISHED;}
public JobID getJobid() {return jobId;}
public long getFinishTime() {return finishTime;}
public int getFinishedMaps() {return finishedMaps;}
public int getFinishedReduces() {return finishedReduces;}
public int getFailedMaps() {return failedMaps;}
public int getFailedReduces() {return failedReduces;}
public Counters getTotalCounters() {return totalCounters;}
public Counters getMapCounters() {return mapCounters;}
public Counters getReduceCounters() {return reduceCounters;}private void cleanupData(Configuration conf) {FileSystem fs = FileSystem.get(conf);MapReduceTestUtil.cleanData(fs, indir);MapReduceTestUtil.generateData(fs, indir);MapReduceTestUtil.cleanData(fs, outdir_1);MapReduceTestUtil.cleanData(fs, outdir_2);MapReduceTestUtil.cleanData(fs, outdir_3);MapReduceTestUtil.cleanData(fs, outdir_4);}
private JobControl createDependencies(Configuration conf, Job job1) {List<ControlledJob> dependingJobs = null;cjob1 = new ControlledJob(job1, dependingJobs);Job job2 = MapReduceTestUtil.createCopyJob(conf, outdir_2, indir);cjob2 = new ControlledJob(job2, dependingJobs);Job job3 = MapReduceTestUtil.createCopyJob(conf, outdir_3, outdir_1, outdir_2);dependingJobs = new ArrayList<ControlledJob>();dependingJobs.add(cjob1);dependingJobs.add(cjob2);cjob3 = new ControlledJob(job3, dependingJobs);Job job4 = MapReduceTestUtil.createCopyJob(conf, outdir_4, outdir_3);dependingJobs = new ArrayList<ControlledJob>();dependingJobs.add(cjob3);cjob4 = new ControlledJob(job4, dependingJobs);JobControl theControl = new JobControl("Test");theControl.addJob(cjob1);theControl.addJob(cjob2);theControl.addJob(cjob3);theControl.addJob(cjob4);Thread theController = new Thread(theControl);theController.start();return theControl;}
private void waitTillAllFinished(JobControl theControl) {while (!theControl.allFinished()) {try {Thread.sleep(100);} catch (Exception e) {}}}
public void testJobControlWithFailJob() {LOG.info("Starting testJobControlWithFailJob");Configuration conf = createJobConf();cleanupData(conf);// create a Fail jobJob job1 = MapReduceTestUtil.createFailJob(conf, outdir_1, indir);// create job dependenciesJobControl theControl = createDependencies(conf, job1);// wait till all the jobs completewaitTillAllFinished(theControl);assertTrue(cjob1.getJobState() == ControlledJob.State.FAILED);assertTrue(cjob2.getJobState() == ControlledJob.State.SUCCESS);assertTrue(cjob3.getJobState() == ControlledJob.State.DEPENDENT_FAILED);assertTrue(cjob4.getJobState() == ControlledJob.State.DEPENDENT_FAILED);theControl.stop();}
public void testJobControlWithKillJob() {LOG.info("Starting testJobControlWithKillJob");Configuration conf = createJobConf();cleanupData(conf);Job job1 = MapReduceTestUtil.createKillJob(conf, outdir_1, indir);JobControl theControl = createDependencies(conf, job1);while (cjob1.getJobState() != ControlledJob.State.RUNNING) {try {Thread.sleep(100);} catch (InterruptedException e) {break;}}// verify adding dependingJo to RUNNING job fails.assertFalse(cjob1.addDependingJob(cjob2));// suspend jobcontrol and resume it againtheControl.suspend();assertTrue(theControl.getThreadState() == JobControl.ThreadState.SUSPENDED);theControl.resume();// kill the first job.cjob1.killJob();// wait till all the jobs completewaitTillAllFinished(theControl);assertTrue(cjob1.getJobState() == ControlledJob.State.FAILED);assertTrue(cjob2.getJobState() == ControlledJob.State.SUCCESS);assertTrue(cjob3.getJobState() == ControlledJob.State.DEPENDENT_FAILED);assertTrue(cjob4.getJobState() == ControlledJob.State.DEPENDENT_FAILED);theControl.stop();}
public void testJobControl() {LOG.info("Starting testJobControl");Configuration conf = createJobConf();cleanupData(conf);Job job1 = MapReduceTestUtil.createCopyJob(conf, outdir_1, indir);JobControl theControl = createDependencies(conf, job1);// wait till all the jobs completewaitTillAllFinished(theControl);assertEquals("Some jobs failed", 0, theControl.getFailedJobList().size());theControl.stop();}
public void testControlledJob() {LOG.info("Starting testControlledJob");Configuration conf = createJobConf();cleanupData(conf);Job job1 = MapReduceTestUtil.createCopyJob(conf, outdir_1, indir);JobControl theControl = createDependencies(conf, job1);while (cjob1.getJobState() != ControlledJob.State.RUNNING) {try {Thread.sleep(100);} catch (InterruptedException e) {break;}}Assert.assertNotNull(cjob1.getMapredJobId());// wait till all the jobs completewaitTillAllFinished(theControl);assertEquals("Some jobs failed", 0, theControl.getFailedJobList().size());theControl.stop();}public void testMapTaskStatusStartAndFinishTimes() {checkTaskStatues(true);}
public void testReduceTaskStatusStartAndFinishTimes() {checkTaskStatues(false);}
private void checkTaskStatues(boolean isMap) {TaskStatus status = null;if (isMap) {status = new MapTaskStatus();} else {status = new ReduceTaskStatus();}long currentTime = System.currentTimeMillis();// start time is set.status.setFinishTime(currentTime);assertEquals("Finish time of the task status set without start time", 0, status.getFinishTime());// Now set the start time to right time.status.setStartTime(currentTime);assertEquals("Start time of the task status not set correctly.", currentTime, status.getStartTime());// try setting wrong start time to task status.long wrongTime = -1;status.setStartTime(wrongTime);assertEquals("Start time of the task status is set to wrong negative value", currentTime, status.getStartTime());// finally try setting wrong finish time i.e. negative value.status.setFinishTime(wrongTime);assertEquals("Finish time of task status is set to wrong negative value", 0, status.getFinishTime());status.setFinishTime(currentTime);assertEquals("Finish time of the task status not set correctly.", currentTime, status.getFinishTime());// test with null task-diagnosticsTaskStatus ts = ((TaskStatus) status.clone());ts.setDiagnosticInfo(null);ts.setDiagnosticInfo("");ts.setStateString(null);ts.setStateString("");((TaskStatus) status.clone()).statusUpdate(ts);// test with null state-string((TaskStatus) status.clone()).statusUpdate(0, null, null);((TaskStatus) status.clone()).statusUpdate(0, "", null);((TaskStatus) status.clone()).statusUpdate(null, 0, "", null, 1);}
public void testTaskDiagnosticsAndStateString() {// check the default caseString test = "hi";final int maxSize = 16;TaskStatus status = new TaskStatus(null, 0, 0, null, test, test, null, null, null) {
@Overrideprotected int getMaxStringSize() {return maxSize;}
@Overridepublic void addFetchFailedMap(TaskAttemptID mapTaskId) {}
@Overridepublic boolean getIsMap() {return false;}};assertEquals("Small diagnostic info test failed", status.getDiagnosticInfo(), test);assertEquals("Small state string test failed", status.getStateString(), test);// now append some small string and checkString newDInfo = test.concat(test);status.setDiagnosticInfo(test);status.setStateString(newDInfo);assertEquals("Small diagnostic info append failed", newDInfo, status.getDiagnosticInfo());assertEquals("Small state-string append failed", newDInfo, status.getStateString());// update the status with small state stringsTaskStatus newStatus = (TaskStatus) status.clone();String newSInfo = "hi1";newStatus.setStateString(newSInfo);status.statusUpdate(newStatus);newDInfo = newDInfo.concat(newStatus.getDiagnosticInfo());assertEquals("Status-update on diagnostic-info failed", newDInfo, status.getDiagnosticInfo());assertEquals("Status-update on state-string failed", newSInfo, status.getStateString());newSInfo = "hi2";status.statusUpdate(0, newSInfo, null);assertEquals("Status-update on state-string failed", newSInfo, status.getStateString());newSInfo = "hi3";status.statusUpdate(null, 0, newSInfo, null, 0);assertEquals("Status-update on state-string failed", newSInfo, status.getStateString());// 20 charsString large = "hihihihihihihihihihi";status.setDiagnosticInfo(large);status.setStateString(large);assertEquals("Large diagnostic info append test failed", maxSize, status.getDiagnosticInfo().length());assertEquals("Large state-string append test failed", maxSize, status.getStateString().length());// update a large status with large stringsnewStatus.setDiagnosticInfo(large + "0");newStatus.setStateString(large + "1");status.statusUpdate(newStatus);assertEquals("Status-update on diagnostic info failed", maxSize, status.getDiagnosticInfo().length());assertEquals("Status-update on state-string failed", maxSize, status.getStateString().length());status.statusUpdate(0, large + "2", null);assertEquals("Status-update on state-string failed", maxSize, status.getStateString().length());status.statusUpdate(null, 0, large + "3", null, 0);assertEquals("Status-update on state-string failed", maxSize, status.getStateString().length());// test passing large string in constructorstatus = new TaskStatus(null, 0, 0, null, large, large, null, null, null) {
@Overrideprotected int getMaxStringSize() {return maxSize;}
@Overridepublic void addFetchFailedMap(TaskAttemptID mapTaskId) {}
@Overridepublic boolean getIsMap() {return false;}};assertEquals("Large diagnostic info test failed", maxSize, status.getDiagnosticInfo().length());assertEquals("Large state-string test failed", maxSize, status.getStateString().length());}
protected int getMaxStringSize() {return maxSize;}
public void addFetchFailedMap(TaskAttemptID mapTaskId) {}
public boolean getIsMap() {return false;}
protected int getMaxStringSize() {return maxSize;}
public void addFetchFailedMap(TaskAttemptID mapTaskId) {}
public boolean getIsMap() {return false;}protected void extendInternal(double newProgress, int newValue) {if (state == null) {return;}state.currentAccumulation += (double) (newValue - previousValue);previousValue = newValue;}public void testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister() {final Configuration conf = new HdfsConfiguration();// create clusterconf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512);final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();HdfsDataInputStream in = null;try {Path path = new Path("/tmp/TestFileLengthOnClusterRestart", "test");DistributedFileSystem dfs = cluster.getFileSystem();FSDataOutputStream out = dfs.create(path);int fileLength = 1030;out.write(new byte[fileLength]);out.hsync();cluster.restartNameNode();cluster.waitActive();in = (HdfsDataInputStream) dfs.open(path, 1024);// immediately.Assert.assertEquals(fileLength, in.getVisibleLength());cluster.shutdownDataNodes();cluster.restartNameNode(false);// This is just for ensuring NN started.verifyNNIsInSafeMode(dfs);try {in = (HdfsDataInputStream) dfs.open(path);Assert.fail("Expected IOException");} catch (IOException e) {Assert.assertTrue(e.getLocalizedMessage().indexOf("Name node is in safe mode") >= 0);}} finally {if (null != in) {in.close();}cluster.shutdown();}}
private void verifyNNIsInSafeMode(DistributedFileSystem dfs) {while (true) {try {if (dfs.isInSafeMode()) {return;} else {throw new IOException("Expected to be in SafeMode");}} catch (IOException e) {}}}public void setup() {if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {LOG.info("MRAppJar " + MiniMRYarnCluster.APPJAR + " not found. Not running test.");return;}if (mrCluster == null) {mrCluster = new MiniMRYarnCluster(getClass().getName());mrCluster.init(new Configuration());mrCluster.start();}// workaround the absent public discache.localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);localFs.setPermission(APP_JAR, new FsPermission("700"));}
public void tearDown() {if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {LOG.info("MRAppJar " + MiniMRYarnCluster.APPJAR + " not found. Not running test.");return;}if (mrCluster != null) {mrCluster.stop();}}
public void testJobHistoryData() {if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {LOG.info("MRAppJar " + MiniMRYarnCluster.APPJAR + " not found. Not running test.");return;}SleepJob sleepJob = new SleepJob();sleepJob.setConf(mrCluster.getConfig());// Job with 3 maps and 2 reducesJob job = sleepJob.createJob(3, 2, 1000, 1, 500, 1);job.setJarByClass(SleepJob.class);// The AppMaster jar itself.job.addFileToClassPath(APP_JAR);job.waitForCompletion(true);Counters counterMR = job.getCounters();JobId jobId = TypeConverter.toYarn(job.getJobID());ApplicationId appID = jobId.getAppId();int pollElapsed = 0;while (true) {Thread.sleep(1000);pollElapsed += 1000;if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {break;}if (pollElapsed >= 60000) {LOG.warn("application did not reach terminal state within 60 seconds");break;}}Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState());Counters counterHS = job.getCounters();//Should we compare each field or convert to V2 counter and compareLOG.info("CounterHS " + counterHS);LOG.info("CounterMR " + counterMR);Assert.assertEquals(counterHS, counterMR);HSClientProtocol historyClient = instantiateHistoryProxy();GetJobReportRequest gjReq = Records.newRecord(GetJobReportRequest.class);gjReq.setJobId(jobId);JobReport jobReport = historyClient.getJobReport(gjReq).getJobReport();verifyJobReport(jobReport, jobId);}
private void verifyJobReport(JobReport jobReport, JobId jobId) {List<AMInfo> amInfos = jobReport.getAMInfos();Assert.assertEquals(1, amInfos.size());AMInfo amInfo = amInfos.get(0);ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(jobId.getAppId(), 1);ContainerId amContainerId = ContainerId.newInstance(appAttemptId, 1);Assert.assertEquals(appAttemptId, amInfo.getAppAttemptId());Assert.assertEquals(amContainerId, amInfo.getContainerId());Assert.assertTrue(jobReport.getSubmitTime() > 0);Assert.assertTrue(jobReport.getStartTime() > 0 && jobReport.getStartTime() >= jobReport.getSubmitTime());Assert.assertTrue(jobReport.getFinishTime() > 0 && jobReport.getFinishTime() >= jobReport.getStartTime());}
private HSClientProtocol instantiateHistoryProxy() {final String serviceAddr = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS);final YarnRPC rpc = YarnRPC.create(conf);HSClientProtocol historyClient = (HSClientProtocol) rpc.getProxy(HSClientProtocol.class, NetUtils.createSocketAddr(serviceAddr), mrCluster.getConfig());return historyClient;}private static ZooKeeper connectZooKeeper(String ensemble) {final CountDownLatch latch = new CountDownLatch(1);ZooKeeper zkc = new ZooKeeper(HOSTPORT, 3600, new Watcher() {
public void process(WatchedEvent event) {if (event.getState() == Watcher.Event.KeeperState.SyncConnected) {latch.countDown();}}});if (!latch.await(10, TimeUnit.SECONDS)) {throw new IOException("Zookeeper took too long to connect");}return zkc;}
public void process(WatchedEvent event) {if (event.getState() == Watcher.Event.KeeperState.SyncConnected) {latch.countDown();}}
public static void setupZooKeeper() {LOG.info("Starting ZK server");zkTmpDir = File.createTempFile("zookeeper", "test");zkTmpDir.delete();zkTmpDir.mkdir();try {zks = new ZooKeeperServer(zkTmpDir, zkTmpDir, ZooKeeperDefaultPort);serverFactory = new NIOServerCnxnFactory();serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), 10);serverFactory.startup(zks);} catch (Exception e) {LOG.error("Exception while instantiating ZooKeeper", e);}boolean b = LocalBookKeeper.waitForServerUp(HOSTPORT, CONNECTION_TIMEOUT);LOG.debug("ZooKeeper server up: " + b);}
public static void shutDownServer() {if (null != zks) {zks.shutdown();}zkTmpDir.delete();}
public void setup() {zkc = connectZooKeeper(HOSTPORT);}
public void teardown() {if (null != zkc) {zkc.close();}}
public void testReadShouldReturnTheZnodePathAfterUpdate() {String data = "inprogressNode";CurrentInprogress ci = new CurrentInprogress(zkc, CURRENT_NODE_PATH);ci.init();ci.update(data);String inprogressNodePath = ci.read();assertEquals("Not returning inprogressZnode", "inprogressNode", inprogressNodePath);}
public void testReadShouldReturnNullAfterClear() {CurrentInprogress ci = new CurrentInprogress(zkc, CURRENT_NODE_PATH);ci.init();ci.update("myInprogressZnode");ci.read();ci.clear();String inprogressNodePath = ci.read();assertEquals("Expecting null to be return", null, inprogressNodePath);}
public void testUpdateShouldFailWithIOEIfVersionNumberChangedAfterRead() {CurrentInprogress ci = new CurrentInprogress(zkc, CURRENT_NODE_PATH);ci.init();ci.update("myInprogressZnode");assertEquals("Not returning myInprogressZnode", "myInprogressZnode", ci.read());// Updating data in-between to change the data to change the version numberci.update("YourInprogressZnode");ci.update("myInprogressZnode");}public Object getDatum() {return datum;}
public void setDatum(Object datum) {this.datum = (JobStatusChanged) datum;}
public JobID getJobId() {return JobID.forName(datum.jobid.toString());}
public String getStatus() {return datum.jobStatus.toString();}
public EventType getEventType() {return EventType.JOB_STATUS_CHANGED;}public void run() {synchronized (TestAsyncDiskService.this) {count++;}}
public void testAsyncDiskService() {String[] vols = new String[] { "/0", "/1" };AsyncDiskService service = new AsyncDiskService(vols);int total = 100;for (int i = 0; i < total; i++) {service.execute(vols[i % 2], new ExampleTask());}Exception e = null;try {service.execute("no_such_volume", new ExampleTask());} catch (RuntimeException ex) {e = ex;}assertNotNull("Executing a task on a non-existing volume should throw an " + "Exception.", e);service.shutdown();if (!service.awaitTermination(5000)) {fail("AsyncDiskService didn't shutdown in 5 seconds.");}assertEquals(total, count);}public void load(long sizeInMB) {for (long i = 0; i < sizeInMB; ++i) {// Create another String object of size 1MBheapSpace.add((Object) new byte[ONE_MB]);}}
public void initialize(ResourceCalculatorPlugin monitor, long totalHeapUsageInMB) {long maxPhysicalMemoryInMB = monitor.getPhysicalMemorySize() / ONE_MB;if (maxPhysicalMemoryInMB < totalHeapUsageInMB) {throw new RuntimeException("Total heap the can be used is " + maxPhysicalMemoryInMB + " bytes while the emulator is configured to emulate a total of " + totalHeapUsageInMB + " bytes");}}
public void reset() {heapSpace.clear();}
protected long getTotalHeapUsageInMB() {return Runtime.getRuntime().totalMemory() / ONE_MB;}
protected long getMaxHeapUsageInMB() {return Runtime.getRuntime().maxMemory() / ONE_MB;}
public float getProgress() {return enabled ? Math.min(1f, ((float) getTotalHeapUsageInMB()) / targetHeapUsageInMB) : 1.0f;}
public void emulate() {if (enabled) {float currentProgress = progress.getProgress();if (prevEmulationProgress < currentProgress && ((currentProgress - prevEmulationProgress) >= emulationInterval || currentProgress == 1)) {long maxHeapSizeInMB = getMaxHeapUsageInMB();long committedHeapSizeInMB = getTotalHeapUsageInMB();// Using a linear weighing function for computing the expected usagelong expectedHeapUsageInMB = Math.min(maxHeapSizeInMB, (long) (targetHeapUsageInMB * currentProgress));if (expectedHeapUsageInMB < maxHeapSizeInMB && committedHeapSizeInMB < expectedHeapUsageInMB) {long bufferInMB = (long) (minFreeHeapRatio * expectedHeapUsageInMB);long currentDifferenceInMB = expectedHeapUsageInMB - committedHeapSizeInMB;long currentIncrementLoadSizeInMB = (long) (currentDifferenceInMB * heapLoadRatio);// Make sure that at least 1 MB is incremented.currentIncrementLoadSizeInMB = Math.max(1, currentIncrementLoadSizeInMB);while (committedHeapSizeInMB + bufferInMB < expectedHeapUsageInMB) {// add blocks in order of X% of the difference, X = 10% by defaultemulatorCore.load(currentIncrementLoadSizeInMB);committedHeapSizeInMB = getTotalHeapUsageInMB();}}// store the emulation progress boundaryprevEmulationProgress = currentProgress;}// reset the core so that the garbage is reclaimedemulatorCore.reset();}}
public void initialize(Configuration conf, ResourceUsageMetrics metrics, ResourceCalculatorPlugin monitor, Progressive progress) {this.progress = progress;// get the target heap usagetargetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;if (targetHeapUsageInMB <= 0) {enabled = false;return;} else {// calibrate the core heap-usage utilityemulatorCore.initialize(monitor, targetHeapUsageInMB);enabled = true;}emulationInterval = conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, DEFAULT_EMULATION_PROGRESS_INTERVAL);minFreeHeapRatio = conf.getFloat(MIN_HEAP_FREE_RATIO, DEFAULT_MIN_FREE_HEAP_RATIO);heapLoadRatio = conf.getFloat(HEAP_LOAD_RATIO, DEFAULT_HEAP_LOAD_RATIO);prevEmulationProgress = 0;}String getAllProcessInfoFromShell() {return infoStr;}
public void tree() {if (!Shell.WINDOWS) {LOG.info("Platform not Windows. Not testing");return;}assertTrue("WindowsBasedProcessTree should be available on Windows", WindowsBasedProcessTree.isAvailable());WindowsBasedProcessTreeTester pTree = new WindowsBasedProcessTreeTester("-1");pTree.infoStr = "3524,1024,1024,500\r\n2844,1024,1024,500\r\n";pTree.updateProcessTree();assertTrue(pTree.getCumulativeVmem() == 2048);assertTrue(pTree.getCumulativeVmem(0) == 2048);assertTrue(pTree.getCumulativeRssmem() == 2048);assertTrue(pTree.getCumulativeRssmem(0) == 2048);assertTrue(pTree.getCumulativeCpuTime() == 1000);pTree.infoStr = "3524,1024,1024,1000\r\n2844,1024,1024,1000\r\n1234,1024,1024,1000\r\n";pTree.updateProcessTree();assertTrue(pTree.getCumulativeVmem() == 3072);assertTrue(pTree.getCumulativeVmem(1) == 2048);assertTrue(pTree.getCumulativeRssmem() == 3072);assertTrue(pTree.getCumulativeRssmem(1) == 2048);assertTrue(pTree.getCumulativeCpuTime() == 3000);pTree.infoStr = "3524,1024,1024,1500\r\n2844,1024,1024,1500\r\n";pTree.updateProcessTree();assertTrue(pTree.getCumulativeVmem() == 2048);assertTrue(pTree.getCumulativeVmem(2) == 2048);assertTrue(pTree.getCumulativeRssmem() == 2048);assertTrue(pTree.getCumulativeRssmem(2) == 2048);assertTrue(pTree.getCumulativeCpuTime() == 4000);}public ApplicationStateDataProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.applicationSubmissionContext != null) {builder.setApplicationSubmissionContext(((ApplicationSubmissionContextPBImpl) applicationSubmissionContext).getProto());}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ApplicationStateDataProto.newBuilder(proto);}viaProto = false;}
public long getSubmitTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasSubmitTime()) {return -1;}return (p.getSubmitTime());}
public void setSubmitTime(long submitTime) {maybeInitBuilder();builder.setSubmitTime(submitTime);}
public long getStartTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;return p.getStartTime();}
public void setStartTime(long startTime) {maybeInitBuilder();builder.setStartTime(startTime);}
public String getUser() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasUser()) {return null;}return (p.getUser());}
public void setUser(String user) {maybeInitBuilder();builder.setUser(user);}
public ApplicationSubmissionContext getApplicationSubmissionContext() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (applicationSubmissionContext != null) {return applicationSubmissionContext;}if (!p.hasApplicationSubmissionContext()) {return null;}applicationSubmissionContext = new ApplicationSubmissionContextPBImpl(p.getApplicationSubmissionContext());return applicationSubmissionContext;}
public void setApplicationSubmissionContext(ApplicationSubmissionContext context) {maybeInitBuilder();if (context == null) {builder.clearApplicationSubmissionContext();}this.applicationSubmissionContext = context;}
public RMAppState getState() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationState()) {return null;}return convertFromProtoFormat(p.getApplicationState());}
public void setState(RMAppState finalState) {maybeInitBuilder();if (finalState == null) {builder.clearApplicationState();return;}builder.setApplicationState(convertToProtoFormat(finalState));}
public String getDiagnostics() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasDiagnostics()) {return null;}return p.getDiagnostics();}
public void setDiagnostics(String diagnostics) {maybeInitBuilder();if (diagnostics == null) {builder.clearDiagnostics();return;}builder.setDiagnostics(diagnostics);}
public long getFinishTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;return p.getFinishTime();}
public void setFinishTime(long finishTime) {maybeInitBuilder();builder.setFinishTime(finishTime);}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
public static RMAppStateProto convertToProtoFormat(RMAppState e) {return RMAppStateProto.valueOf(RM_APP_PREFIX + e.name());}
public static RMAppState convertFromProtoFormat(RMAppStateProto e) {return RMAppState.valueOf(e.name().replace(RM_APP_PREFIX, ""));}public void testCheckpointCreate() {checkpointCreate(ByteBuffer.allocate(BUFSIZE));}
public void testCheckpointCreateDirect() {checkpointCreate(ByteBuffer.allocateDirect(BUFSIZE));}
public void checkpointCreate(ByteBuffer b) {int WRITES = 128;FileSystem fs = mock(FileSystem.class);DataOutputBuffer dob = new DataOutputBuffer();FSDataOutputStream hdfs = spy(new FSDataOutputStream(dob, null));// backed by array@SuppressWarnings("resource") DataOutputBuffer verif = new DataOutputBuffer();when(fs.create(isA(Path.class), eq((short) 1))).thenReturn(hdfs);when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);Path base = new Path("/chk");Path finalLoc = new Path("/chk/checkpoint_chk0");Path tmp = FSCheckpointService.tmpfile(finalLoc);FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService("chk0"), (short) 1);CheckpointWriteChannel out = chk.create();Random r = new Random();final byte[] randBytes = new byte[BUFSIZE];for (int i = 0; i < WRITES; ++i) {r.nextBytes(randBytes);int s = r.nextInt(BUFSIZE - 1);int e = r.nextInt(BUFSIZE - s) + 1;verif.write(randBytes, s, e);b.clear();b.put(randBytes).flip();b.position(s).limit(b.position() + e);out.write(b);}verify(fs, never()).rename(any(Path.class), eq(finalLoc));CheckpointID cid = chk.commit(out);verify(hdfs).close();verify(fs).rename(eq(tmp), eq(finalLoc));assertArrayEquals(Arrays.copyOfRange(verif.getData(), 0, verif.getLength()), Arrays.copyOfRange(dob.getData(), 0, dob.getLength()));}
public void testDelete() {FileSystem fs = mock(FileSystem.class);Path chkloc = new Path("/chk/chk0");when(fs.delete(eq(chkloc), eq(false))).thenReturn(true);Path base = new Path("/otherchk");FSCheckpointID id = new FSCheckpointID(chkloc);FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService("chk0"), (short) 1);assertTrue(chk.delete(id));verify(fs).delete(eq(chkloc), eq(false));}public long getCumulativeCpuUsage() {return cumulativeCpuUsage;}
public void setCumulativeCpuUsage(long usage) {cumulativeCpuUsage = usage;}
public long getVirtualMemoryUsage() {return virtualMemoryUsage;}
public void setVirtualMemoryUsage(long usage) {virtualMemoryUsage = usage;}
public long getPhysicalMemoryUsage() {return physicalMemoryUsage;}
public void setPhysicalMemoryUsage(long usage) {physicalMemoryUsage = usage;}
public long getHeapUsage() {return heapUsage;}
public void setHeapUsage(long usage) {heapUsage = usage;}
public int size() {int size = 0;// long #1size += WritableUtils.getVIntSize(cumulativeCpuUsage);// long #2size += WritableUtils.getVIntSize(virtualMemoryUsage);// long #3size += WritableUtils.getVIntSize(physicalMemoryUsage);// long #4size += WritableUtils.getVIntSize(heapUsage);return size;}
public void readFields(DataInput in) {// long #1cumulativeCpuUsage = WritableUtils.readVLong(in);// long #2virtualMemoryUsage = WritableUtils.readVLong(in);// long #3physicalMemoryUsage = WritableUtils.readVLong(in);// long #4heapUsage = WritableUtils.readVLong(in);}
public void write(DataOutput out) {// long #1WritableUtils.writeVLong(out, cumulativeCpuUsage);// long #2WritableUtils.writeVLong(out, virtualMemoryUsage);// long #3WritableUtils.writeVLong(out, physicalMemoryUsage);// long #4WritableUtils.writeVLong(out, heapUsage);}
private static void compareMetric(long m1, long m2, TreePath loc) {if (m1 != m2) {throw new DeepInequalityException("Value miscompared:" + loc.toString(), loc);}}
private static void compareSize(ResourceUsageMetrics m1, ResourceUsageMetrics m2, TreePath loc) {if (m1.size() != m2.size()) {throw new DeepInequalityException("Size miscompared: " + loc.toString(), loc);}}
public void deepCompare(DeepCompare other, TreePath loc) {if (!(other instanceof ResourceUsageMetrics)) {throw new DeepInequalityException("Comparand has wrong type", loc);}ResourceUsageMetrics metrics2 = (ResourceUsageMetrics) other;compareMetric(getCumulativeCpuUsage(), metrics2.getCumulativeCpuUsage(), new TreePath(loc, "cumulativeCpu"));compareMetric(getVirtualMemoryUsage(), metrics2.getVirtualMemoryUsage(), new TreePath(loc, "virtualMemory"));compareMetric(getPhysicalMemoryUsage(), metrics2.getPhysicalMemoryUsage(), new TreePath(loc, "physicalMemory"));compareMetric(getHeapUsage(), metrics2.getHeapUsage(), new TreePath(loc, "heapUsage"));compareSize(this, metrics2, new TreePath(loc, "size"));}protected void initialize(final DataInputStream in) {this.in = in;}
protected final Op readOp() {final short version = in.readShort();if (version != DataTransferProtocol.DATA_TRANSFER_VERSION) {throw new IOException("Version Mismatch (Expected: " + DataTransferProtocol.DATA_TRANSFER_VERSION + ", Received: " + version + " )");}return Op.read(in);}
protected final void processOp(Op op) {switch(op) {case READ_BLOCK:opReadBlock();break;case WRITE_BLOCK:opWriteBlock(in);break;case REPLACE_BLOCK:opReplaceBlock(in);break;case COPY_BLOCK:opCopyBlock(in);break;case BLOCK_CHECKSUM:opBlockChecksum(in);break;case TRANSFER_BLOCK:opTransferBlock(in);break;case REQUEST_SHORT_CIRCUIT_FDS:opRequestShortCircuitFds(in);break;case RELEASE_SHORT_CIRCUIT_FDS:opReleaseShortCircuitFds(in);break;case REQUEST_SHORT_CIRCUIT_SHM:opRequestShortCircuitShm(in);break;default:throw new IOException("Unknown op " + op + " in data stream");}}
private static CachingStrategy getCachingStrategy(CachingStrategyProto strategy) {Boolean dropBehind = strategy.hasDropBehind() ? strategy.getDropBehind() : null;Long readahead = strategy.hasReadahead() ? strategy.getReadahead() : null;return new CachingStrategy(dropBehind, readahead);}
private void opReadBlock() {OpReadBlockProto proto = OpReadBlockProto.parseFrom(vintPrefixed(in));readBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()), PBHelper.convert(proto.getHeader().getBaseHeader().getToken()), proto.getHeader().getClientName(), proto.getOffset(), proto.getLen(), proto.getSendChecksums(), (proto.hasCachingStrategy() ? getCachingStrategy(proto.getCachingStrategy()) : CachingStrategy.newDefaultStrategy()));}
private void opWriteBlock(DataInputStream in) {final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()), PBHelper.convertStorageType(proto.getStorageType()), PBHelper.convert(proto.getHeader().getBaseHeader().getToken()), proto.getHeader().getClientName(), targets, PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length), PBHelper.convert(proto.getSource()), fromProto(proto.getStage()), proto.getPipelineSize(), proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(), proto.getLatestGenerationStamp(), fromProto(proto.getRequestedChecksum()), (proto.hasCachingStrategy() ? getCachingStrategy(proto.getCachingStrategy()) : CachingStrategy.newDefaultStrategy()));}
private void opTransferBlock(DataInputStream in) {final OpTransferBlockProto proto = OpTransferBlockProto.parseFrom(vintPrefixed(in));final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());transferBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()), PBHelper.convert(proto.getHeader().getBaseHeader().getToken()), proto.getHeader().getClientName(), targets, PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length));}
private void opRequestShortCircuitFds(DataInputStream in) {final OpRequestShortCircuitAccessProto proto = OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));SlotId slotId = (proto.hasSlotId()) ? PBHelper.convert(proto.getSlotId()) : null;requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()), PBHelper.convert(proto.getHeader().getToken()), slotId, proto.getMaxVersion());}
private void opReleaseShortCircuitFds(DataInputStream in) {final ReleaseShortCircuitAccessRequestProto proto = ReleaseShortCircuitAccessRequestProto.parseFrom(vintPrefixed(in));releaseShortCircuitFds(PBHelper.convert(proto.getSlotId()));}
private void opRequestShortCircuitShm(DataInputStream in) {final ShortCircuitShmRequestProto proto = ShortCircuitShmRequestProto.parseFrom(vintPrefixed(in));requestShortCircuitShm(proto.getClientName());}
private void opReplaceBlock(DataInputStream in) {OpReplaceBlockProto proto = OpReplaceBlockProto.parseFrom(vintPrefixed(in));replaceBlock(PBHelper.convert(proto.getHeader().getBlock()), PBHelper.convertStorageType(proto.getStorageType()), PBHelper.convert(proto.getHeader().getToken()), proto.getDelHint(), PBHelper.convert(proto.getSource()));}
private void opCopyBlock(DataInputStream in) {OpCopyBlockProto proto = OpCopyBlockProto.parseFrom(vintPrefixed(in));copyBlock(PBHelper.convert(proto.getHeader().getBlock()), PBHelper.convert(proto.getHeader().getToken()));}
private void opBlockChecksum(DataInputStream in) {OpBlockChecksumProto proto = OpBlockChecksumProto.parseFrom(vintPrefixed(in));blockChecksum(PBHelper.convert(proto.getHeader().getBlock()), PBHelper.convert(proto.getHeader().getToken()));}public void testKeyOnlyTextOutputReader() {String text = "key,value\nkey2,value2\nnocomma\n";PipeMapRed pipeMapRed = new MyPipeMapRed(text);KeyOnlyTextOutputReader outputReader = new KeyOnlyTextOutputReader();outputReader.initialize(pipeMapRed);outputReader.readKeyValue();Assert.assertEquals(new Text("key,value"), outputReader.getCurrentKey());outputReader.readKeyValue();Assert.assertEquals(new Text("key2,value2"), outputReader.getCurrentKey());outputReader.readKeyValue();Assert.assertEquals(new Text("nocomma"), outputReader.getCurrentKey());Assert.assertEquals(false, outputReader.readKeyValue());}
public DataInput getClientInput() {return clientIn;}
public Configuration getConfiguration() {return conf;}public void setupCluster() {Configuration conf = new Configuration();conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);HAUtil.setAllowStandbyReads(conf, true);cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(1).waitSafeMode(false).build();cluster.waitActive();nn0 = cluster.getNameNode(0);nn1 = cluster.getNameNode(1);fs = HATestUtil.configureFailoverFs(cluster, conf);cluster.transitionToActive(0);}
public void shutdownCluster() {if (cluster != null) {cluster.shutdown();}}
public void testQuotasTrackedOnStandby() {fs.mkdirs(TEST_DIR);DistributedFileSystem dfs = (DistributedFileSystem) fs;dfs.setQuota(TEST_DIR, NS_QUOTA, DS_QUOTA);long expectedSize = 3 * BLOCK_SIZE + BLOCK_SIZE / 2;DFSTestUtil.createFile(fs, TEST_FILE, expectedSize, (short) 1, 1L);HATestUtil.waitForStandbyToCatchUp(nn0, nn1);ContentSummary cs = nn1.getRpcServer().getContentSummary(TEST_DIR_STR);assertEquals(NS_QUOTA, cs.getQuota());assertEquals(DS_QUOTA, cs.getSpaceQuota());assertEquals(expectedSize, cs.getSpaceConsumed());assertEquals(1, cs.getDirectoryCount());assertEquals(1, cs.getFileCount());// Append to the file and make sure quota is updated correctly.FSDataOutputStream stm = fs.append(TEST_FILE);try {byte[] data = new byte[(int) (BLOCK_SIZE * 3 / 2)];stm.write(data);expectedSize += data.length;} finally {IOUtils.closeStream(stm);}HATestUtil.waitForStandbyToCatchUp(nn0, nn1);cs = nn1.getRpcServer().getContentSummary(TEST_DIR_STR);assertEquals(NS_QUOTA, cs.getQuota());assertEquals(DS_QUOTA, cs.getSpaceQuota());assertEquals(expectedSize, cs.getSpaceConsumed());assertEquals(1, cs.getDirectoryCount());assertEquals(1, cs.getFileCount());fs.delete(TEST_FILE, true);expectedSize = 0;HATestUtil.waitForStandbyToCatchUp(nn0, nn1);cs = nn1.getRpcServer().getContentSummary(TEST_DIR_STR);assertEquals(NS_QUOTA, cs.getQuota());assertEquals(DS_QUOTA, cs.getSpaceQuota());assertEquals(expectedSize, cs.getSpaceConsumed());assertEquals(1, cs.getDirectoryCount());assertEquals(0, cs.getFileCount());}private String runJob() {OutputStream os = getFileSystem().create(new Path(getInputDir(), "text.txt"));Writer wr = new OutputStreamWriter(os);wr.write("hello1\n");wr.write("hello2\n");wr.write("hello3\n");wr.close();JobConf conf = createJobConf();conf.setJobName("mr");conf.setJobPriority(JobPriority.HIGH);conf.setInputFormat(TextInputFormat.class);conf.setMapOutputKeyClass(LongWritable.class);conf.setMapOutputValueClass(Text.class);conf.setOutputFormat(TextOutputFormat.class);conf.setOutputKeyClass(LongWritable.class);conf.setOutputValueClass(Text.class);conf.setMapperClass(org.apache.hadoop.mapred.lib.IdentityMapper.class);conf.setReducerClass(org.apache.hadoop.mapred.lib.IdentityReducer.class);FileInputFormat.setInputPaths(conf, getInputDir());FileOutputFormat.setOutputPath(conf, getOutputDir());return JobClient.runJob(conf).getID().toString();}
public static int runTool(Configuration conf, Tool tool, String[] args, OutputStream out) {return TestMRJobClient.runTool(conf, tool, args, out);}
static void verifyJobPriority(String jobId, String priority, JobConf conf) {TestMRCJCJobClient test = new TestMRCJCJobClient();test.verifyJobPriority(jobId, priority, conf, test.createJobClient());}
public void testJobClient() {Configuration conf = createJobConf();String jobId = runJob();testGetCounter(jobId, conf);testAllJobList(jobId, conf);testChangingJobPriority(jobId, conf);}
protected CLI createJobClient() {return new JobClient();}public InputStream getDataIn() {return dataIn;}
public InputStream getChecksumIn() {return checksumIn;}
public void close() {IOUtils.closeStream(dataIn);IOUtils.closeStream(checksumIn);}protected void setWaitForRatio(float ratio) {waitForRatio = ratio;}
protected float getWaitForRatio() {return waitForRatio;}
protected void sleep(long time) {try {Thread.sleep((long) (getWaitForRatio() * time));} catch (InterruptedException ex) {System.err.println(MessageFormat.format("Sleep interrupted, {0}", ex.toString()));}}
protected long waitFor(int timeout, Predicate predicate) {return waitFor(timeout, false, predicate);}
protected long waitFor(int timeout, boolean failIfTimeout, Predicate predicate) {long started = Time.now();long mustEnd = Time.now() + (long) (getWaitForRatio() * timeout);long lastEcho = 0;try {long waiting = mustEnd - Time.now();System.out.println(MessageFormat.format("Waiting up to [{0}] msec", waiting));boolean eval;while (!(eval = predicate.evaluate()) && Time.now() < mustEnd) {if ((Time.now() - lastEcho) > 5000) {waiting = mustEnd - Time.now();System.out.println(MessageFormat.format("Waiting up to [{0}] msec", waiting));lastEcho = Time.now();}Thread.sleep(100);}if (!eval) {if (failIfTimeout) {fail(MessageFormat.format("Waiting timed out after [{0}] msec", timeout));} else {System.out.println(MessageFormat.format("Waiting timed out after [{0}] msec", timeout));}}return (eval) ? Time.now() - started : -1;} catch (Exception ex) {throw new RuntimeException(ex);}}public static void registerCommands(CommandFactory factory) {factory.addClass(MoveFromLocal.class, "-moveFromLocal");factory.addClass(MoveToLocal.class, "-moveToLocal");factory.addClass(Rename.class, "-mv");}
protected void processPath(PathData src, PathData target) {// unlike copy, don't merge existing dirs during moveif (target.exists && target.stat.isDirectory()) {throw new PathExistsException(target.toString());}super.processPath(src, target);}
protected void postProcessPath(PathData src) {if (!src.fs.delete(src.path, false)) {// we have no way to know the actual error...PathIOException e = new PathIOException(src.toString());e.setOperation("remove");throw e;}}
protected void processOptions(LinkedList<String> args) {throw new IOException("Option '-moveToLocal' is not implemented yet.");}
protected void processOptions(LinkedList<String> args) {CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE);cf.parse(args);getRemoteDestination(args);}
protected void processPath(PathData src, PathData target) {if (!src.fs.getUri().equals(target.fs.getUri())) {throw new PathIOException(src.toString(), "Does not match target filesystem");}if (target.exists) {throw new PathExistsException(target.toString());}if (!target.fs.rename(src.path, target.path)) {// we have no way to know the actual error...throw new PathIOException(src.toString());}}public void trackApp(ApplicationAttemptId appAttemptId, String oldAppId) {super.trackApp(appAttemptId, oldAppId);FairScheduler fair = (FairScheduler) scheduler;final FSAppAttempt app = fair.getSchedulerApp(appAttemptId);metrics.register("variable.app." + oldAppId + ".demand.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getDemand().getMemory();}});metrics.register("variable.app." + oldAppId + ".demand.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getDemand().getVirtualCores();}});metrics.register("variable.app." + oldAppId + ".usage.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getResourceUsage().getMemory();}});metrics.register("variable.app." + oldAppId + ".usage.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getResourceUsage().getVirtualCores();}});metrics.register("variable.app." + oldAppId + ".minshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getMinShare().getMemory();}});metrics.register("variable.app." + oldAppId + ".minshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getMinShare().getMemory();}});metrics.register("variable.app." + oldAppId + ".maxshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return Math.min(app.getMaxShare().getMemory(), totalMemoryMB);}});metrics.register("variable.app." + oldAppId + ".maxshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return Math.min(app.getMaxShare().getVirtualCores(), totalVCores);}});metrics.register("variable.app." + oldAppId + ".fairshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getFairShare().getVirtualCores();}});metrics.register("variable.app." + oldAppId + ".fairshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return app.getFairShare().getVirtualCores();}});}
public Integer getValue() {return app.getDemand().getMemory();}
public Integer getValue() {return app.getDemand().getVirtualCores();}
public Integer getValue() {return app.getResourceUsage().getMemory();}
public Integer getValue() {return app.getResourceUsage().getVirtualCores();}
public Integer getValue() {return app.getMinShare().getMemory();}
public Integer getValue() {return app.getMinShare().getMemory();}
public Integer getValue() {return Math.min(app.getMaxShare().getMemory(), totalMemoryMB);}
public Integer getValue() {return Math.min(app.getMaxShare().getVirtualCores(), totalVCores);}
public Integer getValue() {return app.getFairShare().getVirtualCores();}
public Integer getValue() {return app.getFairShare().getVirtualCores();}
public void trackQueue(String queueName) {trackedQueues.add(queueName);FairScheduler fair = (FairScheduler) scheduler;final FSQueue queue = fair.getQueueManager().getQueue(queueName);metrics.register("variable.queue." + queueName + ".demand.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getDemand().getMemory();}});metrics.register("variable.queue." + queueName + ".demand.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getDemand().getVirtualCores();}});metrics.register("variable.queue." + queueName + ".usage.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getResourceUsage().getMemory();}});metrics.register("variable.queue." + queueName + ".usage.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getResourceUsage().getVirtualCores();}});metrics.register("variable.queue." + queueName + ".minshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getMinShare().getMemory();}});metrics.register("variable.queue." + queueName + ".minshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getMinShare().getVirtualCores();}});metrics.register("variable.queue." + queueName + ".maxshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {if (!maxReset && SLSRunner.simulateInfoMap.containsKey("Number of nodes") && SLSRunner.simulateInfoMap.containsKey("Node memory (MB)") && SLSRunner.simulateInfoMap.containsKey("Node VCores")) {int numNMs = Integer.parseInt(SLSRunner.simulateInfoMap.get("Number of nodes").toString());int numMemoryMB = Integer.parseInt(SLSRunner.simulateInfoMap.get("Node memory (MB)").toString());int numVCores = Integer.parseInt(SLSRunner.simulateInfoMap.get("Node VCores").toString());totalMemoryMB = numNMs * numMemoryMB;totalVCores = numNMs * numVCores;maxReset = false;}return Math.min(queue.getMaxShare().getMemory(), totalMemoryMB);}});metrics.register("variable.queue." + queueName + ".maxshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return Math.min(queue.getMaxShare().getVirtualCores(), totalVCores);}});metrics.register("variable.queue." + queueName + ".fairshare.memory", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getFairShare().getMemory();}});metrics.register("variable.queue." + queueName + ".fairshare.vcores", new Gauge<Integer>() {
@Overridepublic Integer getValue() {return queue.getFairShare().getVirtualCores();}});}
public Integer getValue() {return queue.getDemand().getMemory();}
public Integer getValue() {return queue.getDemand().getVirtualCores();}
public Integer getValue() {return queue.getResourceUsage().getMemory();}
public Integer getValue() {return queue.getResourceUsage().getVirtualCores();}
public Integer getValue() {return queue.getMinShare().getMemory();}
public Integer getValue() {return queue.getMinShare().getVirtualCores();}
public Integer getValue() {if (!maxReset && SLSRunner.simulateInfoMap.containsKey("Number of nodes") && SLSRunner.simulateInfoMap.containsKey("Node memory (MB)") && SLSRunner.simulateInfoMap.containsKey("Node VCores")) {int numNMs = Integer.parseInt(SLSRunner.simulateInfoMap.get("Number of nodes").toString());int numMemoryMB = Integer.parseInt(SLSRunner.simulateInfoMap.get("Node memory (MB)").toString());int numVCores = Integer.parseInt(SLSRunner.simulateInfoMap.get("Node VCores").toString());totalMemoryMB = numNMs * numMemoryMB;totalVCores = numNMs * numVCores;maxReset = false;}return Math.min(queue.getMaxShare().getMemory(), totalMemoryMB);}
public Integer getValue() {return Math.min(queue.getMaxShare().getVirtualCores(), totalVCores);}
public Integer getValue() {return queue.getFairShare().getMemory();}
public Integer getValue() {return queue.getFairShare().getVirtualCores();}
public void untrackQueue(String queueName) {trackedQueues.remove(queueName);metrics.remove("variable.queue." + queueName + ".demand.memory");metrics.remove("variable.queue." + queueName + ".demand.vcores");metrics.remove("variable.queue." + queueName + ".usage.memory");metrics.remove("variable.queue." + queueName + ".usage.vcores");metrics.remove("variable.queue." + queueName + ".minshare.memory");metrics.remove("variable.queue." + queueName + ".minshare.vcores");metrics.remove("variable.queue." + queueName + ".maxshare.memory");metrics.remove("variable.queue." + queueName + ".maxshare.vcores");metrics.remove("variable.queue." + queueName + ".fairshare.memory");metrics.remove("variable.queue." + queueName + ".fairshare.vcores");}public void testFactory() {Configuration conf = new Configuration();conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, UserProvider.SCHEME_NAME + ":///," + JavaKeyStoreProvider.SCHEME_NAME + "://file" + tmpDir + "/test.jks");List<CredentialProvider> providers = CredentialProviderFactory.getProviders(conf);assertEquals(2, providers.size());assertEquals(UserProvider.class, providers.get(0).getClass());assertEquals(JavaKeyStoreProvider.class, providers.get(1).getClass());assertEquals(UserProvider.SCHEME_NAME + ":///", providers.get(0).toString());assertEquals(JavaKeyStoreProvider.SCHEME_NAME + "://file" + tmpDir + "/test.jks", providers.get(1).toString());}
public void testFactoryErrors() {Configuration conf = new Configuration();conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, "unknown:///");try {List<CredentialProvider> providers = CredentialProviderFactory.getProviders(conf);assertTrue("should throw!", false);} catch (IOException e) {assertEquals("No CredentialProviderFactory for unknown:/// in " + CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, e.getMessage());}}
public void testUriErrors() {Configuration conf = new Configuration();conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, "unkn@own:/x/y");try {List<CredentialProvider> providers = CredentialProviderFactory.getProviders(conf);assertTrue("should throw!", false);} catch (IOException e) {assertEquals("Bad configuration of " + CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH + " at unkn@own:/x/y", e.getMessage());}}
private static char[] generatePassword(int length) {StringBuffer sb = new StringBuffer();Random r = new Random();for (int i = 0; i < length; i++) {sb.append(chars[r.nextInt(chars.length)]);}return sb.toString().toCharArray();}
static void checkSpecificProvider(Configuration conf, String ourUrl) {CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);char[] passwd = generatePassword(16);assertEquals(null, provider.getCredentialEntry("no-such-key"));assertEquals(null, provider.getCredentialEntry("key"));try {provider.createCredentialEntry("pass", passwd);} catch (Exception e) {e.printStackTrace();throw e;}assertArrayEquals(passwd, provider.getCredentialEntry("pass").getCredential());try {provider.createCredentialEntry("pass", passwd);assertTrue("should throw", false);} catch (IOException e) {assertEquals("Credential pass already exists in " + ourUrl, e.getMessage());}provider.deleteCredentialEntry("pass");try {provider.deleteCredentialEntry("pass");assertTrue("should throw", false);} catch (IOException e) {assertEquals("Credential pass does not exist in " + ourUrl, e.getMessage());}char[] passTwo = new char[] { '1', '2', '3' };provider.createCredentialEntry("pass", passwd);provider.createCredentialEntry("pass2", passTwo);assertArrayEquals(passTwo, provider.getCredentialEntry("pass2").getCredential());// write them to disk so that configuration.getPassword will find themprovider.flush();// configuration.getPassword should get this from providerassertArrayEquals(passTwo, conf.getPassword("pass2"));// configuration.getPassword should get this from configconf.set("onetwothree", "123");assertArrayEquals(passTwo, conf.getPassword("onetwothree"));// we are disabling the fallback to clear text configconf.set(CredentialProvider.CLEAR_TEXT_FALLBACK, "false");assertArrayEquals(null, conf.getPassword("onetwothree"));// get a new instance of the provider to ensure it was saved correctlyprovider = CredentialProviderFactory.getProviders(conf).get(0);assertTrue(provider != null);assertArrayEquals(new char[] { '1', '2', '3' }, provider.getCredentialEntry("pass2").getCredential());assertArrayEquals(passwd, provider.getCredentialEntry("pass").getCredential());List<String> creds = provider.getAliases();assertTrue("Credentials should have been returned.", creds.size() == 2);assertTrue("Returned Credentials should have included pass.", creds.contains("pass"));assertTrue("Returned Credentials should have included pass2.", creds.contains("pass2"));}
public void testUserProvider() {Configuration conf = new Configuration();final String ourUrl = UserProvider.SCHEME_NAME + ":///";conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, ourUrl);checkSpecificProvider(conf, ourUrl);// see if the credentials are actually in the UGICredentials credentials = UserGroupInformation.getCurrentUser().getCredentials();assertArrayEquals(new byte[] { '1', '2', '3' }, credentials.getSecretKey(new Text("pass2")));}
public void testJksProvider() {Configuration conf = new Configuration();final String ourUrl = JavaKeyStoreProvider.SCHEME_NAME + "://file" + tmpDir + "/test.jks";File file = new File(tmpDir, "test.jks");file.delete();conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, ourUrl);checkSpecificProvider(conf, ourUrl);Path path = ProviderUtils.unnestUri(new URI(ourUrl));FileSystem fs = path.getFileSystem(conf);FileStatus s = fs.getFileStatus(path);assertTrue(s.getPermission().toString().equals("rwx------"));assertTrue(file + " should exist", file.isFile());// check permission retention after explicit changefs.setPermission(path, new FsPermission("777"));checkPermissionRetention(conf, ourUrl, path);}
public void checkPermissionRetention(Configuration conf, String ourUrl, Path path) {CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);// let's add a new credential and flush and check that permissions are still set to 777char[] cred = new char[32];for (int i = 0; i < cred.length; ++i) {cred[i] = (char) i;}// create a new keytry {provider.createCredentialEntry("key5", cred);} catch (Exception e) {e.printStackTrace();throw e;}provider.flush();// get a new instance of the provider to ensure it was saved correctlyprovider = CredentialProviderFactory.getProviders(conf).get(0);assertArrayEquals(cred, provider.getCredentialEntry("key5").getCredential());FileSystem fs = path.getFileSystem(conf);FileStatus s = fs.getFileStatus(path);assertTrue("Permissions should have been retained from the preexisting " + "keystore.", s.getPermission().toString().equals("rwxrwxrwx"));}public void setUp() {fc.mkdir(fileContextTestHelper.getTestRootPath(fc), FileContext.DEFAULT_PERM, true);}
public void tearDown() {fc.delete(fileContextTestHelper.getTestRootPath(fc), true);}
public void testFcCopy() {final String ts = "some random text";Path file1 = fileContextTestHelper.getTestRootPath(fc, "file1");Path file2 = fileContextTestHelper.getTestRootPath(fc, "file2");writeFile(fc, file1, ts.getBytes());assertTrue(fc.util().exists(file1));fc.util().copy(file1, file2);// verify that newly copied file2 existsassertTrue("Failed to copy file2  ", fc.util().exists(file2));// verify that file2 contains test stringassertTrue("Copied files does not match ", Arrays.equals(ts.getBytes(), readFile(fc, file2, ts.getBytes().length)));}
public void testRecursiveFcCopy() {final String ts = "some random text";Path dir1 = fileContextTestHelper.getTestRootPath(fc, "dir1");Path dir2 = fileContextTestHelper.getTestRootPath(fc, "dir2");Path file1 = new Path(dir1, "file1");fc.mkdir(dir1, null, false);writeFile(fc, file1, ts.getBytes());assertTrue(fc.util().exists(file1));Path file2 = new Path(dir2, "file1");fc.util().copy(dir1, dir2);// verify that newly copied file2 existsassertTrue("Failed to copy file2  ", fc.util().exists(file2));// verify that file2 contains test stringassertTrue("Copied files does not match ", Arrays.equals(ts.getBytes(), readFile(fc, file2, ts.getBytes().length)));}public void setFairShare(Resource resource) {fairShareMB.set(resource.getMemory());fairShareVCores.set(resource.getVirtualCores());}
public int getFairShareMB() {return fairShareMB.value();}
public int getFairShareVirtualCores() {return fairShareVCores.value();}
public void setSteadyFairShare(Resource resource) {steadyFairShareMB.set(resource.getMemory());steadyFairShareVCores.set(resource.getVirtualCores());}
public int getSteadyFairShareMB() {return steadyFairShareMB.value();}
public int getSteadyFairShareVCores() {return steadyFairShareVCores.value();}
public void setMinShare(Resource resource) {minShareMB.set(resource.getMemory());minShareVCores.set(resource.getVirtualCores());}
public int getMinShareMB() {return minShareMB.value();}
public int getMinShareVirtualCores() {return minShareVCores.value();}
public void setMaxShare(Resource resource) {maxShareMB.set(resource.getMemory());maxShareVCores.set(resource.getVirtualCores());}
public int getMaxShareMB() {return maxShareMB.value();}
public int getMaxShareVirtualCores() {return maxShareVCores.value();}
public static synchronized FSQueueMetrics forQueue(String queueName, Queue parent, boolean enableUserMetrics, Configuration conf) {MetricsSystem ms = DefaultMetricsSystem.instance();QueueMetrics metrics = queueMetrics.get(queueName);if (metrics == null) {metrics = new FSQueueMetrics(ms, queueName, parent, enableUserMetrics, conf).tag(QUEUE_INFO, queueName);// Register with the MetricsSystemsif (ms != null) {metrics = ms.register(sourceName(queueName).toString(), "Metrics for queue: " + queueName, metrics);}queueMetrics.put(queueName, metrics);}return (FSQueueMetrics) metrics;}public void initServerAndWait(String fsimage) {initServer(fsimage);try {channel.getCloseFuture().await();} catch (InterruptedException e) {LOG.info("Interrupted. Stopping the WebImageViewer.");shutdown();}}
public void initServer(String fsimage) {FSImageLoader loader = FSImageLoader.load(fsimage);ChannelPipeline pipeline = Channels.pipeline();pipeline.addLast("channelTracker", new SimpleChannelUpstreamHandler() {
@Overridepublic void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {allChannels.add(e.getChannel());}});pipeline.addLast("httpDecoder", new HttpRequestDecoder());pipeline.addLast("requestHandler", new FSImageHandler(loader));pipeline.addLast("stringEncoder", new StringEncoder());pipeline.addLast("httpEncoder", new HttpResponseEncoder());bootstrap.setPipeline(pipeline);channel = bootstrap.bind(address);allChannels.add(channel);address = (InetSocketAddress) channel.getLocalAddress();LOG.info("WebImageViewer started. Listening on " + address.toString() + ". Press Ctrl+C to stop the viewer.");}
public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent e) {allChannels.add(e.getChannel());}
public void shutdown() {allChannels.close().awaitUninterruptibly();factory.releaseExternalResources();}
public int getPort() {return address.getPort();}public void setupMocksAndRenewer() {MOCK_DFSCLIENT = createMockClient();renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);renewer.setGraceSleepPeriod(FAST_GRACE_PERIOD);}
private DFSClient createMockClient() {DFSClient mock = Mockito.mock(DFSClient.class);Mockito.doReturn(true).when(mock).isClientRunning();Mockito.doReturn((int) FAST_GRACE_PERIOD).when(mock).getHdfsTimeout();Mockito.doReturn("myclient").when(mock).getClientName();return mock;}
public void testInstanceSharing() {// the same instanceLeaseRenewer lr = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);LeaseRenewer lr2 = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);Assert.assertSame(lr, lr2);// But a different UGI should return a different instanceLeaseRenewer lr3 = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_B, MOCK_DFSCLIENT);Assert.assertNotSame(lr, lr3);// instance.LeaseRenewer lr4 = LeaseRenewer.getInstance("someOtherAuthority", FAKE_UGI_B, MOCK_DFSCLIENT);Assert.assertNotSame(lr, lr4);Assert.assertNotSame(lr3, lr4);}
public void testRenewal() {// Keep track of how many times the lease gets renewedfinal AtomicInteger leaseRenewalCount = new AtomicInteger();Mockito.doAnswer(new Answer<Boolean>() {
@Overridepublic Boolean answer(InvocationOnMock invocation) throws Throwable {leaseRenewalCount.incrementAndGet();return true;}}).when(MOCK_DFSCLIENT).renewLease();// Set up a file so that we start renewing our lease.DFSOutputStream mockStream = Mockito.mock(DFSOutputStream.class);long fileId = 123L;renewer.put(fileId, mockStream, MOCK_DFSCLIENT);// Wait for lease to get renewedlong failTime = Time.now() + 5000;while (Time.now() < failTime && leaseRenewalCount.get() == 0) {Thread.sleep(50);}if (leaseRenewalCount.get() == 0) {Assert.fail("Did not renew lease at all!");}renewer.closeFile(fileId, MOCK_DFSCLIENT);}
public Boolean answer(InvocationOnMock invocation) {leaseRenewalCount.incrementAndGet();return true;}
public void testManyDfsClientsWhereSomeNotOpen() {// First DFSClient has no files open so doesn't renew leases.final DFSClient mockClient1 = createMockClient();Mockito.doReturn(false).when(mockClient1).renewLease();assertSame(renewer, LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, mockClient1));// Set up a file so that we start renewing our lease.DFSOutputStream mockStream1 = Mockito.mock(DFSOutputStream.class);long fileId = 456L;renewer.put(fileId, mockStream1, mockClient1);// Second DFSClient does renew leasefinal DFSClient mockClient2 = createMockClient();Mockito.doReturn(true).when(mockClient2).renewLease();assertSame(renewer, LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, mockClient2));// Set up a file so that we start renewing our lease.DFSOutputStream mockStream2 = Mockito.mock(DFSOutputStream.class);renewer.put(fileId, mockStream2, mockClient2);// Wait for lease to get renewedGenericTestUtils.waitFor(new Supplier<Boolean>() {
@Overridepublic Boolean get() {try {Mockito.verify(mockClient1, Mockito.atLeastOnce()).renewLease();Mockito.verify(mockClient2, Mockito.atLeastOnce()).renewLease();return true;} catch (AssertionError err) {LeaseRenewer.LOG.warn("Not yet satisfied", err);return false;} catch (IOException e) {throw new RuntimeException(e);}}}, 100, 10000);renewer.closeFile(fileId, mockClient1);renewer.closeFile(fileId, mockClient2);}
public Boolean get() {try {Mockito.verify(mockClient1, Mockito.atLeastOnce()).renewLease();Mockito.verify(mockClient2, Mockito.atLeastOnce()).renewLease();return true;} catch (AssertionError err) {LeaseRenewer.LOG.warn("Not yet satisfied", err);return false;} catch (IOException e) {throw new RuntimeException(e);}}
public void testThreadName() {DFSOutputStream mockStream = Mockito.mock(DFSOutputStream.class);long fileId = 789L;Assert.assertFalse("Renewer not initially running", renewer.isRunning());// Pretend to open a filerenewer.put(fileId, mockStream, MOCK_DFSCLIENT);Assert.assertTrue("Renewer should have started running", renewer.isRunning());// Check the thread name is reasonableString threadName = renewer.getDaemonName();Assert.assertEquals("LeaseRenewer:myuser@hdfs://nn1/", threadName);// Pretend to close the filerenewer.closeFile(fileId, MOCK_DFSCLIENT);renewer.setEmptyTime(Time.now());// Should stop the renewer running within a few secondslong failTime = Time.now() + 5000;while (renewer.isRunning() && Time.now() < failTime) {Thread.sleep(50);}Assert.assertFalse(renewer.isRunning());}public RegisterNodeManagerResponseProto registerNodeManager(RpcController controller, RegisterNodeManagerRequestProto proto) {RegisterNodeManagerRequestPBImpl request = new RegisterNodeManagerRequestPBImpl(proto);try {RegisterNodeManagerResponse response = real.registerNodeManager(request);return ((RegisterNodeManagerResponsePBImpl) response).getProto();} catch (YarnException e) {throw new ServiceException(e);} catch (IOException e) {throw new ServiceException(e);}}
public NodeHeartbeatResponseProto nodeHeartbeat(RpcController controller, NodeHeartbeatRequestProto proto) {NodeHeartbeatRequestPBImpl request = new NodeHeartbeatRequestPBImpl(proto);try {NodeHeartbeatResponse response = real.nodeHeartbeat(request);return ((NodeHeartbeatResponsePBImpl) response).getProto();} catch (YarnException e) {throw new ServiceException(e);} catch (IOException e) {throw new ServiceException(e);}}public GetCountersResponseProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.counters != null) {builder.setCounters(convertToProtoFormat(this.counters));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = GetCountersResponseProto.newBuilder(proto);}viaProto = false;}
public Counters getCounters() {GetCountersResponseProtoOrBuilder p = viaProto ? proto : builder;if (this.counters != null) {return this.counters;}if (!p.hasCounters()) {return null;}this.counters = convertFromProtoFormat(p.getCounters());return this.counters;}
public void setCounters(Counters counters) {maybeInitBuilder();if (counters == null)builder.clearCounters();this.counters = counters;}
private CountersPBImpl convertFromProtoFormat(CountersProto p) {return new CountersPBImpl(p);}
private CountersProto convertToProtoFormat(Counters t) {return ((CountersPBImpl) t).getProto();}public void setUp() {conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512L);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();cluster.waitActive();fs = cluster.getFileSystem();}
public void tearDown() {cluster.shutdown();}
public void testShutdown() {if (System.getProperty("os.name").startsWith("Windows")) {/**   * This test depends on OS not allowing file creations on a directory   * that does not have write permissions for the user. Apparently it is* not the case on Windows (at least under Cygwin), and possibly AIX.   * This is disabled on Windows.   */return;}// Bring up two more datanodescluster.startDataNodes(conf, 2, true, null, null);cluster.waitActive();final int dnIndex = 0;String bpid = cluster.getNamesystem().getBlockPoolId();File storageDir = cluster.getInstanceStorageDir(dnIndex, 0);File dir1 = MiniDFSCluster.getRbwDir(storageDir, bpid);storageDir = cluster.getInstanceStorageDir(dnIndex, 1);File dir2 = MiniDFSCluster.getRbwDir(storageDir, bpid);try {// make the data directory of the first datanode to be readonlyassertTrue("Couldn't chmod local vol", dir1.setReadOnly());assertTrue("Couldn't chmod local vol", dir2.setReadOnly());// create files and make sure that first datanode will be downDataNode dn = cluster.getDataNodes().get(dnIndex);for (int i = 0; dn.isDatanodeUp(); i++) {Path fileName = new Path("/test.txt" + i);DFSTestUtil.createFile(fs, fileName, 1024, (short) 2, 1L);DFSTestUtil.waitReplication(fs, fileName, (short) 2);fs.delete(fileName, true);}} finally {// restore its old permissionFileUtil.setWritable(dir1, true);FileUtil.setWritable(dir2, true);}}
public void testReplicationError() {// create a file of replication factor of 1final Path fileName = new Path("/test.txt");final int fileLen = 1;DFSTestUtil.createFile(fs, fileName, 1, (short) 1, 1L);DFSTestUtil.waitReplication(fs, fileName, (short) 1);// get the block belonged to the created fileLocatedBlocks blocks = NameNodeAdapter.getBlockLocations(cluster.getNameNode(), fileName.toString(), 0, (long) fileLen);assertEquals("Should only find 1 block", blocks.locatedBlockCount(), 1);LocatedBlock block = blocks.get(0);// bring up a second datanodecluster.startDataNodes(conf, 1, true, null, null);cluster.waitActive();final int sndNode = 1;DataNode datanode = cluster.getDataNodes().get(sndNode);// replicate the block to the second datanodeInetSocketAddress target = datanode.getXferAddress();Socket s = new Socket(target.getAddress(), target.getPort());// write the header.DataOutputStream out = new DataOutputStream(s.getOutputStream());DataChecksum checksum = DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512);new Sender(out).writeBlock(block.getBlock(), StorageType.DEFAULT, BlockTokenSecretManager.DUMMY_TOKEN, "", new DatanodeInfo[0], new StorageType[0], null, BlockConstructionStage.PIPELINE_SETUP_CREATE, 1, 0L, 0L, 0L, checksum, CachingStrategy.newDefaultStrategy());out.flush();// close the connection before sending the content of the blockout.close();// the temporary block & meta files should be deletedString bpid = cluster.getNamesystem().getBlockPoolId();File storageDir = cluster.getInstanceStorageDir(sndNode, 0);File dir1 = MiniDFSCluster.getRbwDir(storageDir, bpid);storageDir = cluster.getInstanceStorageDir(sndNode, 1);File dir2 = MiniDFSCluster.getRbwDir(storageDir, bpid);while (dir1.listFiles().length != 0 || dir2.listFiles().length != 0) {Thread.sleep(100);}// then increase the file's replication factorfs.setReplication(fileName, (short) 2);// replication should succeedDFSTestUtil.waitReplication(fs, fileName, (short) 1);// clean up the filefs.delete(fileName, false);}
public void testLocalDirs() {Configuration conf = new Configuration();final String permStr = conf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY);FsPermission expected = new FsPermission(permStr);// Check permissions on directories in 'dfs.datanode.data.dir'FileSystem localFS = FileSystem.getLocal(conf);for (DataNode dn : cluster.getDataNodes()) {for (FsVolumeSpi v : dn.getFSDataset().getVolumes()) {String dir = v.getBasePath();Path dataDir = new Path(dir);FsPermission actual = localFS.getFileStatus(dataDir).getPermission();assertEquals("Permission for dir: " + dataDir + ", is " + actual + ", while expected is " + expected, expected, actual);}}}
public void testcheckDiskError() {if (cluster.getDataNodes().size() <= 0) {cluster.startDataNodes(conf, 1, true, null, null);cluster.waitActive();}DataNode dataNode = cluster.getDataNodes().get(0);long slackTime = dataNode.checkDiskErrorInterval / 2;//checking for disk errordataNode.checkDiskErrorAsync();Thread.sleep(dataNode.checkDiskErrorInterval);long lastDiskErrorCheck = dataNode.getLastDiskErrorCheck();assertTrue("Disk Error check is not performed within  " + dataNode.checkDiskErrorInterval + "  ms", ((Time.monotonicNow() - lastDiskErrorCheck) < (dataNode.checkDiskErrorInterval + slackTime)));}protected AbstractFSContract createContract(Configuration conf) {return new NativeS3Contract(conf);}public Pattern compiled() {return compiled;}
public static Pattern compile(String globPattern) {return new GlobPattern(globPattern).compiled();}
public boolean matches(CharSequence s) {return compiled.matcher(s).matches();}
public void set(String glob) {StringBuilder regex = new StringBuilder();int setOpen = 0;int curlyOpen = 0;int len = glob.length();hasWildcard = false;for (int i = 0; i < len; i++) {char c = glob.charAt(i);switch(c) {case BACKSLASH:if (++i >= len) {error("Missing escaped character", glob, i);}regex.append(c).append(glob.charAt(i));continue;case '.':case '$':case '(':case ')':case '|':case '+':// escape regex special chars that are not glob special charsregex.append(BACKSLASH);break;case '*':regex.append('.');hasWildcard = true;break;case '?':regex.append('.');hasWildcard = true;continue;case // start of a group'{':// non-capturingregex.append("(?:");curlyOpen++;hasWildcard = true;continue;case ',':regex.append(curlyOpen > 0 ? '|' : c);continue;case '}':if (curlyOpen > 0) {// end of a groupcurlyOpen--;regex.append(")");continue;}break;case '[':if (setOpen > 0) {error("Unclosed character class", glob, i);}setOpen++;hasWildcard = true;break;case // ^ inside [...] can be unescaped'^':if (setOpen == 0) {regex.append(BACKSLASH);}break;case // [! needs to be translated to [^'!':regex.append(setOpen > 0 && '[' == glob.charAt(i - 1) ? '^' : '!');continue;case ']':// We'll just let the regex compiler do the real work.setOpen = 0;break;default:}regex.append(c);}if (setOpen > 0) {error("Unclosed character class", glob, len);}if (curlyOpen > 0) {error("Unclosed group", glob, len);}compiled = Pattern.compile(regex.toString());}
public boolean hasWildcard() {return hasWildcard;}
private static void error(String message, String pattern, int pos) {throw new PatternSyntaxException(message, pattern, pos);}private static int buckets2words(int vectorSize) {return ((vectorSize - 1) >>> 4) + 1;}
public void add(Key key) {if (key == null) {throw new NullPointerException("key can not be null");}int[] h = hash.hash(key);hash.clear();for (int i = 0; i < nbHash; i++) {// div 16int wordNum = h[i] >> 4;// (mod 16) * 4int bucketShift = (h[i] & 0x0f) << 2;long bucketMask = 15L << bucketShift;long bucketValue = (buckets[wordNum] & bucketMask) >>> bucketShift;// only increment if the count in the bucket is less than BUCKET_MAX_VALUEif (bucketValue < BUCKET_MAX_VALUE) {// increment by 1buckets[wordNum] = (buckets[wordNum] & ~bucketMask) | ((bucketValue + 1) << bucketShift);}}}
public void delete(Key key) {if (key == null) {throw new NullPointerException("Key may not be null");}if (!membershipTest(key)) {throw new IllegalArgumentException("Key is not a member");}int[] h = hash.hash(key);hash.clear();for (int i = 0; i < nbHash; i++) {// div 16int wordNum = h[i] >> 4;// (mod 16) * 4int bucketShift = (h[i] & 0x0f) << 2;long bucketMask = 15L << bucketShift;long bucketValue = (buckets[wordNum] & bucketMask) >>> bucketShift;// only decrement if the count in the bucket is between 0 and BUCKET_MAX_VALUEif (bucketValue >= 1 && bucketValue < BUCKET_MAX_VALUE) {// decrement by 1buckets[wordNum] = (buckets[wordNum] & ~bucketMask) | ((bucketValue - 1) << bucketShift);}}}
public void and(Filter filter) {if (filter == null || !(filter instanceof CountingBloomFilter) || filter.vectorSize != this.vectorSize || filter.nbHash != this.nbHash) {throw new IllegalArgumentException("filters cannot be and-ed");}CountingBloomFilter cbf = (CountingBloomFilter) filter;int sizeInWords = buckets2words(vectorSize);for (int i = 0; i < sizeInWords; i++) {this.buckets[i] &= cbf.buckets[i];}}
public boolean membershipTest(Key key) {if (key == null) {throw new NullPointerException("Key may not be null");}int[] h = hash.hash(key);hash.clear();for (int i = 0; i < nbHash; i++) {// div 16int wordNum = h[i] >> 4;// (mod 16) * 4int bucketShift = (h[i] & 0x0f) << 2;long bucketMask = 15L << bucketShift;if ((buckets[wordNum] & bucketMask) == 0) {return false;}}return true;}
public int approximateCount(Key key) {int res = Integer.MAX_VALUE;int[] h = hash.hash(key);hash.clear();for (int i = 0; i < nbHash; i++) {// div 16int wordNum = h[i] >> 4;// (mod 16) * 4int bucketShift = (h[i] & 0x0f) << 2;long bucketMask = 15L << bucketShift;long bucketValue = (buckets[wordNum] & bucketMask) >>> bucketShift;if (bucketValue < res)res = (int) bucketValue;}if (res != Integer.MAX_VALUE) {return res;} else {return 0;}}
public void not() {throw new UnsupportedOperationException("not() is undefined for " + this.getClass().getName());}
public void or(Filter filter) {if (filter == null || !(filter instanceof CountingBloomFilter) || filter.vectorSize != this.vectorSize || filter.nbHash != this.nbHash) {throw new IllegalArgumentException("filters cannot be or-ed");}CountingBloomFilter cbf = (CountingBloomFilter) filter;int sizeInWords = buckets2words(vectorSize);for (int i = 0; i < sizeInWords; i++) {this.buckets[i] |= cbf.buckets[i];}}
public void xor(Filter filter) {throw new UnsupportedOperationException("xor() is undefined for " + this.getClass().getName());}
public String toString() {StringBuilder res = new StringBuilder();for (int i = 0; i < vectorSize; i++) {if (i > 0) {res.append(" ");}// div 16int wordNum = i >> 4;// (mod 16) * 4int bucketShift = (i & 0x0f) << 2;long bucketMask = 15L << bucketShift;long bucketValue = (buckets[wordNum] & bucketMask) >>> bucketShift;res.append(bucketValue);}return res.toString();}
public void write(DataOutput out) {super.write(out);int sizeInWords = buckets2words(vectorSize);for (int i = 0; i < sizeInWords; i++) {out.writeLong(buckets[i]);}}
public void readFields(DataInput in) {super.readFields(in);int sizeInWords = buckets2words(vectorSize);buckets = new long[sizeInWords];for (int i = 0; i < sizeInWords; i++) {buckets[i] = in.readLong();}}public void close() {if (this.proxy != null) {RPC.stopProxy(this.proxy);}}
public GetApplicationReportResponse getApplicationReport(GetApplicationReportRequest request) {GetApplicationReportRequestProto requestProto = ((GetApplicationReportRequestPBImpl) request).getProto();try {return new GetApplicationReportResponsePBImpl(proxy.getApplicationReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationsResponse getApplications(GetApplicationsRequest request) {GetApplicationsRequestProto requestProto = ((GetApplicationsRequestPBImpl) request).getProto();try {return new GetApplicationsResponsePBImpl(proxy.getApplications(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationAttemptReportResponse getApplicationAttemptReport(GetApplicationAttemptReportRequest request) {GetApplicationAttemptReportRequestProto requestProto = ((GetApplicationAttemptReportRequestPBImpl) request).getProto();try {return new GetApplicationAttemptReportResponsePBImpl(proxy.getApplicationAttemptReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationAttemptsResponse getApplicationAttempts(GetApplicationAttemptsRequest request) {GetApplicationAttemptsRequestProto requestProto = ((GetApplicationAttemptsRequestPBImpl) request).getProto();try {return new GetApplicationAttemptsResponsePBImpl(proxy.getApplicationAttempts(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetContainerReportResponse getContainerReport(GetContainerReportRequest request) {GetContainerReportRequestProto requestProto = ((GetContainerReportRequestPBImpl) request).getProto();try {return new GetContainerReportResponsePBImpl(proxy.getContainerReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetContainersResponse getContainers(GetContainersRequest request) {GetContainersRequestProto requestProto = ((GetContainersRequestPBImpl) request).getProto();try {return new GetContainersResponsePBImpl(proxy.getContainers(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetDelegationTokenResponse getDelegationToken(GetDelegationTokenRequest request) {GetDelegationTokenRequestProto requestProto = ((GetDelegationTokenRequestPBImpl) request).getProto();try {return new GetDelegationTokenResponsePBImpl(proxy.getDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public RenewDelegationTokenResponse renewDelegationToken(RenewDelegationTokenRequest request) {RenewDelegationTokenRequestProto requestProto = ((RenewDelegationTokenRequestPBImpl) request).getProto();try {return new RenewDelegationTokenResponsePBImpl(proxy.renewDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public CancelDelegationTokenResponse cancelDelegationToken(CancelDelegationTokenRequest request) {CancelDelegationTokenRequestProto requestProto = ((CancelDelegationTokenRequestPBImpl) request).getProto();try {return new CancelDelegationTokenResponsePBImpl(proxy.cancelDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}public short applyNewPermission(FileStatus file) {FsPermission perms = file.getPermission();int existing = perms.toShort();boolean exeOk = file.isDirectory() || (existing & 0111) != 0;return (short) combineModes(existing, exeOk);}public JobId getJobId() {return jobID;}public void initialize(PipeMapRed pipeMapRed) {super.initialize(pipeMapRed);clientIn = pipeMapRed.getClientInput();conf = pipeMapRed.getConfiguration();numKeyFields = pipeMapRed.getNumOfKeyFields();separator = pipeMapRed.getFieldSeparator();lineReader = new LineReader((InputStream) clientIn, conf);key = new Text();value = new Text();line = new Text();}
public boolean readKeyValue() {if (lineReader.readLine(line) <= 0) {return false;}bytes = line.getBytes();splitKeyVal(bytes, line.getLength(), key, value);line.clear();return true;}
public Text getCurrentKey() {return key;}
public Text getCurrentValue() {return value;}
public String getLastOutput() {if (bytes != null) {try {return new String(bytes, "UTF-8");} catch (UnsupportedEncodingException e) {return "<undecodable>";}} else {return null;}}
private void splitKeyVal(byte[] line, int length, Text key, Text val) {// Need to find numKeyFields separatorsint pos = UTF8ByteArrayUtils.findBytes(line, 0, length, separator);for (int k = 1; k < numKeyFields && pos != -1; k++) {pos = UTF8ByteArrayUtils.findBytes(line, pos + separator.length, length, separator);}try {if (pos == -1) {key.set(line, 0, length);val.set("");} else {StreamKeyValUtil.splitKeyVal(line, 0, length, key, val, pos, separator.length);}} catch (CharacterCodingException e) {throw new IOException(StringUtils.stringifyException(e));}}public String getMessage() {return this.message;}public TaskId getTaskID() {return taskID;}public static NMContainerStatus newInstance(ContainerId containerId, ContainerState containerState, Resource allocatedResource, String diagnostics, int containerExitStatus, Priority priority, long creationTime) {NMContainerStatus status = Records.newRecord(NMContainerStatus.class);status.setContainerId(containerId);status.setContainerState(containerState);status.setAllocatedResource(allocatedResource);status.setDiagnostics(diagnostics);status.setContainerExitStatus(containerExitStatus);status.setPriority(priority);status.setCreationTime(creationTime);return status;}public RefreshUserToGroupsMappingsResponseProto refreshUserToGroupsMappings(RpcController controller, RefreshUserToGroupsMappingsRequestProto request) {try {impl.refreshUserToGroupsMappings();} catch (IOException e) {throw new ServiceException(e);}return VOID_REFRESH_USER_GROUPS_MAPPING_RESPONSE;}
public RefreshSuperUserGroupsConfigurationResponseProto refreshSuperUserGroupsConfiguration(RpcController controller, RefreshSuperUserGroupsConfigurationRequestProto request) {try {impl.refreshSuperUserGroupsConfiguration();} catch (IOException e) {throw new ServiceException(e);}return VOID_REFRESH_SUPERUSER_GROUPS_CONFIGURATION_RESPONSE;}public long getDataLength() {return dataLength;}
public byte[] getChecksum() {return checksum;}public NodeHealthStatusProto getProto() {mergeLocalToProto();this.proto = this.viaProto ? this.proto : this.builder.build();this.viaProto = true;return this.proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToProto() {if (this.viaProto)maybeInitBuilder();this.proto = this.builder.build();this.viaProto = true;}
private void maybeInitBuilder() {if (this.viaProto || this.builder == null) {this.builder = NodeHealthStatusProto.newBuilder(this.proto);}this.viaProto = false;}
public boolean getIsNodeHealthy() {NodeHealthStatusProtoOrBuilder p = this.viaProto ? this.proto : this.builder;return p.getIsNodeHealthy();}
public void setIsNodeHealthy(boolean isNodeHealthy) {maybeInitBuilder();this.builder.setIsNodeHealthy(isNodeHealthy);}
public String getHealthReport() {NodeHealthStatusProtoOrBuilder p = this.viaProto ? this.proto : this.builder;if (!p.hasHealthReport()) {return null;}return (p.getHealthReport());}
public void setHealthReport(String healthReport) {maybeInitBuilder();if (healthReport == null) {this.builder.clearHealthReport();return;}this.builder.setHealthReport((healthReport));}
public long getLastHealthReportTime() {NodeHealthStatusProtoOrBuilder p = this.viaProto ? this.proto : this.builder;return (p.getLastHealthReportTime());}
public void setLastHealthReportTime(long lastHealthReport) {maybeInitBuilder();this.builder.setLastHealthReportTime((lastHealthReport));}public void testJobConf() {JobConf conf = new JobConf();// test default valuePattern pattern = conf.getJarUnpackPattern();assertEquals(Pattern.compile("(?:classes/|lib/).*").toString(), pattern.toString());// default valueassertFalse(conf.getKeepFailedTaskFiles());conf.setKeepFailedTaskFiles(true);assertTrue(conf.getKeepFailedTaskFiles());// default valueassertNull(conf.getKeepTaskFilesPattern());conf.setKeepTaskFilesPattern("123454");assertEquals("123454", conf.getKeepTaskFilesPattern());// default valueassertNotNull(conf.getWorkingDirectory());conf.setWorkingDirectory(new Path("test"));assertTrue(conf.getWorkingDirectory().toString().endsWith("test"));// default valueassertEquals(1, conf.getNumTasksToExecutePerJvm());// default valueassertNull(conf.getKeyFieldComparatorOption());conf.setKeyFieldComparatorOptions("keySpec");assertEquals("keySpec", conf.getKeyFieldComparatorOption());// default valueassertFalse(conf.getUseNewReducer());conf.setUseNewReducer(true);assertTrue(conf.getUseNewReducer());// defaultassertTrue(conf.getMapSpeculativeExecution());assertTrue(conf.getReduceSpeculativeExecution());assertTrue(conf.getSpeculativeExecution());conf.setReduceSpeculativeExecution(false);assertTrue(conf.getSpeculativeExecution());conf.setMapSpeculativeExecution(false);assertFalse(conf.getSpeculativeExecution());assertFalse(conf.getMapSpeculativeExecution());assertFalse(conf.getReduceSpeculativeExecution());conf.setSessionId("ses");assertEquals("ses", conf.getSessionId());assertEquals(3, conf.getMaxTaskFailuresPerTracker());conf.setMaxTaskFailuresPerTracker(2);assertEquals(2, conf.getMaxTaskFailuresPerTracker());assertEquals(0, conf.getMaxMapTaskFailuresPercent());conf.setMaxMapTaskFailuresPercent(50);assertEquals(50, conf.getMaxMapTaskFailuresPercent());assertEquals(0, conf.getMaxReduceTaskFailuresPercent());conf.setMaxReduceTaskFailuresPercent(70);assertEquals(70, conf.getMaxReduceTaskFailuresPercent());// by defaultassertEquals(JobPriority.NORMAL.name(), conf.getJobPriority().name());conf.setJobPriority(JobPriority.HIGH);assertEquals(JobPriority.HIGH.name(), conf.getJobPriority().name());assertNull(conf.getJobSubmitHostName());conf.setJobSubmitHostName("hostname");assertEquals("hostname", conf.getJobSubmitHostName());// defaultassertNull(conf.getJobSubmitHostAddress());conf.setJobSubmitHostAddress("ww");assertEquals("ww", conf.getJobSubmitHostAddress());// default valueassertFalse(conf.getProfileEnabled());conf.setProfileEnabled(true);assertTrue(conf.getProfileEnabled());// default valueassertEquals(conf.getProfileTaskRange(true).toString(), "0-2");assertEquals(conf.getProfileTaskRange(false).toString(), "0-2");conf.setProfileTaskRange(true, "0-3");assertEquals(conf.getProfileTaskRange(false).toString(), "0-2");assertEquals(conf.getProfileTaskRange(true).toString(), "0-3");// default valueassertNull(conf.getMapDebugScript());conf.setMapDebugScript("mDbgScript");assertEquals("mDbgScript", conf.getMapDebugScript());// default valueassertNull(conf.getReduceDebugScript());conf.setReduceDebugScript("rDbgScript");assertEquals("rDbgScript", conf.getReduceDebugScript());// default valueassertNull(conf.getJobLocalDir());assertEquals("default", conf.getQueueName());conf.setQueueName("qname");assertEquals("qname", conf.getQueueName());conf.setMemoryForMapTask(100 * 1000);assertEquals(100 * 1000, conf.getMemoryForMapTask());conf.setMemoryForReduceTask(1000 * 1000);assertEquals(1000 * 1000, conf.getMemoryForReduceTask());assertEquals(-1, conf.getMaxPhysicalMemoryForTask());assertEquals("The variable key is no longer used.", JobConf.deprecatedString("key"));// so that they won't override mapred.child.java.optsassertEquals("mapreduce.map.java.opts should not be set by default", null, conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS));assertEquals("mapreduce.reduce.java.opts should not be set by default", null, conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS));}
public void testDeprecatedPropertyNameForTaskVmem() {JobConf configuration = new JobConf();configuration.setLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY, 1024);configuration.setLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY, 1024);Assert.assertEquals(1024, configuration.getMemoryForMapTask());Assert.assertEquals(1024, configuration.getMemoryForReduceTask());// Make sure new property names aren't broken by the old onesconfiguration.setLong(JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY, 1025);configuration.setLong(JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY, 1025);Assert.assertEquals(1025, configuration.getMemoryForMapTask());Assert.assertEquals(1025, configuration.getMemoryForReduceTask());configuration.setMemoryForMapTask(2048);configuration.setMemoryForReduceTask(2048);Assert.assertEquals(2048, configuration.getLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY, -1));Assert.assertEquals(2048, configuration.getLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY, -1));// Make sure new property names aren't broken by the old onesAssert.assertEquals(2048, configuration.getLong(JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY, -1));Assert.assertEquals(2048, configuration.getLong(JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY, -1));}public long value() {return value;}
public synchronized void incr() {++value;setChanged();}
public synchronized void incr(long delta) {value += delta;setChanged();}
public synchronized void decr() {--value;setChanged();}
public synchronized void decr(long delta) {value -= delta;setChanged();}
public void set(long value) {this.value = value;setChanged();}
public void snapshot(MetricsRecordBuilder builder, boolean all) {if (all || changed()) {builder.addGauge(info(), value);clearChanged();}}public boolean isUnwrapRequired() {return false;}
public boolean isWrapRequired() {return false;}
public XDR unwrap(RpcCall request, byte[] data) {throw new UnsupportedOperationException();}
public byte[] wrap(RpcCall request, XDR response) {throw new UnsupportedOperationException();}
public int getUid() {throw new UnsupportedOperationException();}
public int getGid() {throw new UnsupportedOperationException();}
public int[] getAuxGids() {throw new UnsupportedOperationException();}public void setUp() {YarnConfiguration conf = new YarnConfiguration();conf.set(YarnConfiguration.PROXY_ADDRESS, proxyAddress);webAppProxy = new WebAppProxyServer();webAppProxy.init(conf);}
public void tearDown() {webAppProxy.stop();}
public void testStart() {assertEquals(STATE.INITED, webAppProxy.getServiceState());webAppProxy.start();for (Service service : webAppProxy.getServices()) {if (service instanceof WebAppProxy) {assertEquals(((WebAppProxy) service).getBindAddress(), proxyAddress);}}assertEquals(STATE.STARTED, webAppProxy.getServiceState());}
public void testBindAddress() {YarnConfiguration conf = new YarnConfiguration();InetSocketAddress defaultBindAddress = WebAppProxyServer.getBindAddress(conf);Assert.assertEquals("Web Proxy default bind address port is incorrect", YarnConfiguration.DEFAULT_PROXY_PORT, defaultBindAddress.getPort());}public void testDeleteEmptyFile() {final Path file = new Path("/test/testDeleteEmptyFile");createEmptyFile(file);SwiftTestUtils.noteAction("about to delete");assertDeleted(file, true);}
public void testDeleteEmptyFileTwice() {final Path file = new Path("/test/testDeleteEmptyFileTwice");createEmptyFile(file);assertDeleted(file, true);SwiftTestUtils.noteAction("multiple creates, and deletes");assertFalse("Delete returned true", fs.delete(file, false));createEmptyFile(file);assertDeleted(file, true);assertFalse("Delete returned true", fs.delete(file, false));}
public void testDeleteNonEmptyFile() {final Path file = new Path("/test/testDeleteNonEmptyFile");createFile(file);assertDeleted(file, true);}
public void testDeleteNonEmptyFileTwice() {final Path file = new Path("/test/testDeleteNonEmptyFileTwice");createFile(file);assertDeleted(file, true);assertFalse("Delete returned true", fs.delete(file, false));createFile(file);assertDeleted(file, true);assertFalse("Delete returned true", fs.delete(file, false));}
public void testDeleteTestDir() {final Path file = new Path("/test/");fs.delete(file, true);assertPathDoesNotExist("Test dir found", file);}
public void testRmRootDirRecursiveIsForbidden() {Path root = path("/");Path testFile = path("/test");createFile(testFile);assertTrue("rm(/) returned false", fs.delete(root, true));assertExists("Root dir is missing", root);assertPathDoesNotExist("test file not deleted", testFile);}public FailTaskAttemptResponseProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = FailTaskAttemptResponseProto.newBuilder(proto);}viaProto = false;}public static ResourceLocalizationSpec newResourceLocalizationSpec(LocalResource rsrc, Path path) {URL local = ConverterUtils.getYarnUrlFromPath(path);ResourceLocalizationSpec resourceLocalizationSpec = Records.newRecord(ResourceLocalizationSpec.class);resourceLocalizationSpec.setDestinationDirectory(local);resourceLocalizationSpec.setResource(rsrc);return resourceLocalizationSpec;}public void setup() {conf = new Configuration();// can be nameservice-scoped.conf.set(ZKFailoverController.ZK_QUORUM_KEY + ".ns1", hostPort);conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, AlwaysSucceedFencer.class.getName());conf.setBoolean(DFSConfigKeys.DFS_HA_AUTO_FAILOVER_ENABLED_KEY, true);// the restart of the daemons between test cases.conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY, 0);conf.setInt(DFSConfigKeys.DFS_HA_ZKFC_PORT_KEY + ".ns1.nn1", 10023);conf.setInt(DFSConfigKeys.DFS_HA_ZKFC_PORT_KEY + ".ns1.nn2", 10024);MiniDFSNNTopology topology = new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf("ns1").addNN(new MiniDFSNNTopology.NNConf("nn1").setIpcPort(10021)).addNN(new MiniDFSNNTopology.NNConf("nn2").setIpcPort(10022)));cluster = new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(0).build();cluster.waitActive();ctx = new TestContext();ctx.addThread(thr1 = new ZKFCThread(ctx, 0));assertEquals(0, thr1.zkfc.run(new String[] { "-formatZK" }));thr1.start();waitForHAState(0, HAServiceState.ACTIVE);ctx.addThread(thr2 = new ZKFCThread(ctx, 1));thr2.start();// Wait for the ZKFCs to fully start upZKFCTestUtil.waitForHealthState(thr1.zkfc, HealthMonitor.State.SERVICE_HEALTHY, ctx);ZKFCTestUtil.waitForHealthState(thr2.zkfc, HealthMonitor.State.SERVICE_HEALTHY, ctx);fs = HATestUtil.configureFailoverFs(cluster, conf);}
public void shutdown() {cluster.shutdown();if (thr1 != null) {thr1.interrupt();}if (thr2 != null) {thr2.interrupt();}if (ctx != null) {ctx.stop();}}
public void testFailoverAndBackOnNNShutdown() {Path p1 = new Path("/dir1");Path p2 = new Path("/dir2");// Write some data on the first NNfs.mkdirs(p1);// Shut it down, causing automatic failovercluster.shutdownNameNode(0);// Data should still exist. Write some on the new NNassertTrue(fs.exists(p1));fs.mkdirs(p2);assertEquals(AlwaysSucceedFencer.getLastFencedService().getAddress(), thr1.zkfc.getLocalTarget().getAddress());// Start the first node back upcluster.restartNameNode(0);// This should have no effect -- the new node should be STANDBY.waitForHAState(0, HAServiceState.STANDBY);assertTrue(fs.exists(p1));assertTrue(fs.exists(p2));// Shut down the second node, which should failback to the firstcluster.shutdownNameNode(1);waitForHAState(0, HAServiceState.ACTIVE);// First node should see what was written on the second node while it was down.assertTrue(fs.exists(p1));assertTrue(fs.exists(p2));assertEquals(AlwaysSucceedFencer.getLastFencedService().getAddress(), thr2.zkfc.getLocalTarget().getAddress());}
public void testManualFailover() {thr2.zkfc.getLocalTarget().getZKFCProxy(conf, 15000).gracefulFailover();waitForHAState(0, HAServiceState.STANDBY);waitForHAState(1, HAServiceState.ACTIVE);thr1.zkfc.getLocalTarget().getZKFCProxy(conf, 15000).gracefulFailover();waitForHAState(0, HAServiceState.ACTIVE);waitForHAState(1, HAServiceState.STANDBY);}
public void testManualFailoverWithDFSHAAdmin() {DFSHAAdmin tool = new DFSHAAdmin();tool.setConf(conf);assertEquals(0, tool.run(new String[] { "-failover", "nn1", "nn2" }));waitForHAState(0, HAServiceState.STANDBY);waitForHAState(1, HAServiceState.ACTIVE);assertEquals(0, tool.run(new String[] { "-failover", "nn2", "nn1" }));waitForHAState(0, HAServiceState.ACTIVE);waitForHAState(1, HAServiceState.STANDBY);}
private void waitForHAState(int nnidx, final HAServiceState state) {final NameNode nn = cluster.getNameNode(nnidx);GenericTestUtils.waitFor(new Supplier<Boolean>() {
@Overridepublic Boolean get() {try {return nn.getRpcServer().getServiceStatus().getState() == state;} catch (Exception e) {e.printStackTrace();return false;}}}, 50, 15000);}
public Boolean get() {try {return nn.getRpcServer().getServiceStatus().getState() == state;} catch (Exception e) {e.printStackTrace();return false;}}
public void doWork() {try {assertEquals(0, zkfc.run(new String[0]));} catch (InterruptedException ie) {}}public void checkEnabled() {if (this == DISABLE) {throw new UnsupportedOperationException("This feature is disabled.  Please refer to " + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_ENABLE_KEY + " configuration property.");}}
public boolean satisfy(final short replication, final DatanodeInfo[] existings, final boolean isAppend, final boolean isHflushed) {final int n = existings == null ? 0 : existings.length;if (n == 0 || n >= replication) {//don't need to add datanode for any policy.return false;} else if (this == DISABLE || this == NEVER) {return false;} else if (this == ALWAYS) {return true;} else {//DEFAULTif (replication < 3) {return false;} else {if (n <= (replication / 2)) {return true;} else {return isAppend || isHflushed;}}}}
public static ReplaceDatanodeOnFailure get(final Configuration conf) {final boolean enabled = conf.getBoolean(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_ENABLE_KEY, DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_ENABLE_DEFAULT);if (!enabled) {return DISABLE;}final String policy = conf.get(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY, DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_DEFAULT);for (int i = 1; i < values().length; i++) {final ReplaceDatanodeOnFailure rdof = values()[i];if (rdof.name().equalsIgnoreCase(policy)) {return rdof;}}throw new HadoopIllegalArgumentException("Illegal configuration value for " + DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY + ": " + policy);}
public void write(final Configuration conf) {conf.setBoolean(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_ENABLE_KEY, this != DISABLE);conf.set(DFSConfigKeys.DFS_CLIENT_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY_KEY, name());}public void tearDown() {if (testAccount != null) {testAccount.cleanup();testAccount = null;}}
public void testContainerExistAfterDoesNotExist() {testAccount = AzureBlobStorageTestAccount.create("", EnumSet.noneOf(CreateOptions.class));assumeNotNull(testAccount);CloudBlobContainer container = testAccount.getRealContainer();FileSystem fs = testAccount.getFileSystem();// Starting off with the container not thereassertFalse(container.exists());// state to DoesNotExisttry {fs.listStatus(new Path("/"));assertTrue("Should've thrown.", false);} catch (FileNotFoundException ex) {assertTrue("Unexpected exception: " + ex, ex.getMessage().contains("does not exist."));}assertFalse(container.exists());// Create a container outside of the WASB FileSystemcontainer.create();// Add a file to the container outside of the WASB FileSystemCloudBlockBlob blob = testAccount.getBlobReference("foo");BlobOutputStream outputStream = blob.openOutputStream();outputStream.write(new byte[10]);outputStream.close();// Make sure the file is visibleassertTrue(fs.exists(new Path("/foo")));assertTrue(container.exists());}
public void testContainerCreateAfterDoesNotExist() {testAccount = AzureBlobStorageTestAccount.create("", EnumSet.noneOf(CreateOptions.class));assumeNotNull(testAccount);CloudBlobContainer container = testAccount.getRealContainer();FileSystem fs = testAccount.getFileSystem();// Starting off with the container not thereassertFalse(container.exists());// state to DoesNotExisttry {assertNull(fs.listStatus(new Path("/")));assertTrue("Should've thrown.", false);} catch (FileNotFoundException ex) {assertTrue("Unexpected exception: " + ex, ex.getMessage().contains("does not exist."));}assertFalse(container.exists());// Create a container outside of the WASB FileSystemcontainer.create();// Write should succeedassertTrue(fs.createNewFile(new Path("/foo")));assertTrue(container.exists());}
public void testContainerCreateOnWrite() {testAccount = AzureBlobStorageTestAccount.create("", EnumSet.noneOf(CreateOptions.class));assumeNotNull(testAccount);CloudBlobContainer container = testAccount.getRealContainer();FileSystem fs = testAccount.getFileSystem();// Starting off with the container not thereassertFalse(container.exists());// A list shouldn't create the container.try {fs.listStatus(new Path("/"));assertTrue("Should've thrown.", false);} catch (FileNotFoundException ex) {assertTrue("Unexpected exception: " + ex, ex.getMessage().contains("does not exist."));}assertFalse(container.exists());// Neither should a read.try {fs.open(new Path("/foo"));assertFalse("Should've thrown.", true);} catch (FileNotFoundException ex) {}assertFalse(container.exists());// Neither should a renameassertFalse(fs.rename(new Path("/foo"), new Path("/bar")));assertFalse(container.exists());// But a write should.assertTrue(fs.createNewFile(new Path("/foo")));assertTrue(container.exists());}
public void testContainerChecksWithSas() {testAccount = AzureBlobStorageTestAccount.create("", EnumSet.of(CreateOptions.UseSas));assumeNotNull(testAccount);CloudBlobContainer container = testAccount.getRealContainer();FileSystem fs = testAccount.getFileSystem();// The container shouldn't be thereassertFalse(container.exists());// A write should just failtry {fs.createNewFile(new Path("/foo"));assertFalse("Should've thrown.", true);} catch (AzureException ex) {}assertFalse(container.exists());}public void testDfsUrls() {Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();FileSystem fs = cluster.getFileSystem();// in TestStreamHandlerFsUrlStreamHandlerFactory factory = new org.apache.hadoop.fs.FsUrlStreamHandlerFactory();java.net.URL.setURLStreamHandlerFactory(factory);Path filePath = new Path("/thefile");try {byte[] fileContent = new byte[1024];for (int i = 0; i < fileContent.length; ++i) fileContent[i] = (byte) i;OutputStream os = fs.create(filePath);os.write(fileContent);os.close();URI uri = fs.getUri();URL fileURL = new URL(uri.getScheme(), uri.getHost(), uri.getPort(), filePath.toString());InputStream is = fileURL.openStream();assertNotNull(is);byte[] bytes = new byte[4096];assertEquals(1024, is.read(bytes));is.close();for (int i = 0; i < fileContent.length; ++i) assertEquals(fileContent[i], bytes[i]);// Cleanup: delete the filefs.delete(filePath, false);} finally {fs.close();cluster.shutdown();}}
public void testFileUrls() {// URLStreamHandler is already set in JVM by testDfsUrls() Configuration conf = new HdfsConfiguration();// Locate the test temporary directory.if (!TEST_ROOT_DIR.exists()) {if (!TEST_ROOT_DIR.mkdirs())throw new IOException("Cannot create temporary directory: " + TEST_ROOT_DIR);}File tmpFile = new File(TEST_ROOT_DIR, "thefile");URI uri = tmpFile.toURI();FileSystem fs = FileSystem.get(uri, conf);try {byte[] fileContent = new byte[1024];for (int i = 0; i < fileContent.length; ++i) fileContent[i] = (byte) i;OutputStream os = fs.create(new Path(uri.getPath()));os.write(fileContent);os.close();URL fileURL = uri.toURL();InputStream is = fileURL.openStream();assertNotNull(is);byte[] bytes = new byte[4096];assertEquals(1024, is.read(bytes));is.close();for (int i = 0; i < fileContent.length; ++i) assertEquals(fileContent[i], bytes[i]);// Cleanup: delete the filefs.delete(new Path(uri.getPath()), false);} finally {fs.close();}}protected void initStorage(Configuration conf) {}
protected void startStorage() {}
protected void closeStorage() {}
public HistoryServerState loadState() {throw new UnsupportedOperationException("Cannot load state from null store");}
public void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) {}
public void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) {}
public void removeToken(MRDelegationTokenIdentifier tokenId) {}
public void storeTokenMasterKey(DelegationKey key) {}
public void removeTokenMasterKey(DelegationKey key) {}public ContainerIdProto getProto() {return proto;}
public int getId() {Preconditions.checkNotNull(proto);return proto.getId();}
protected void setId(int id) {Preconditions.checkNotNull(builder);builder.setId((id));}
public ApplicationAttemptId getApplicationAttemptId() {return this.applicationAttemptId;}
protected void setApplicationAttemptId(ApplicationAttemptId atId) {if (atId != null) {Preconditions.checkNotNull(builder);builder.setAppAttemptId(convertToProtoFormat(atId));}this.applicationAttemptId = atId;}
private ApplicationAttemptIdPBImpl convertFromProtoFormat(ApplicationAttemptIdProto p) {return new ApplicationAttemptIdPBImpl(p);}
private ApplicationAttemptIdProto convertToProtoFormat(ApplicationAttemptId t) {return ((ApplicationAttemptIdPBImpl) t).getProto();}
protected void build() {proto = builder.build();builder = null;}public static synchronized void init(Configuration conf) {if (!isInited) {if (conf == null) {conf = new JobConf();}GROUP_NAME_MAX = conf.getInt(COUNTER_GROUP_NAME_MAX_KEY, COUNTER_GROUP_NAME_MAX_DEFAULT);COUNTER_NAME_MAX = conf.getInt(COUNTER_NAME_MAX_KEY, COUNTER_NAME_MAX_DEFAULT);GROUPS_MAX = conf.getInt(COUNTER_GROUPS_MAX_KEY, COUNTER_GROUPS_MAX_DEFAULT);COUNTERS_MAX = conf.getInt(COUNTERS_MAX_KEY, COUNTERS_MAX_DEFAULT);}isInited = true;}
public static int getGroupNameMax() {if (!isInited) {init(null);}return GROUP_NAME_MAX;}
public static int getCounterNameMax() {if (!isInited) {init(null);}return COUNTER_NAME_MAX;}
public static int getGroupsMax() {if (!isInited) {init(null);}return GROUPS_MAX;}
public static int getCountersMax() {if (!isInited) {init(null);}return COUNTERS_MAX;}
public static String filterName(String name, int maxLen) {return name.length() > maxLen ? name.substring(0, maxLen - 1) : name;}
public static String filterCounterName(String name) {return filterName(name, getCounterNameMax());}
public static String filterGroupName(String name) {return filterName(name, getGroupNameMax());}
public synchronized void checkCounters(int size) {if (firstViolation != null) {throw new LimitExceededException(firstViolation);}int countersMax = getCountersMax();if (size > countersMax) {firstViolation = new LimitExceededException("Too many counters: " + size + " max=" + countersMax);throw firstViolation;}}
public synchronized void incrCounters() {checkCounters(totalCounters + 1);++totalCounters;}
public synchronized void checkGroups(int size) {if (firstViolation != null) {throw new LimitExceededException(firstViolation);}int groupsMax = getGroupsMax();if (size > groupsMax) {firstViolation = new LimitExceededException("Too many counter groups: " + size + " max=" + groupsMax);}}
public synchronized LimitExceededException violation() {return firstViolation;}public ApplicationAttemptId getApplicationAttemptId() {return this.applicationAttemptId;}
public void write(DataOutput out) {ApplicationId appId = this.applicationAttemptId.getApplicationId();out.writeLong(appId.getClusterTimestamp());out.writeInt(appId.getId());out.writeInt(this.applicationAttemptId.getAttemptId());out.writeInt(this.keyId);}
public void readFields(DataInput in) {long clusterTimeStamp = in.readLong();int appId = in.readInt();int attemptId = in.readInt();ApplicationId applicationId = ApplicationId.newInstance(clusterTimeStamp, appId);this.applicationAttemptId = ApplicationAttemptId.newInstance(applicationId, attemptId);this.keyId = in.readInt();}
public Text getKind() {return KIND_NAME;}
public UserGroupInformation getUser() {if (this.applicationAttemptId == null || "".equals(this.applicationAttemptId.toString())) {return null;}return UserGroupInformation.createRemoteUser(this.applicationAttemptId.toString());}
public int getKeyId() {return this.keyId;}
protected Text getKind() {return KIND_NAME;}public GetApplicationAttemptReportRequestProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null) {return false;}if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (applicationAttemptId != null) {builder.setApplicationAttemptId(convertToProtoFormat(this.applicationAttemptId));}}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = GetApplicationAttemptReportRequestProto.newBuilder(proto);}viaProto = false;}
public ApplicationAttemptId getApplicationAttemptId() {if (this.applicationAttemptId != null) {return this.applicationAttemptId;}GetApplicationAttemptReportRequestProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationAttemptId()) {return null;}this.applicationAttemptId = convertFromProtoFormat(p.getApplicationAttemptId());return this.applicationAttemptId;}
public void setApplicationAttemptId(ApplicationAttemptId applicationAttemptId) {maybeInitBuilder();if (applicationAttemptId == null) {builder.clearApplicationAttemptId();}this.applicationAttemptId = applicationAttemptId;}
private ApplicationAttemptIdPBImpl convertFromProtoFormat(ApplicationAttemptIdProto p) {return new ApplicationAttemptIdPBImpl(p);}
private ApplicationAttemptIdProto convertToProtoFormat(ApplicationAttemptId t) {return ((ApplicationAttemptIdPBImpl) t).getProto();}public void setUp() {conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 100);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, 100);cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(3)).build();for (int i = 0; i < 3; i++) {cluster.waitActive(i);}}
public void tearDown() {if (cluster != null)cluster.shutdown();}
private void stopBPServiceThreads(int numStopThreads, DataNode dn) {BPOfferService[] bpoList = dn.getAllBpOs();int expected = dn.getBpOsCount() - numStopThreads;int index = numStopThreads - 1;while (index >= 0) {bpoList[index--].stop();}// Total 30 seconds MAX wait timeint iterations = 3000;while (dn.getBpOsCount() != expected && iterations > 0) {Thread.sleep(WAIT_TIME_IN_MILLIS);iterations--;}assertEquals("Mismatch in number of BPServices running", expected, dn.getBpOsCount());}
public void testBPServiceExit() {DataNode dn = cluster.getDataNodes().get(0);stopBPServiceThreads(1, dn);assertTrue("DataNode should not exit", dn.isDatanodeUp());stopBPServiceThreads(2, dn);assertFalse("DataNode should exit", dn.isDatanodeUp());}public boolean checkAccess(UserGroupInformation callerUGI, TimelineEntity entity) {if (LOG.isDebugEnabled()) {LOG.debug("Verifying the access of " + (callerUGI == null ? null : callerUGI.getShortUserName()) + " on the timeline entity " + new EntityIdentifier(entity.getEntityId(), entity.getEntityType()));}if (!adminAclsManager.areACLsEnabled()) {return true;}Set<Object> values = entity.getPrimaryFilters().get(SystemFilter.ENTITY_OWNER.toString());if (values == null || values.size() != 1) {throw new YarnException("Owner information of the timeline entity " + new EntityIdentifier(entity.getEntityId(), entity.getEntityType()) + " is corrupted.");}String owner = values.iterator().next().toString();// allowed user/group listif (callerUGI != null && (adminAclsManager.isAdmin(callerUGI) || callerUGI.getShortUserName().equals(owner))) {return true;}return false;}
public AdminACLsManager setAdminACLsManager(AdminACLsManager adminAclsManager) {AdminACLsManager oldAdminACLsManager = this.adminAclsManager;this.adminAclsManager = adminAclsManager;return oldAdminACLsManager;}private static long waitForVerification(int infoPort, FileSystem fs, Path file, int blocksValidated, long newTime, long timeout) {URL url = new URL("http://localhost:" + infoPort + "/blockScannerReport?listblocks");long lastWarnTime = Time.monotonicNow();if (newTime <= 0)newTime = 1L;long verificationTime = 0;String block = DFSTestUtil.getFirstBlock(fs, file).getBlockName();long failtime = (timeout <= 0) ? Long.MAX_VALUE : Time.monotonicNow() + timeout;while (verificationTime < newTime) {if (failtime < Time.monotonicNow()) {throw new TimeoutException("failed to achieve block verification after " + timeout + " msec.  Current verification timestamp = " + verificationTime + ", requested verification time > " + newTime);}String response = DFSTestUtil.urlGet(url);if (blocksValidated >= 0) {for (Matcher matcher = pattern_blockVerify.matcher(response); matcher.find(); ) {if (block.equals(matcher.group(1))) {assertEquals(1, blocksValidated);break;}}}for (Matcher matcher = pattern.matcher(response); matcher.find(); ) {if (block.equals(matcher.group(1))) {verificationTime = Long.parseLong(matcher.group(2));break;}}if (verificationTime < newTime) {long now = Time.monotonicNow();if ((now - lastWarnTime) >= 5 * 1000) {LOG.info("Waiting for verification of " + block);lastWarnTime = now;}try {Thread.sleep(500);} catch (InterruptedException ignored) {}}}return verificationTime;}
public void testDatanodeBlockScanner() {long startTime = Time.monotonicNow();Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();FileSystem fs = cluster.getFileSystem();Path file1 = new Path("/tmp/testBlockVerification/file1");Path file2 = new Path("/tmp/testBlockVerification/file2");/* * Write the first file and restart the cluster. */DFSTestUtil.createFile(fs, file1, 10, (short) 1, 0);cluster.shutdown();cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(false).build();cluster.waitActive();DFSClient dfsClient = new DFSClient(new InetSocketAddress("localhost", cluster.getNameNodePort()), conf);fs = cluster.getFileSystem();DatanodeInfo dn = dfsClient.datanodeReport(DatanodeReportType.LIVE)[0];/* * The cluster restarted. The block should be verified by now. */assertTrue(waitForVerification(dn.getInfoPort(), fs, file1, 1, startTime, TIMEOUT) >= startTime);/* * Create a new file and read the block. The block should be marked  * verified since the client reads the block and verifies checksum.  */DFSTestUtil.createFile(fs, file2, 10, (short) 1, 0);IOUtils.copyBytes(fs.open(file2), new IOUtils.NullOutputStream(), conf, true);assertTrue(waitForVerification(dn.getInfoPort(), fs, file2, 2, startTime, TIMEOUT) >= startTime);cluster.shutdown();}
public static boolean corruptReplica(ExtendedBlock blk, int replica) {return MiniDFSCluster.corruptReplica(replica, blk);}
public void testBlockCorruptionPolicy() {Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 1000L);Random random = new Random();FileSystem fs = null;int rand = random.nextInt(3);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();cluster.waitActive();fs = cluster.getFileSystem();Path file1 = new Path("/tmp/testBlockVerification/file1");DFSTestUtil.createFile(fs, file1, 1024, (short) 3, 0);ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, file1);DFSTestUtil.waitReplication(fs, file1, (short) 3);assertFalse(DFSTestUtil.allBlockReplicasCorrupt(cluster, file1, 0));// Corrupt random replica of block assertTrue(MiniDFSCluster.corruptReplica(rand, block));// Restart the datanode hoping the corrupt block to be reportedcluster.restartDataNode(rand);// We have 2 good replicas and block is not corruptDFSTestUtil.waitReplication(fs, file1, (short) 2);assertFalse(DFSTestUtil.allBlockReplicasCorrupt(cluster, file1, 0));// and we should get all the replicas assertTrue(MiniDFSCluster.corruptReplica(0, block));assertTrue(MiniDFSCluster.corruptReplica(1, block));assertTrue(MiniDFSCluster.corruptReplica(2, block));// bad blocks to the NN when all replicas are bad.for (DataNode dn : cluster.getDataNodes()) {DataNodeTestUtils.runBlockScannerForBlock(dn, block);}// its replicasDFSTestUtil.waitReplication(fs, file1, (short) 3);assertTrue(DFSTestUtil.allBlockReplicasCorrupt(cluster, file1, 0));cluster.shutdown();}
public void testBlockCorruptionRecoveryPolicy1() {// Test recovery of 1 corrupt replicaLOG.info("Testing corrupt replica recovery for one corrupt replica");blockCorruptionRecoveryPolicy(4, (short) 3, 1);}
public void testBlockCorruptionRecoveryPolicy2() {// Test recovery of 2 corrupt replicasLOG.info("Testing corrupt replica recovery for two corrupt replicas");blockCorruptionRecoveryPolicy(5, (short) 3, 2);}
private void blockCorruptionRecoveryPolicy(int numDataNodes, short numReplicas, int numCorruptReplicas) {Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 30L);conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 3);conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 3L);conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_REPLICATION_CONSIDERLOAD_KEY, false);conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_PENDING_TIMEOUT_SEC_KEY, 5L);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fs = cluster.getFileSystem();Path file1 = new Path("/tmp/testBlockCorruptRecovery/file");DFSTestUtil.createFile(fs, file1, 1024, numReplicas, 0);ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, file1);final int ITERATIONS = 10;// Wait until block is replicated to numReplicasDFSTestUtil.waitReplication(fs, file1, numReplicas);for (int k = 0; ; k++) {// Corrupt numCorruptReplicas replicas of block int[] corruptReplicasDNIDs = new int[numCorruptReplicas];for (int i = 0, j = 0; (j != numCorruptReplicas) && (i < numDataNodes); i++) {if (corruptReplica(block, i)) {corruptReplicasDNIDs[j++] = i;LOG.info("successfully corrupted block " + block + " on node " + i + " " + cluster.getDataNodes().get(i).getDisplayName());}}// and causes the indexes of all nodes above them in the list to change.for (int i = numCorruptReplicas - 1; i >= 0; i--) {LOG.info("restarting node with corrupt replica: position " + i + " node " + corruptReplicasDNIDs[i] + " " + cluster.getDataNodes().get(corruptReplicasDNIDs[i]).getDisplayName());cluster.restartDataNode(corruptReplicasDNIDs[i]);}// Loop until all corrupt replicas are reportedtry {DFSTestUtil.waitCorruptReplicas(fs, cluster.getNamesystem(), file1, block, numCorruptReplicas);} catch (TimeoutException e) {if (k > ITERATIONS) {throw e;}LOG.info("Timed out waiting for corrupt replicas, trying again, iteration " + k);continue;}break;}// Loop until the block recovers after replicationDFSTestUtil.waitReplication(fs, file1, numReplicas);assertFalse(DFSTestUtil.allBlockReplicasCorrupt(cluster, file1, 0));// corruptReplicasMapDFSTestUtil.waitCorruptReplicas(fs, cluster.getNamesystem(), file1, block, 0);cluster.shutdown();}
public void testTruncatedBlockReport() {final Configuration conf = new HdfsConfiguration();final short REPLICATION_FACTOR = (short) 2;final Path fileName = new Path("/file1");conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 3L);conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 3);conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 3L);conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_REPLICATION_CONSIDERLOAD_KEY, false);long startTime = Time.monotonicNow();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION_FACTOR).build();cluster.waitActive();ExtendedBlock block;try {FileSystem fs = cluster.getFileSystem();DFSTestUtil.createFile(fs, fileName, 1, REPLICATION_FACTOR, 0);DFSTestUtil.waitReplication(fs, fileName, REPLICATION_FACTOR);block = DFSTestUtil.getFirstBlock(fs, fileName);} finally {cluster.shutdown();}// then truncate it on datanode 0.cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION_FACTOR).format(false).build();cluster.waitActive();try {FileSystem fs = cluster.getFileSystem();int infoPort = cluster.getDataNodes().get(0).gepublic void testClose() {MiniDFSCluster cluster = new MiniDFSCluster.Builder(new HdfsConfiguration()).build();try {cluster.waitActive();DataNode dn = cluster.getDataNodes().get(0);FsDatasetImpl dataSet = (FsDatasetImpl) DataNodeTestUtils.getFSDataset(dn);String bpid = cluster.getNamesystem().getBlockPoolId();ExtendedBlock[] blocks = setup(bpid, dataSet);// test closetestClose(dataSet, blocks);} finally {cluster.shutdown();}}
public void testAppend() {MiniDFSCluster cluster = new MiniDFSCluster.Builder(new HdfsConfiguration()).build();try {cluster.waitActive();DataNode dn = cluster.getDataNodes().get(0);FsDatasetImpl dataSet = (FsDatasetImpl) DataNodeTestUtils.getFSDataset(dn);String bpid = cluster.getNamesystem().getBlockPoolId();ExtendedBlock[] blocks = setup(bpid, dataSet);// test appendtestAppend(bpid, dataSet, blocks);} finally {cluster.shutdown();}}
public void testWriteToRbw() {MiniDFSCluster cluster = new MiniDFSCluster.Builder(new HdfsConfiguration()).build();try {cluster.waitActive();DataNode dn = cluster.getDataNodes().get(0);FsDatasetImpl dataSet = (FsDatasetImpl) DataNodeTestUtils.getFSDataset(dn);String bpid = cluster.getNamesystem().getBlockPoolId();ExtendedBlock[] blocks = setup(bpid, dataSet);// test writeToRbwtestWriteToRbw(dataSet, blocks);} finally {cluster.shutdown();}}
public void testWriteToTempoary() {MiniDFSCluster cluster = new MiniDFSCluster.Builder(new HdfsConfiguration()).build();try {cluster.waitActive();DataNode dn = cluster.getDataNodes().get(0);FsDatasetImpl dataSet = (FsDatasetImpl) DataNodeTestUtils.getFSDataset(dn);String bpid = cluster.getNamesystem().getBlockPoolId();ExtendedBlock[] blocks = setup(bpid, dataSet);// test writeToTemporarytestWriteToTemporary(dataSet, blocks);} finally {cluster.shutdown();}}
private ExtendedBlock[] setup(String bpid, FsDatasetImpl dataSet) {ExtendedBlock[] blocks = new ExtendedBlock[] { new ExtendedBlock(bpid, 1, 1, 2001), new ExtendedBlock(bpid, 2, 1, 2002), new ExtendedBlock(bpid, 3, 1, 2003), new ExtendedBlock(bpid, 4, 1, 2004), new ExtendedBlock(bpid, 5, 1, 2005), new ExtendedBlock(bpid, 6, 1, 2006) };ReplicaMap replicasMap = dataSet.volumeMap;FsVolumeImpl vol = dataSet.volumes.getNextVolume(StorageType.DEFAULT, 0);ReplicaInfo replicaInfo = new FinalizedReplica(blocks[FINALIZED].getLocalBlock(), vol, vol.getCurrentDir().getParentFile());replicasMap.add(bpid, replicaInfo);replicaInfo.getBlockFile().createNewFile();replicaInfo.getMetaFile().createNewFile();replicasMap.add(bpid, new ReplicaInPipeline(blocks[TEMPORARY].getBlockId(), blocks[TEMPORARY].getGenerationStamp(), vol, vol.createTmpFile(bpid, blocks[TEMPORARY].getLocalBlock()).getParentFile()));replicaInfo = new ReplicaBeingWritten(blocks[RBW].getLocalBlock(), vol, vol.createRbwFile(bpid, blocks[RBW].getLocalBlock()).getParentFile(), null);replicasMap.add(bpid, replicaInfo);replicaInfo.getBlockFile().createNewFile();replicaInfo.getMetaFile().createNewFile();replicasMap.add(bpid, new ReplicaWaitingToBeRecovered(blocks[RWR].getLocalBlock(), vol, vol.createRbwFile(bpid, blocks[RWR].getLocalBlock()).getParentFile()));replicasMap.add(bpid, new ReplicaUnderRecovery(new FinalizedReplica(blocks[RUR].getLocalBlock(), vol, vol.getCurrentDir().getParentFile()), 2007));return blocks;}
private void testAppend(String bpid, FsDatasetImpl dataSet, ExtendedBlock[] blocks) {long newGS = blocks[FINALIZED].getGenerationStamp() + 1;final FsVolumeImpl v = (FsVolumeImpl) dataSet.volumeMap.get(bpid, blocks[FINALIZED].getLocalBlock()).getVolume();long available = v.getCapacity() - v.getDfsUsed();long expectedLen = blocks[FINALIZED].getNumBytes();try {v.decDfsUsed(bpid, -available);blocks[FINALIZED].setNumBytes(expectedLen + 100);dataSet.append(blocks[FINALIZED], newGS, expectedLen);Assert.fail("Should not have space to append to an RWR replica" + blocks[RWR]);} catch (DiskOutOfSpaceException e) {Assert.assertTrue(e.getMessage().startsWith("Insufficient space for appending to "));}v.decDfsUsed(bpid, available);blocks[FINALIZED].setNumBytes(expectedLen);newGS = blocks[RBW].getGenerationStamp() + 1;dataSet.append(blocks[FINALIZED], newGS, // successfulblocks[FINALIZED].getNumBytes());blocks[FINALIZED].setGenerationStamp(newGS);try {dataSet.append(blocks[TEMPORARY], blocks[TEMPORARY].getGenerationStamp() + 1, blocks[TEMPORARY].getNumBytes());Assert.fail("Should not have appended to a temporary replica " + blocks[TEMPORARY]);} catch (ReplicaNotFoundException e) {Assert.assertEquals(ReplicaNotFoundException.UNFINALIZED_REPLICA + blocks[TEMPORARY], e.getMessage());}try {dataSet.append(blocks[RBW], blocks[RBW].getGenerationStamp() + 1, blocks[RBW].getNumBytes());Assert.fail("Should not have appended to an RBW replica" + blocks[RBW]);} catch (ReplicaNotFoundException e) {Assert.assertEquals(ReplicaNotFoundException.UNFINALIZED_REPLICA + blocks[RBW], e.getMessage());}try {dataSet.append(blocks[RWR], blocks[RWR].getGenerationStamp() + 1, blocks[RBW].getNumBytes());Assert.fail("Should not have appended to an RWR replica" + blocks[RWR]);} catch (ReplicaNotFoundException e) {Assert.assertEquals(ReplicaNotFoundException.UNFINALIZED_REPLICA + blocks[RWR], e.getMessage());}try {dataSet.append(blocks[RUR], blocks[RUR].getGenerationStamp() + 1, blocks[RUR].getNumBytes());Assert.fail("Should not have appended to an RUR replica" + blocks[RUR]);} catch (ReplicaNotFoundException e) {Assert.assertEquals(ReplicaNotFoundException.UNFINALIZED_REPLICA + blocks[RUR], e.getMessage());}try {dataSet.append(blocks[NON_EXISTENT], blocks[NON_EXISTENT].getGenerationStamp(), blocks[NON_EXISTENT].getNumBytes());Assert.fail("Should not have appended to a non-existent replica " + blocks[NON_EXISTENT]);} catch (ReplicaNotFoundException e) {Assert.assertEquals(ReplicaNotFoundException.NON_EXISTENT_REPLICA + blocks[NON_EXISTENT], e.getMessage());}newGS = blocks[FINALIZED].getGenerationStamp() + 1;dataSet.recoverAppend(blocks[FINALIZED], newGS, // successfulblocks[FINALIZED].getNumBytes());blocks[FINALIZED].setGenerationStamp(newGS);try {dataSet.recoverAppend(blocks[TEMPORARY], blocks[TEMPORARY].getGenerationStamp() + 1, blocks[TEMPORARY].getNumBytes());Assert.fail("Should not have appended to a temporary replica " + blocks[TEMPORARY]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}newGS = blocks[RBW].getGenerationStamp() + 1;dataSet.recoverAppend(blocks[RBW], newGS, blocks[RBW].getNumBytes());blocks[RBW].setGenerationStamp(newGS);try {dataSet.recoverAppend(blocks[RWR], blocks[RWR].getGenerationStamp() + 1, blocks[RBW].getNumBytes());Assert.fail("Should not have appended to an RWR replica" + blocks[RWR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}try {dataSet.recoverAppend(blocks[RUR], blocks[RUR].getGenerationStamp() + 1, blocks[RUR].getNumBytes());Assert.fail("Should not have appended to an RUR replica" + blocks[RUR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}try {dataSet.recoverAppend(blocks[NON_EXISTENT], blocks[NON_EXISTENT].getGenerationStamp(), blocks[NON_EXISTENT].getNumBytes());Assert.fail("Should not have appended to a non-existent replica " + blocks[NON_EXISTENT]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_EXISTENT_REPLICA));}}
private void testClose(FsDatasetImpl dataSet, ExtendedBlock[] blocks) {long newGS = blocks[FINALIZED].getGenerationStamp() + 1;dataSet.recoverClose(blocks[FINALIZED], newGS, // successfulblocks[FINALIZED].getNumBytes());blocks[FINALIZED].setGenerationStamp(newGS);try {dataSet.recoverClose(blocks[TEMPORARY], blocks[TEMPORARY].getGenerationStamp() + 1, blocks[TEMPORARY].getNumBytes());Assert.fail("Should not have recovered close a temporary replica " + blocks[TEMPORARY]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}newGS = blocks[RBW].getGenerationStamp() + 1;dataSet.recoverClose(blocks[RBW], newGS, blocks[RBW].getNumBytes());blocks[RBW].setGenerationStamp(newGS);try {dataSet.recoverClose(blocks[RWR], blocks[RWR].getGenerationStamp() + 1, blocks[RBW].getNumBytes());Assert.fail("Should not have recovered close an RWR replica" + blocks[RWR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}try {dataSet.recoverClose(blocks[RUR], blocks[RUR].getGenerationStamp() + 1, blocks[RUR].getNumBytes());Assert.fail("Should not have recovered close an RUR replica" + blocks[RUR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.UNFINALIZED_AND_NONRBW_REPLICA));}try {dataSet.recoverClose(blocks[NON_EXISTENT], blocks[NON_EXISTENT].getGenerationStamp(), blocks[NON_EXISTENT].getNumBytes());Assert.fail("Should not have recovered close a non-existent replica " + blocks[NON_EXISTENT]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_EXISTENT_REPLICA));}}
private void testWriteToRbw(FsDatasetImpl dataSet, ExtendedBlock[] blocks) {try {dataSet.recoverRbw(blocks[FINALIZED], blocks[FINALIZED].getGenerationStamp() + 1, 0L, blocks[FINALIZED].getNumBytes());Assert.fail("Should not have recovered a finalized replica " + blocks[FINALIZED]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_RBW_REPLICA));}try {dataSet.createRbw(StorageType.DEFAULT, blocks[FINALIZED]);Assert.fail("Should not have created a replica that's already " + "finalized " + blocks[FINALIZED]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.recoverRbw(blocks[TEMPORARY], blocks[TEMPORARY].getGenerationStamp() + 1, 0L, blocks[TEMPORARY].getNumBytes());Assert.fail("Should not have recovered a temporary replica " + blocks[TEMPORARY]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_RBW_REPLICA));}try {dataSet.createRbw(StorageType.DEFAULT, blocks[TEMPORARY]);Assert.fail("Should not have created a replica that had created as " + "temporary " + blocks[TEMPORARY]);} catch (ReplicaAlreadyExistsException e) {}dataSet.recoverRbw(blocks[RBW], blocks[RBW].getGenerationStamp() + 1, 0L, // expect to be successfulblocks[RBW].getNumBytes());try {dataSet.createRbw(StorageType.DEFAULT, blocks[RBW]);Assert.fail("Should not have created a replica that had created as RBW " + blocks[RBW]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.recoverRbw(blocks[RWR], blocks[RWR].getGenerationStamp() + 1, 0L, blocks[RWR].getNumBytes());Assert.fail("Should not have recovered a RWR replica " + blocks[RWR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_RBW_REPLICA));}try {dataSet.createRbw(StorageType.DEFAULT, blocks[RWR]);Assert.fail("Should not have created a replica that was waiting to be " + "recovered " + blocks[RWR]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.recoverRbw(blocks[RUR], blocks[RUR].getGenerationStamp() + 1, 0L, blocks[RUR].getNumBytes());Assert.fail("Should not have recovered a RUR replica " + blocks[RUR]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().startsWith(ReplicaNotFoundException.NON_RBW_REPLICA));}try {dataSet.createRbw(StorageType.DEFAULT, blocks[RUR]);Assert.fail("Should not have created a replica that was under recovery " + blocks[RUR]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.recoverRbw(blocks[NON_EXISTENT], blocks[NON_EXISTENT].getGenerationStamp() + 1, 0L, blocks[NON_EXISTENT].getNumBytes());Assert.fail("Cannot recover a non-existent replica " + blocks[NON_EXISTENT]);} catch (ReplicaNotFoundException e) {Assert.assertTrue(e.getMessage().contains(ReplicaNotFoundException.NON_EXISTENT_REPLICA));}dataSet.createRbw(StorageType.DEFAULT, blocks[NON_EXISTENT]);}
private void testWriteToTemporary(FsDatasetImpl dataSet, ExtendedBlock[] blocks) {try {dataSet.createTemporary(StorageType.DEFAULT, blocks[FINALIZED]);Assert.fail("Should not have created a temporary replica that was " + "finalized " + blocks[FINALIZED]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.createTemporary(StorageType.DEFAULT, blocks[TEMPORARY]);Assert.fail("Should not have created a replica that had created as" + "temporary " + blocks[TEMPORARY]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.createTemporary(StorageType.DEFAULT, blocks[RBW]);Assert.fail("Should not have created a replica that had created as RBW " + blocks[RBW]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.createTemporary(StorageType.DEFAULT, blocks[RWR]);Assert.fail("Should not have created a replica that was waiting to be " + "recovered " + blocks[RWR]);} catch (ReplicaAlreadyExistsException e) {}try {dataSet.createTemporary(StorageType.DEFAULT, blocks[RUR]);Assert.fail("Should not have created a replica that was under recovery " + blocks[RUR]);} catch (ReplicaAlreadyExistsException e) {}dataSet.createTemporary(StorageType.DEFAULT, blocks[NON_EXISTENT]);}tInfoPort();assertTrue(waitForVerification(infoPort, fs, fileName, 1, startTime, TIMEOUT) >= startTime);// Truncate replica of blockif (!changeReplicaLength(block, 0, -1)) {throw new IOException("failed to find or change length of replica on node 0 " + cluster.getDataNodes().get(0).getDisplayName());}} finally {cluster.shutdown();}// handled correctlycluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION_FACTOR).format(false).build();cluster.startDataNodes(conf, 1, true, null, null);// now we have 3 datanodescluster.waitActive();// Assure the cluster has left safe mode.cluster.waitClusterUp();assertFalse("failed to leave safe mode", cluster.getNameNode().isInSafeMode());try {// and the block to be replicatedDFSTestUtil.waitReplication(cluster.getFileSystem(), fileName, REPLICATION_FACTOR);// Make sure that truncated block will be deletedwaitForBlockDeleted(block, 0, TIMEOUT);} finally {cluster.shutdown();}}
static boolean changeReplicaLength(ExtendedBlock blk, int dnIndex, int lenDelta) {File blockFile = MiniDFSCluster.getBlockFile(dnIndex, blk);if (blockFile != null && blockFile.exists()) {RandomAccessFile raFile = new RandomAccessFile(blockFile, "rw");raFile.setLength(raFile.length() + lenDelta);raFile.close();return true;}LOG.info("failed to change length of block " + blk);return false;}
private static void waitForBlockDeleted(ExtendedBlock blk, int dnIndex, long timeout) {File blockFile = MiniDFSCluster.getBlockFile(dnIndex, blk);long failtime = Time.monotonicNow() + ((timeout > 0) ? timeout : Long.MAX_VALUE);while (blockFile != null && blockFile.exists()) {if (failtime < Time.monotonicNow()) {throw new TimeoutException("waited too long for blocks to be deleted: " + blockFile.getPath() + (blockFile.exists() ? " still exists; " : " is absent; "));}Thread.sleep(100);blockFile = MiniDFSCluster.getBlockFile(dnIndex, blk);}}
public void testReplicaInfoParsing() {testReplicaInfoParsingSingle(BASE_PATH);testReplicaInfoParsingSingle(BASE_PATH + "/subdir1");testReplicaInfoParsingSingle(BASE_PATH + "/subdir1/subdir2/subdir3");}
private static void testReplicaInfoParsingSingle(String subDirPath) {File testFile = new File(subDirPath);assertEquals(BASE_PATH, ReplicaInfo.parseBaseDir(testFile).baseDirPath);}
public void testDuplicateScans() {long startTime = Time.monotonicNow();MiniDFSCluster cluster = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).build();FileSystem fs = null;try {fs = cluster.getFileSystem();DataNode dataNode = cluster.getDataNodes().get(0);int infoPort = dataNode.getInfoPort();long scanTimeBefore = 0, scanTimeAfter = 0;for (int i = 1; i < 10; i++) {Path fileName = new Path("/test" + i);DFSTestUtil.createFile(fs, fileName, 1024, (short) 1, 1000L);waitForVerification(infoPort, fs, fileName, i, startTime, TIMEOUT);if (i > 1) {scanTimeAfter = DataNodeTestUtils.getLatestScanTime(dataNode, DFSTestUtil.getFirstBlock(fs, new Path("/test" + (i - 1))));assertFalse("scan time shoud not be 0", scanTimeAfter == 0);assertEquals("There should not be duplicate scan", scanTimeBefore, scanTimeAfter);}scanTimeBefore = DataNodeTestUtils.getLatestScanTime(dataNode, DFSTestUtil.getFirstBlock(fs, new Path("/test" + i)));}cluster.restartDataNode(0);Thread.sleep(10000);dataNode = cluster.getDataNodes().get(0);scanTimeAfter = DataNodeTestUtils.getLatestScanTime(dataNode, DFSTestUtil.getFirstBlock(fs, new Path("/test" + (9))));assertEquals("There should not be duplicate scan", scanTimeBefore, scanTimeAfter);} finally {IOUtils.closeStream(fs);cluster.shutdown();}}public static void setWorkingDirectory(Job job, Path workingDirectory) {job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, workingDirectory.toString());}
public static void setCommitDirectory(Job job, Path commitDirectory) {job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, commitDirectory.toString());}
public static Path getWorkingDirectory(Job job) {return getWorkingDirectory(job.getConfiguration());}
private static Path getWorkingDirectory(Configuration conf) {String workingDirectory = conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH);if (workingDirectory == null || workingDirectory.isEmpty()) {return null;} else {return new Path(workingDirectory);}}
public static Path getCommitDirectory(Job job) {return getCommitDirectory(job.getConfiguration());}
private static Path getCommitDirectory(Configuration conf) {String commitDirectory = conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH);if (commitDirectory == null || commitDirectory.isEmpty()) {return null;} else {return new Path(commitDirectory);}}
public OutputCommitter getOutputCommitter(TaskAttemptContext context) {return new CopyCommitter(getOutputPath(context), context);}
public void checkOutputSpecs(JobContext context) {Configuration conf = context.getConfiguration();if (getCommitDirectory(conf) == null) {throw new IllegalStateException("Commit directory not configured");}Path workingPath = getWorkingDirectory(conf);if (workingPath == null) {throw new IllegalStateException("Working directory not configured");}// get delegation token for outDir's file systemTokenCache.obtainTokensForNamenodes(context.getCredentials(), new Path[] { workingPath }, conf);}public Text getKind() {return KIND_NAME;}
public UserGroupInformation getUser() {if (jobid == null || "".equals(jobid.toString())) {return null;}return UserGroupInformation.createRemoteUser(jobid.toString());}
public Text getJobId() {return jobid;}
public void readFields(DataInput in) {jobid.readFields(in);}
public void write(DataOutput out) {jobid.write(out);}
protected Text getKind() {return KIND_NAME;}public int compare(RawComparable o1, RawComparable o2) {return compare(o1.buffer(), o1.offset(), o1.size(), o2.buffer(), o2.offset(), o2.size());}
public int compare(byte[] a, int off1, int len1, byte[] b, int off2, int len2) {return cmp.compare(a, off1, len1, b, off2, len2);}
public long magnitude() {return magnitude;}
public int compare(Scalar o1, Scalar o2) {long diff = o1.magnitude() - o2.magnitude();if (diff < 0)return -1;if (diff > 0)return 1;return 0;}
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);}
public int compare(Object o1, Object o2) {throw new RuntimeException("Object comparison not supported");}protected void serviceInit(Configuration conf) {try {//TODO Is this required?FileContext.getLocalFSFileContext(conf);} catch (UnsupportedFileSystemException e) {throw new YarnRuntimeException("Failed to start ContainersLauncher", e);}super.serviceInit(conf);}
protected void serviceStop() {containerLauncher.shutdownNow();super.serviceStop();}
public void handle(ContainersLauncherEvent event) {// TODO: ContainersLauncher launches containers one by one!!Container container = event.getContainer();ContainerId containerId = container.getContainerId();switch(event.getType()) {case LAUNCH_CONTAINER:Application app = context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());ContainerLaunch launch = new ContainerLaunch(context, getConfig(), dispatcher, exec, app, event.getContainer(), dirsHandler, containerManager);containerLauncher.submit(launch);running.put(containerId, launch);break;case RECOVER_CONTAINER:app = context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());launch = new RecoveredContainerLaunch(context, getConfig(), dispatcher, exec, app, event.getContainer(), dirsHandler, containerManager);containerLauncher.submit(launch);running.put(containerId, launch);break;case CLEANUP_CONTAINER:ContainerLaunch launcher = running.remove(containerId);if (launcher == null) {// Container not launched. So nothing needs to be done.return;}// no sub-processes are alive.try {launcher.cleanupContainer();} catch (IOException e) {LOG.warn("Got exception while cleaning container " + containerId + ". Ignoring.");}break;}}public static ApplicationAttemptStateData newInstance(ApplicationAttemptId attemptId, Container container, ByteBuffer attemptTokens, long startTime, RMAppAttemptState finalState, String finalTrackingUrl, String diagnostics, FinalApplicationStatus amUnregisteredFinalStatus, int exitStatus) {ApplicationAttemptStateData attemptStateData = Records.newRecord(ApplicationAttemptStateData.class);attemptStateData.setAttemptId(attemptId);attemptStateData.setMasterContainer(container);attemptStateData.setAppAttemptTokens(attemptTokens);attemptStateData.setState(finalState);attemptStateData.setFinalTrackingUrl(finalTrackingUrl);attemptStateData.setDiagnostics(diagnostics);attemptStateData.setStartTime(startTime);attemptStateData.setFinalApplicationStatus(amUnregisteredFinalStatus);attemptStateData.setAMContainerExitStatus(exitStatus);return attemptStateData;}
public static ApplicationAttemptStateData newInstance(ApplicationAttemptState attemptState) {Credentials credentials = attemptState.getAppAttemptCredentials();ByteBuffer appAttemptTokens = null;if (credentials != null) {DataOutputBuffer dob = new DataOutputBuffer();credentials.writeTokenStorageToStream(dob);appAttemptTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());}return newInstance(attemptState.getAttemptId(), attemptState.getMasterContainer(), appAttemptTokens, attemptState.getStartTime(), attemptState.getState(), attemptState.getFinalTrackingUrl(), attemptState.getDiagnostics(), attemptState.getFinalApplicationStatus(), attemptState.getAMContainerExitStatus());}public void startUpCluster(long splitThreshold) {conf = new HdfsConfiguration();conf.setLong(DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY, splitThreshold);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPL_FACTOR).build();fs = cluster.getFileSystem();bpid = cluster.getNamesystem().getBlockPoolId();}
public void shutDownCluster() {if (cluster != null) {fs.close();cluster.shutdown();cluster = null;}}
private void createFile(String filenamePrefix, int blockCount) {Path path = new Path("/" + filenamePrefix + ".dat");DFSTestUtil.createFile(fs, path, BLOCK_SIZE, blockCount * BLOCK_SIZE, BLOCK_SIZE, REPL_FACTOR, seed);}
private void verifyCapturedArguments(ArgumentCaptor<StorageBlockReport[]> captor, int expectedReportsPerCall, int expectedTotalBlockCount) {List<StorageBlockReport[]> listOfReports = captor.getAllValues();int numBlocksReported = 0;for (StorageBlockReport[] reports : listOfReports) {assertThat(reports.length, is(expectedReportsPerCall));for (StorageBlockReport report : reports) {BlockListAsLongs blockList = new BlockListAsLongs(report.getBlocks());numBlocksReported += blockList.getNumberOfBlocks();}}assert (numBlocksReported >= expectedTotalBlockCount);}
public void testAlwaysSplit() {startUpCluster(0);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(cluster.getStoragesPerDatanode())).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, 1, BLOCKS_IN_FILE);}
public void testCornerCaseUnderThreshold() {startUpCluster(BLOCKS_IN_FILE + 1);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(1)).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, cluster.getStoragesPerDatanode(), BLOCKS_IN_FILE);}
public void testCornerCaseAtThreshold() {startUpCluster(BLOCKS_IN_FILE);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(cluster.getStoragesPerDatanode())).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, 1, BLOCKS_IN_FILE);}public static boolean needsQuoting(byte[] data, int off, int len) {for (int i = off; i < off + len; ++i) {switch(data[i]) {case '&':case '<':case '>':case '\'':case '"':return true;default:break;}}return false;}
public static boolean needsQuoting(String str) {if (str == null) {return false;}byte[] bytes = str.getBytes();return needsQuoting(bytes, 0, bytes.length);}
public static void quoteHtmlChars(OutputStream output, byte[] buffer, int off, int len) {for (int i = off; i < off + len; i++) {switch(buffer[i]) {case '&':output.write(ampBytes);break;case '<':output.write(ltBytes);break;case '>':output.write(gtBytes);break;case '\'':output.write(aposBytes);break;case '"':output.write(quotBytes);break;default:output.write(buffer, i, 1);}}}
public static String quoteHtmlChars(String item) {if (item == null) {return null;}byte[] bytes = item.getBytes();if (needsQuoting(bytes, 0, bytes.length)) {ByteArrayOutputStream buffer = new ByteArrayOutputStream();try {quoteHtmlChars(buffer, bytes, 0, bytes.length);} catch (IOException ioe) {}return buffer.toString();} else {return item;}}
public static OutputStream quoteOutputStream(final OutputStream out) {return new OutputStream() {
private byte[] data = new byte[1];
@Overridepublic void write(byte[] data, int off, int len) throws IOException {quoteHtmlChars(out, data, off, len);}
@Overridepublic void write(int b) throws IOException {data[0] = (byte) b;quoteHtmlChars(out, data, 0, 1);}
@Overridepublic void flush() throws IOException {out.flush();}
@Overridepublic void close() throws IOException {out.close();}};}
public void write(byte[] data, int off, int len) {quoteHtmlChars(out, data, off, len);}
public void write(int b) {data[0] = (byte) b;quoteHtmlChars(out, data, 0, 1);}
public void flush() {out.flush();}
public void close() {out.close();}
public static String unquoteHtmlChars(String item) {if (item == null) {return null;}int next = item.indexOf('&');// nothing was quotedif (next == -1) {return item;}int len = item.length();int posn = 0;StringBuilder buffer = new StringBuilder();while (next != -1) {buffer.append(item.substring(posn, next));if (item.startsWith("&amp;", next)) {buffer.append('&');next += 5;} else if (item.startsWith("&apos;", next)) {buffer.append('\'');next += 6;} else if (item.startsWith("&gt;", next)) {buffer.append('>');next += 4;} else if (item.startsWith("&lt;", next)) {buffer.append('<');next += 4;} else if (item.startsWith("&quot;", next)) {buffer.append('"');next += 6;} else {int end = item.indexOf(';', next) + 1;if (end == 0) {end = len;}throw new IllegalArgumentException("Bad HTML quoting for " + item.substring(next, end));}posn = next;next = item.indexOf('&', posn);}buffer.append(item.substring(posn, len));return buffer.toString();}
public static void main(String[] args) {for (String arg : args) {System.out.println("Original: " + arg);String quoted = quoteHtmlChars(arg);System.out.println("Quoted: " + quoted);String unquoted = unquoteHtmlChars(quoted);System.out.println("Unquoted: " + unquoted);System.out.println();}}private void fillKey(BytesWritable o) {int len = keyLenRNG.nextInt();if (len < MIN_KEY_LEN)len = MIN_KEY_LEN;o.setSize(len);int n = MIN_KEY_LEN;while (n < len) {byte[] word = dict[random.nextInt(dict.length)];int l = Math.min(word.length, len - n);System.arraycopy(word, 0, o.get(), n, l);n += l;}if (sorted && WritableComparator.compareBytes(lastKey.get(), MIN_KEY_LEN, lastKey.getSize() - MIN_KEY_LEN, o.get(), MIN_KEY_LEN, o.getSize() - MIN_KEY_LEN) > 0) {incrementPrefix();}System.arraycopy(prefix, 0, o.get(), 0, MIN_KEY_LEN);lastKey.set(o);}
private void fillValue(BytesWritable o) {int len = valLenRNG.nextInt();o.setSize(len);int n = 0;while (n < len) {byte[] word = dict[random.nextInt(dict.length)];int l = Math.min(word.length, len - n);System.arraycopy(word, 0, o.get(), n, l);n += l;}}
private void incrementPrefix() {for (int i = MIN_KEY_LEN - 1; i >= 0; --i) {++prefix[i];if (prefix[i] != 0)return;}throw new RuntimeException("Prefix overflown");}
public void next(BytesWritable key, BytesWritable value, boolean dupKey) {if (dupKey) {key.set(lastKey);} else {fillKey(key);}fillValue(value);}public void testDirNoAnnotation() {TestDirHelper.getTestDir();}
public void testJettyNoAnnotation() {TestJettyHelper.getJettyServer();}
public void testJettyNoAnnotation2() {TestJettyHelper.getJettyURL();}
public void testHdfsNoAnnotation() {TestHdfsHelper.getHdfsConf();}
public void testHdfsNoAnnotation2() {TestHdfsHelper.getHdfsTestDir();}
public void testDirAnnotation() {assertNotNull(TestDirHelper.getTestDir());}
public void waitFor() {long start = Time.now();long waited = waitFor(1000, new Predicate() {
@Overridepublic boolean evaluate() throws Exception {return true;}});long end = Time.now();assertEquals(waited, 0, 50);assertEquals(end - start - waited, 0, 50);}
public boolean evaluate() {return true;}
public void waitForTimeOutRatio1() {setWaitForRatio(1);long start = Time.now();long waited = waitFor(200, new Predicate() {
@Overridepublic boolean evaluate() throws Exception {return false;}});long end = Time.now();assertEquals(waited, -1);assertEquals(end - start, 200, 50);}
public boolean evaluate() {return false;}
public void waitForTimeOutRatio2() {setWaitForRatio(2);long start = Time.now();long waited = waitFor(200, new Predicate() {
@Overridepublic boolean evaluate() throws Exception {return false;}});long end = Time.now();assertEquals(waited, -1);assertEquals(end - start, 200 * getWaitForRatio(), 50 * getWaitForRatio());}
public boolean evaluate() {return false;}
public void sleepRatio1() {setWaitForRatio(1);long start = Time.now();sleep(100);long end = Time.now();assertEquals(end - start, 100, 50);}
public void sleepRatio2() {setWaitForRatio(1);long start = Time.now();sleep(100);long end = Time.now();assertEquals(end - start, 100 * getWaitForRatio(), 50 * getWaitForRatio());}
public void testHadoopFileSystem() {Configuration conf = TestHdfsHelper.getHdfsConf();FileSystem fs = FileSystem.get(conf);try {OutputStream os = fs.create(new Path(TestHdfsHelper.getHdfsTestDir(), "foo"));os.write(new byte[] { 1 });os.close();InputStream is = fs.open(new Path(TestHdfsHelper.getHdfsTestDir(), "foo"));assertEquals(is.read(), 1);assertEquals(is.read(), -1);is.close();} finally {fs.close();}}
protected void doGet(HttpServletRequest req, HttpServletResponse resp) {resp.getWriter().write("foo");}
public void testJetty() {Context context = new Context();context.setContextPath("/");context.addServlet(MyServlet.class, "/bar");Server server = TestJettyHelper.getJettyServer();server.addHandler(context);server.start();URL url = new URL(TestJettyHelper.getJettyURL(), "/bar");HttpURLConnection conn = (HttpURLConnection) url.openConnection();assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));assertEquals(reader.readLine(), "foo");reader.close();}
public void testException0() {throw new RuntimeException("foo");}
public void testException1() {throw new RuntimeException("foo");}public void init(String compression, String outputFile, int numRecords1stBlock, int numRecords2ndBlock) {this.compression = compression;this.outputFile = outputFile;this.records1stBlock = numRecords1stBlock;this.records2ndBlock = numRecords2ndBlock;}
public void setUp() {conf = new Configuration();path = new Path(ROOT, outputFile);fs = path.getFileSystem(conf);out = fs.create(path);writer = new Writer(out, BLOCK_SIZE, compression, null, conf);writer.append("keyZ".getBytes(), "valueZ".getBytes());writer.append("keyM".getBytes(), "valueM".getBytes());writer.append("keyN".getBytes(), "valueN".getBytes());writer.append("keyA".getBytes(), "valueA".getBytes());closeOutput();}
public void tearDown() {fs.delete(path, true);}
public void testFailureScannerWithKeys() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);try {Scanner scanner = reader.createScannerByKey("aaa".getBytes(), "zzz".getBytes());Assert.fail("Failed to catch creating scanner with keys on unsorted file.");} catch (RuntimeException e) {} finally {reader.close();}}
public void testScan() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);Scanner scanner = reader.createScanner();try {// read key and valuebyte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");scanner.advance();// now try get value firstvbuf = new byte[BUF_SIZE];vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");kbuf = new byte[BUF_SIZE];klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyM");} finally {scanner.close();reader.close();}}
public void testScanRange() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);Scanner scanner = reader.createScanner();try {// read key and valuebyte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");scanner.advance();// now try get value firstvbuf = new byte[BUF_SIZE];vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");kbuf = new byte[BUF_SIZE];klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyM");} finally {scanner.close();reader.close();}}
public void testFailureSeek() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {// can't find ceiltry {scanner.lowerBound("keyN".getBytes());Assert.fail("Cannot search in a unsorted TFile!");} catch (Exception e) {} finally {}// can't find highertry {scanner.upperBound("keyA".getBytes());Assert.fail("Cannot search higher in a unsorted TFile!");} catch (Exception e) {} finally {}// can't seektry {scanner.seekTo("keyM".getBytes());Assert.fail("Cannot search a unsorted TFile!");} catch (Exception e) {} finally {}} finally {scanner.close();reader.close();}}
private void closeOutput() {if (writer != null) {writer.close();writer = null;out.close();out = null;}}/** Compute next point. * Assume the current point is H(index). * Compute H(index+1). *  * @return a 2-dimensional point with coordinates in [0,1)^2 */double[] nextPoint() {index++;for (int i = 0; i < K.length; i++) {for (int j = 0; j < K[i]; j++) {d[i][j]++;x[i] += q[i][j];if (d[i][j] < P[i]) {break;}d[i][j] = 0;x[i] -= (j == 0 ? 1.0 : q[i][j - 1]);}}return x;}
public void map(LongWritable offset, LongWritable size, Context context) {final HaltonSequence haltonsequence = new HaltonSequence(offset.get());long numInside = 0L;long numOutside = 0L;for (long i = 0; i < size.get(); ) {//generate points in a unit squarefinal double[] point = haltonsequence.nextPoint();//count points inside/outside of the inscribed circle of the squarefinal double x = point[0] - 0.5;final double y = point[1] - 0.5;if (x * x + y * y > 0.25) {numOutside++;} else {numInside++;}//report statusi++;if (i % 1000 == 0) {context.setStatus("Generated " + i + " samples.");}}//output map resultscontext.write(new BooleanWritable(true), new LongWritable(numInside));context.write(new BooleanWritable(false), new LongWritable(numOutside));}
public void reduce(BooleanWritable isInside, Iterable<LongWritable> values, Context context) {if (isInside.get()) {for (LongWritable val : values) {numInside += val.get();}} else {for (LongWritable val : values) {numOutside += val.get();}}}
public void cleanup(Context context) {//write output to a fileConfiguration conf = context.getConfiguration();Path outDir = new Path(conf.get(FileOutputFormat.OUTDIR));Path outFile = new Path(outDir, "reduce-out");FileSystem fileSys = FileSystem.get(conf);SequenceFile.Writer writer = SequenceFile.createWriter(fileSys, conf, outFile, LongWritable.class, LongWritable.class, CompressionType.NONE);writer.append(new LongWritable(numInside), new LongWritable(numOutside));writer.close();}
public static BigDecimal estimatePi(int numMaps, long numPoints, Path tmpDir, Configuration conf) {Job job = new Job(conf);//setup job confjob.setJobName(QuasiMonteCarlo.class.getSimpleName());job.setJarByClass(QuasiMonteCarlo.class);job.setInputFormatClass(SequenceFileInputFormat.class);job.setOutputKeyClass(BooleanWritable.class);job.setOutputValueClass(LongWritable.class);job.setOutputFormatClass(SequenceFileOutputFormat.class);job.setMapperClass(QmcMapper.class);job.setReducerClass(QmcReducer.class);job.setNumReduceTasks(1);// multiple writers to the same file.job.setSpeculativeExecution(false);//setup input/output directoriesfinal Path inDir = new Path(tmpDir, "in");final Path outDir = new Path(tmpDir, "out");FileInputFormat.setInputPaths(job, inDir);FileOutputFormat.setOutputPath(job, outDir);final FileSystem fs = FileSystem.get(conf);if (fs.exists(tmpDir)) {throw new IOException("Tmp directory " + fs.makeQualified(tmpDir) + " already exists.  Please remove it first.");}if (!fs.mkdirs(inDir)) {throw new IOException("Cannot create input directory " + inDir);}try {//generate an input file for each map taskfor (int i = 0; i < numMaps; ++i) {final Path file = new Path(inDir, "part" + i);final LongWritable offset = new LongWritable(i * numPoints);final LongWritable size = new LongWritable(numPoints);final SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, LongWritable.class, LongWritable.class, CompressionType.NONE);try {writer.append(offset, size);} finally {writer.close();}System.out.println("Wrote input for Map #" + i);}//start a map/reduce jobSystem.out.println("Starting Job");final long startTime = System.currentTimeMillis();job.waitForCompletion(true);final double duration = (System.currentTimeMillis() - startTime) / 1000.0;System.out.println("Job Finished in " + duration + " seconds");//read outputsPath inFile = new Path(outDir, "reduce-out");LongWritable numInside = new LongWritable();LongWritable numOutside = new LongWritable();SequenceFile.Reader reader = new SequenceFile.Reader(fs, inFile, conf);try {reader.next(numInside, numOutside);} finally {reader.close();}//compute estimated valuefinal BigDecimal numTotal = BigDecimal.valueOf(numMaps).multiply(BigDecimal.valueOf(numPoints));return BigDecimal.valueOf(4).setScale(20).multiply(BigDecimal.valueOf(numInside.get())).divide(numTotal, RoundingMode.HALF_UP);} finally {fs.delete(tmpDir, true);}}
public int run(String[] args) {if (args.length != 2) {System.err.println("Usage: " + getClass().getName() + " <nMaps> <nSamples>");ToolRunner.printGenericCommandUsage(System.err);return 2;}final int nMaps = Integer.parseInt(args[0]);final long nSamples = Long.parseLong(args[1]);long now = System.currentTimeMillis();int rand = new Random().nextInt(Integer.MAX_VALUE);final Path tmpDir = new Path(TMP_DIR_PREFIX + "_" + now + "_" + rand);System.out.println("Number of Maps  = " + nMaps);System.out.println("Samples per Map = " + nSamples);System.out.println("Estimated value of Pi is " + estimatePi(nMaps, nSamples, tmpDir, getConf()));return 0;}
public static void main(String[] argv) {System.exit(ToolRunner.run(null, new QuasiMonteCarlo(), argv));}public synchronized boolean needsInput() {if (state == GzipStateLabel.DEFLATE_STREAM) {// most common casereturn inflater.needsInput();}// verify userBufLen <= 0return (state != GzipStateLabel.FINISHED);}
public synchronized void setInput(byte[] b, int off, int len) {if (b == null) {throw new NullPointerException();}if (off < 0 || len < 0 || off > b.length - len) {throw new ArrayIndexOutOfBoundsException();}userBuf = b;userBufOff = off;// note:  might be zerouserBufLen = len;}
public synchronized int decompress(byte[] b, int off, int len) {int numAvailBytes = 0;if (state != GzipStateLabel.DEFLATE_STREAM) {executeHeaderState();if (userBufLen <= 0) {return numAvailBytes;}}// "executeDeflateStreamState()"if (state == GzipStateLabel.DEFLATE_STREAM) {// userBufLen will be zeroif (userBufLen > 0) {inflater.setInput(userBuf, userBufOff, userBufLen);userBufOff += userBufLen;userBufLen = 0;}// now decompress it into b[]try {numAvailBytes = inflater.inflate(b, off, len);} catch (DataFormatException dfe) {throw new IOException(dfe.getMessage());}// CRC-32 is on _uncompressed_ datacrc.update(b, off, numAvailBytes);if (inflater.finished()) {state = GzipStateLabel.TRAILER_CRC;int bytesRemaining = inflater.getRemaining();assert (bytesRemaining >= 0) : "logic error: Inflater finished; byte-count is inconsistent";// be a (class) member variable...seems excessive for a sanity checkuserBufOff -= bytesRemaining;// or "+=", but guaranteed 0 coming inuserBufLen = bytesRemaining;} else {// minor optimizationreturn numAvailBytes;}}executeTrailerState();return numAvailBytes;}
private void executeHeaderState() {// to call decompress() first, setInput() second:if (userBufLen <= 0) {return;}// "basic"/required header:  somewhere in first 10 bytesif (state == GzipStateLabel.HEADER_BASIC) {// (or 10-headerBytesRead)int n = Math.min(userBufLen, 10 - localBufOff);// modifies userBufLen, etc.checkAndCopyBytesToLocal(n);if (localBufOff >= 10) {// sig, compression method, flagbitsprocessBasicHeader();// no further need for basic headerlocalBufOff = 0;state = GzipStateLabel.HEADER_EXTRA_FIELD;}}if (userBufLen <= 0) {return;}if (state == GzipStateLabel.HEADER_EXTRA_FIELD) {if (hasExtraField) {// or already have 2 bytes & waiting to finish skipping specified lengthif (numExtraFieldBytesRemaining < 0) {int n = Math.min(userBufLen, 2 - localBufOff);checkAndCopyBytesToLocal(n);if (localBufOff >= 2) {numExtraFieldBytesRemaining = readUShortLE(localBuf, 0);localBufOff = 0;}}if (numExtraFieldBytesRemaining > 0 && userBufLen > 0) {int n = Math.min(userBufLen, numExtraFieldBytesRemaining);// modifies userBufLen, etc.checkAndSkipBytes(n);numExtraFieldBytesRemaining -= n;}if (numExtraFieldBytesRemaining == 0) {state = GzipStateLabel.HEADER_FILENAME;}} else {state = GzipStateLabel.HEADER_FILENAME;}}if (userBufLen <= 0) {return;}if (state == GzipStateLabel.HEADER_FILENAME) {if (hasFilename) {boolean doneWithFilename = checkAndSkipBytesUntilNull();if (!doneWithFilename) {// exit early:  used up entire buffer without hitting NULLreturn;}}state = GzipStateLabel.HEADER_COMMENT;}if (userBufLen <= 0) {return;}if (state == GzipStateLabel.HEADER_COMMENT) {if (hasComment) {boolean doneWithComment = checkAndSkipBytesUntilNull();if (!doneWithComment) {// exit early:  used up entire bufferreturn;}}state = GzipStateLabel.HEADER_CRC;}if (userBufLen <= 0) {return;}if (state == GzipStateLabel.HEADER_CRC) {if (hasHeaderCRC) {assert (localBufOff < 2);int n = Math.min(userBufLen, 2 - localBufOff);copyBytesToLocal(n);if (localBufOff >= 2) {long headerCRC = readUShortLE(localBuf, 0);if (headerCRC != (crc.getValue() & 0xffff)) {throw new IOException("gzip header CRC failure");}localBufOff = 0;crc.reset();state = GzipStateLabel.DEFLATE_STREAM;}} else {// will reuse for CRC-32 of uncompressed datacrc.reset();// switching to Inflater nowstate = GzipStateLabel.DEFLATE_STREAM;}}}
private void executeTrailerState() {if (userBufLen <= 0) {return;}// stored in the gzip trailerif (state == GzipStateLabel.TRAILER_CRC) {// initially 0, but may need multiple callsassert (localBufOff < 4);int n = Math.min(userBufLen, 4 - localBufOff);copyBytesToLocal(n);if (localBufOff >= 4) {long streamCRC = readUIntLE(localBuf, 0);if (streamCRC != crc.getValue()) {throw new IOException("gzip stream CRC failure");}localBufOff = 0;crc.reset();state = GzipStateLabel.TRAILER_SIZE;}}if (userBufLen <= 0) {return;}// stored in the gzip trailerif (state == GzipStateLabel.TRAILER_SIZE) {// initially 0, but may need multiple callsassert (localBufOff < 4);int n = Math.min(userBufLen, 4 - localBufOff);// modifies userBufLen, etc.copyBytesToLocal(n);if (localBufOff >= 4) {// should be strictly ==long inputSize = readUIntLE(localBuf, 0);if (inputSize != (inflater.getBytesWritten() & 0xffffffffL)) {throw new IOException("stored gzip size doesn't match decompressed size");}localBufOff = 0;state = GzipStateLabel.FINISHED;}}if (state == GzipStateLabel.FINISHED) {return;}}
public synchronized long getBytesRead() {return headerBytesRead + inflater.getBytesRead() + trailerBytesRead;}
public synchronized int getRemaining() {return userBufLen;}
public synchronized boolean needsDictionary() {return inflater.needsDictionary();}
public synchronized void setDictionary(byte[] b, int off, int len) {inflater.setDictionary(b, off, len);}
public synchronized boolean finished() {return (state == GzipStateLabel.FINISHED);}
public synchronized void reset() {// could optionally emit INFO message if state != GzipStateLabel.FINISHEDinflater.reset();state = GzipStateLabel.HEADER_BASIC;crc.reset();userBufOff = userBufLen = 0;localBufOff = 0;headerBytesRead = 0;trailerBytesRead = 0;numExtraFieldBytesRemaining = -1;hasExtraField = false;hasFilename = false;hasComment = false;hasHeaderCRC = false;}
public synchronized void end() {inflater.end();}
private void processBasicHeader() {if (readUShortLE(localBuf, 0) != GZIP_MAGIC_ID) {throw new IOException("not a gzip file");}if (readUByte(localBuf, 2) != GZIP_DEFLATE_METHOD) {throw new IOException("gzip data not compressed with deflate method");}int flg = readUByte(localBuf, 3);if ((flg & GZIP_FLAGBITS_RESERVED) != 0) {throw new IOException("unknown gzip format (reserved flagbits set)");}hasExtraField = ((flg & GZIP_FLAGBIT_EXTRA_FIELD) != 0);hasFilename = ((flg & GZIP_FLAGBIT_FILENAME) != 0);hasComment = ((flg & GZIP_FLAGBIT_COMMENT) != 0);hasHeaderCRC = ((flg & GZIP_FLAGBIT_HEADER_CRC) != 0);}
private void checkAndCopyBytesToLocal(int len) {System.arraycopy(userBuf, userBufOff, localBuf, localBufOff, len);localBufOff += len;// alternatively, could call checkAndSkipBytes(len) for rest...crc.update(userBuf, userBufOff, len);userBufOff += len;userBufLen -= len;headerBytesRead += len;}
private void checkAndSkipBytes(int len) {crc.update(userBuf, userBufOff, len);userBufOff += len;userBufLen -= len;headerBytesRead += len;}
private boolean checkAndSkipBytesUntilNull() {boolean hitNull = false;if (userBufLen > 0) {do {hitNull = (userBuf[userBufOff] == 0);crc.update(userBuf[userBufOff]);++userBufOff;--userBufLen;++headerBytesRead;} while (userBufLen > 0 && !hitNull);}return hitNull;}
private void copyBytesToLocal(int len) {System.arraycopy(userBuf, userBufOff, localBuf, localBufOff, len);localBufOff += len;userBufOff += len;userBufLen -= len;if (state == GzipStateLabel.TRAILER_CRC || state == GzipStateLabel.TRAILER_SIZE) {trailerBytesRead += len;} else {headerBytesRead += len;}}
private int readUByte(byte[] b, int off) {return ((int) b[off] & 0xff);}
private int readUShortLE(byte[] b, int off) {return ((((b[off + 1] & 0xff) << 8) | ((b[off] & 0xff))) & 0xffff);}
private long readUIntLE(byte[] b, int off) {return ((((long) (b[off + 3] & 0xff) << 24) | ((long) (b[off + 2] & 0xff) << 16) | ((long) (b[off + 1] & 0xff) << 8) | ((long) (b[off] & 0xff))) & 0xffffffffL);}public void testGetNewStamp() {int numDataNodes = 1;Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();try {cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();NamenodeProtocols namenode = cluster.getNameNodeRpc();/* Test writing to finalized replicas */Path file = new Path("dataprotocol.dat");DFSTestUtil.createFile(fileSys, file, 1L, (short) numDataNodes, 0L);// get the first blockid for the fileExtendedBlock firstBlock = DFSTestUtil.getFirstBlock(fileSys, file);// test getNewStampAndToken on a finalized blocktry {namenode.updateBlockForPipeline(firstBlock, "");Assert.fail("Can not get a new GS from a finalized block");} catch (IOException e) {Assert.assertTrue(e.getMessage().contains("is not under Construction"));}// test getNewStampAndToken on a non-existent blocktry {long newBlockId = firstBlock.getBlockId() + 1;ExtendedBlock newBlock = new ExtendedBlock(firstBlock.getBlockPoolId(), newBlockId, 0, firstBlock.getGenerationStamp());namenode.updateBlockForPipeline(newBlock, "");Assert.fail("Cannot get a new GS from a non-existent block");} catch (IOException e) {Assert.assertTrue(e.getMessage().contains("does not exist"));}// change first block to a RBWDFSOutputStream out = null;try {out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();FSDataInputStream in = null;try {in = fileSys.open(file);firstBlock = DFSTestUtil.getAllBlocks(in).get(0).getBlock();} finally {IOUtils.closeStream(in);}// test non-lease holderDFSClient dfs = ((DistributedFileSystem) fileSys).dfs;try {namenode.updateBlockForPipeline(firstBlock, "test" + dfs.clientName);Assert.fail("Cannot get a new GS for a non lease holder");} catch (LeaseExpiredException e) {Assert.assertTrue(e.getMessage().startsWith("Lease mismatch"));}// test null lease holdertry {namenode.updateBlockForPipeline(firstBlock, null);Assert.fail("Cannot get a new GS for a null lease holder");} catch (LeaseExpiredException e) {Assert.assertTrue(e.getMessage().startsWith("Lease mismatch"));}// test getNewStampAndToken on a rbw blocknamenode.updateBlockForPipeline(firstBlock, dfs.clientName);} finally {IOUtils.closeStream(out);}} finally {cluster.shutdown();}}
public void testPipelineRecoveryForLastBlock() {DFSClientFaultInjector faultInjector = Mockito.mock(DFSClientFaultInjector.class);DFSClientFaultInjector oldInjector = DFSClientFaultInjector.instance;DFSClientFaultInjector.instance = faultInjector;Configuration conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_RETRIES_KEY, 3);MiniDFSCluster cluster = null;try {int numDataNodes = 3;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol1.dat");Mockito.when(faultInjector.failPacket()).thenReturn(true);DFSTestUtil.createFile(fileSys, file, 68000000L, (short) numDataNodes, 0L);// Read should succeed.FSDataInputStream in = fileSys.open(file);try {int c = in.read();} catch (org.apache.hadoop.hdfs.BlockMissingException bme) {Assert.fail("Block is missing because the file was closed with" + " corrupt replicas.");}} finally {DFSClientFaultInjector.instance = oldInjector;if (cluster != null) {cluster.shutdown();}}}
public void testPipelineRecoveryOnOOB() {Configuration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_CLIENT_DATANODE_RESTART_TIMEOUT_KEY, "15");MiniDFSCluster cluster = null;try {int numDataNodes = 1;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol2.dat");DFSTestUtil.createFile(fileSys, file, 10240L, (short) 1, 0L);DFSOutputStream out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();DFSAdmin dfsadmin = new DFSAdmin(conf);DataNode dn = cluster.getDataNodes().get(0);final String dnAddr = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args1 = { "-shutdownDatanode", dnAddr, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args1));// Wait long enough to receive an OOB ack before closing the file.Thread.sleep(4000);// Retart the datanode cluster.restartDataNode(0, true);// The following forces a data packet and end of block packets to be sent. out.close();} finally {if (cluster != null) {cluster.shutdown();}}}
public void testPipelineRecoveryOnRestartFailure() {Configuration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_CLIENT_DATANODE_RESTART_TIMEOUT_KEY, "5");MiniDFSCluster cluster = null;try {int numDataNodes = 2;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol3.dat");DFSTestUtil.createFile(fileSys, file, 10240L, (short) 2, 0L);DFSOutputStream out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();DFSAdmin dfsadmin = new DFSAdmin(conf);DataNode dn = cluster.getDataNodes().get(0);final String dnAddr1 = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args1 = { "-shutdownDatanode", dnAddr1, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args1));Thread.sleep(4000);// expire and regular pipeline recovery will kick in. out.close();// At this point there is only one node in the cluster. out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();dn = cluster.getDataNodes().get(1);final String dnAddr2 = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args2 = { "-shutdownDatanode", dnAddr2, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args2));Thread.sleep(4000);try {// close should failout.close();assert false;} catch (IOException ioe) {}} finally {if (cluster != null) {cluster.shutdown();}}}public void set(byte[] bytes) {this.count = (bytes == null) ? 0 : bytes.length;this.bytes = bytes;}
public final void copy(byte[] bytes, int offset, int length) {if (this.bytes == null || this.bytes.length < length) {this.bytes = new byte[length];}System.arraycopy(bytes, offset, this.bytes, 0, length);this.count = length;}
public byte[] get() {if (bytes == null) {bytes = new byte[0];}return bytes;}
public int getCount() {return count;}
public int getCapacity() {return this.get().length;}
public void setCapacity(int newCapacity) {if (newCapacity < 0) {throw new IllegalArgumentException("Invalid capacity argument " + newCapacity);}if (newCapacity == 0) {this.bytes = null;this.count = 0;return;}if (newCapacity != getCapacity()) {byte[] data = new byte[newCapacity];if (newCapacity < count) {count = newCapacity;}if (count != 0) {System.arraycopy(this.get(), 0, data, 0, count);}bytes = data;}}
public void reset() {setCapacity(0);}
public void truncate() {setCapacity(count);}
public void append(byte[] bytes, int offset, int length) {setCapacity(count + length);System.arraycopy(bytes, offset, this.get(), count, length);count = count + length;}
public void append(byte[] bytes) {append(bytes, 0, bytes.length);}
public int hashCode() {int hash = 1;byte[] b = this.get();for (int i = 0; i < count; i++) hash = (31 * hash) + b[i];return hash;}
public int compareTo(Object other) {Buffer right = ((Buffer) other);byte[] lb = this.get();byte[] rb = right.get();for (int i = 0; i < count && i < right.count; i++) {int a = (lb[i] & 0xff);int b = (rb[i] & 0xff);if (a != b) {return a - b;}}return count - right.count;}
public boolean equals(Object other) {if (other instanceof Buffer && this != other) {return compareTo(other) == 0;}return (this == other);}
public String toString() {StringBuilder sb = new StringBuilder(2 * count);for (int idx = 0; idx < count; idx++) {sb.append(Character.forDigit((bytes[idx] & 0xF0) >> 4, 16));sb.append(Character.forDigit(bytes[idx] & 0x0F, 16));}return sb.toString();}
public String toString(String charsetName) {return new String(this.get(), 0, this.getCount(), charsetName);}
public Object clone() {Buffer result = (Buffer) super.clone();result.copy(this.get(), 0, this.getCount());return result;}public void testRecovery() {YarnConfiguration conf = new YarnConfiguration();conf.setBoolean(YarnConfiguration.NM_RECOVERY_ENABLED, true);final NodeId nodeId = NodeId.newInstance("somehost", 1234);final ApplicationAttemptId attempt1 = ApplicationAttemptId.newInstance(ApplicationId.newInstance(1, 1), 1);final ApplicationAttemptId attempt2 = ApplicationAttemptId.newInstance(ApplicationId.newInstance(2, 2), 2);NMTokenKeyGeneratorForTest keygen = new NMTokenKeyGeneratorForTest();NMMemoryStateStoreService stateStore = new NMMemoryStateStoreService();stateStore.init(conf);stateStore.start();NMTokenSecretManagerInNM secretMgr = new NMTokenSecretManagerInNM(stateStore);secretMgr.setNodeId(nodeId);MasterKey currentKey = keygen.generateKey();secretMgr.setMasterKey(currentKey);NMTokenIdentifier attemptToken1 = getNMTokenId(secretMgr.createNMToken(attempt1, nodeId, "user1"));NMTokenIdentifier attemptToken2 = getNMTokenId(secretMgr.createNMToken(attempt2, nodeId, "user2"));secretMgr.appAttemptStartContainer(attemptToken1);secretMgr.appAttemptStartContainer(attemptToken2);assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt1));assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt2));assertNotNull(secretMgr.retrievePassword(attemptToken1));assertNotNull(secretMgr.retrievePassword(attemptToken2));// restart and verify key is still there and token still validsecretMgr = new NMTokenSecretManagerInNM(stateStore);secretMgr.recover();secretMgr.setNodeId(nodeId);assertEquals(currentKey, secretMgr.getCurrentKey());assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt1));assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt2));assertNotNull(secretMgr.retrievePassword(attemptToken1));assertNotNull(secretMgr.retrievePassword(attemptToken2));// roll master key and remove an appcurrentKey = keygen.generateKey();secretMgr.setMasterKey(currentKey);secretMgr.appFinished(attempt1.getApplicationId());// restart and verify attempt1 key is still valid due to prev key persistsecretMgr = new NMTokenSecretManagerInNM(stateStore);secretMgr.recover();secretMgr.setNodeId(nodeId);assertEquals(currentKey, secretMgr.getCurrentKey());assertFalse(secretMgr.isAppAttemptNMTokenKeyPresent(attempt1));assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt2));assertNotNull(secretMgr.retrievePassword(attemptToken1));assertNotNull(secretMgr.retrievePassword(attemptToken2));// attempt2 is still good due to app key persistcurrentKey = keygen.generateKey();secretMgr.setMasterKey(currentKey);secretMgr = new NMTokenSecretManagerInNM(stateStore);secretMgr.recover();secretMgr.setNodeId(nodeId);assertEquals(currentKey, secretMgr.getCurrentKey());assertFalse(secretMgr.isAppAttemptNMTokenKeyPresent(attempt1));assertTrue(secretMgr.isAppAttemptNMTokenKeyPresent(attempt2));try {secretMgr.retrievePassword(attemptToken1);fail("attempt token should not still be valid");} catch (InvalidToken e) {}assertNotNull(secretMgr.retrievePassword(attemptToken2));// remove last attempt, restart, verify both tokens are now badsecretMgr.appFinished(attempt2.getApplicationId());secretMgr = new NMTokenSecretManagerInNM(stateStore);secretMgr.recover();secretMgr.setNodeId(nodeId);assertEquals(currentKey, secretMgr.getCurrentKey());assertFalse(secretMgr.isAppAttemptNMTokenKeyPresent(attempt1));assertFalse(secretMgr.isAppAttemptNMTokenKeyPresent(attempt2));try {secretMgr.retrievePassword(attemptToken1);fail("attempt token should not still be valid");} catch (InvalidToken e) {}try {secretMgr.retrievePassword(attemptToken2);fail("attempt token should not still be valid");} catch (InvalidToken e) {}stateStore.close();}
private NMTokenIdentifier getNMTokenId(org.apache.hadoop.yarn.api.records.Token token) {Token<NMTokenIdentifier> convertedToken = ConverterUtils.convertFromYarn(token, (Text) null);return convertedToken.decodeIdentifier();}
public MasterKey generateKey() {return createNewMasterKey().getMasterKey();}public Object invoke(Object proxy, Method method, Object[] args) {RetryPolicy policy = methodNameToPolicyMap.get(method.getName());if (policy == null) {policy = defaultPolicy;}// The number of times this method invocation has been failed over.int invocationFailoverCount = 0;final boolean isRpc = isRpcInvocation(currentProxy.proxy);final int callId = isRpc ? Client.nextCallId() : RpcConstants.INVALID_CALL_ID;int retries = 0;while (true) {// failed method invocations from triggering multiple failover attempts.long invocationAttemptFailoverCount;synchronized (proxyProvider) {invocationAttemptFailoverCount = proxyProviderFailoverCount;}if (isRpc) {Client.setCallIdAndRetryCount(callId, retries);}try {Object ret = invokeMethod(method, args);hasMadeASuccessfulCall = true;return ret;} catch (Exception e) {boolean isIdempotentOrAtMostOnce = proxyProvider.getInterface().getMethod(method.getName(), method.getParameterTypes()).isAnnotationPresent(Idempotent.class);if (!isIdempotentOrAtMostOnce) {isIdempotentOrAtMostOnce = proxyProvider.getInterface().getMethod(method.getName(), method.getParameterTypes()).isAnnotationPresent(AtMostOnce.class);}RetryAction action = policy.shouldRetry(e, retries++, invocationFailoverCount, isIdempotentOrAtMostOnce);if (action.action == RetryAction.RetryDecision.FAIL) {if (action.reason != null) {LOG.warn("Exception while invoking " + currentProxy.proxy.getClass() + "." + method.getName() + " over " + currentProxy.proxyInfo + ". Not retrying because " + action.reason, e);}throw e;} else {boolean worthLogging = !(invocationFailoverCount == 0 && !hasMadeASuccessfulCall);worthLogging |= LOG.isDebugEnabled();if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY && worthLogging) {String msg = "Exception while invoking " + method.getName() + " of class " + currentProxy.proxy.getClass().getSimpleName() + " over " + currentProxy.proxyInfo;if (invocationFailoverCount > 0) {msg += " after " + invocationFailoverCount + " fail over attempts";}msg += ". Trying to fail over " + formatSleepMessage(action.delayMillis);LOG.info(msg, e);} else {if (LOG.isDebugEnabled()) {LOG.debug("Exception while invoking " + method.getName() + " of class " + currentProxy.proxy.getClass().getSimpleName() + " over " + currentProxy.proxyInfo + ". Retrying " + formatSleepMessage(action.delayMillis), e);}}if (action.delayMillis > 0) {Thread.sleep(action.delayMillis);}if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {synchronized (proxyProvider) {if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {proxyProvider.performFailover(currentProxy.proxy);proxyProviderFailoverCount++;} else {LOG.warn("A failover has occurred since the start of this method" + " invocation attempt.");}currentProxy = proxyProvider.getProxy();}invocationFailoverCount++;}}}}}
private static String formatSleepMessage(long millis) {if (millis > 0) {return "after sleeping for " + millis + "ms.";} else {return "immediately.";}}
protected Object invokeMethod(Method method, Object[] args) {try {if (!method.isAccessible()) {method.setAccessible(true);}return method.invoke(currentProxy.proxy, args);} catch (InvocationTargetException e) {throw e.getCause();}}
static boolean isRpcInvocation(Object proxy) {if (proxy instanceof ProtocolTranslator) {proxy = ((ProtocolTranslator) proxy).getUnderlyingProxyObject();}if (!Proxy.isProxyClass(proxy.getClass())) {return false;}final InvocationHandler ih = Proxy.getInvocationHandler(proxy);return ih instanceof RpcInvocationHandler;}
public void close() {proxyProvider.close();}
public ConnectionId getConnectionId() {return RPC.getConnectionIdForProxy(currentProxy.proxy);}public void close() {if (this.proxy != null) {RPC.stopProxy(this.proxy);}}
public KillApplicationResponse forceKillApplication(KillApplicationRequest request) {KillApplicationRequestProto requestProto = ((KillApplicationRequestPBImpl) request).getProto();try {return new KillApplicationResponsePBImpl(proxy.forceKillApplication(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationReportResponse getApplicationReport(GetApplicationReportRequest request) {GetApplicationReportRequestProto requestProto = ((GetApplicationReportRequestPBImpl) request).getProto();try {return new GetApplicationReportResponsePBImpl(proxy.getApplicationReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetClusterMetricsResponse getClusterMetrics(GetClusterMetricsRequest request) {GetClusterMetricsRequestProto requestProto = ((GetClusterMetricsRequestPBImpl) request).getProto();try {return new GetClusterMetricsResponsePBImpl(proxy.getClusterMetrics(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetNewApplicationResponse getNewApplication(GetNewApplicationRequest request) {GetNewApplicationRequestProto requestProto = ((GetNewApplicationRequestPBImpl) request).getProto();try {return new GetNewApplicationResponsePBImpl(proxy.getNewApplication(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public SubmitApplicationResponse submitApplication(SubmitApplicationRequest request) {SubmitApplicationRequestProto requestProto = ((SubmitApplicationRequestPBImpl) request).getProto();try {return new SubmitApplicationResponsePBImpl(proxy.submitApplication(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationsResponse getApplications(GetApplicationsRequest request) {GetApplicationsRequestProto requestProto = ((GetApplicationsRequestPBImpl) request).getProto();try {return new GetApplicationsResponsePBImpl(proxy.getApplications(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetClusterNodesResponse getClusterNodes(GetClusterNodesRequest request) {GetClusterNodesRequestProto requestProto = ((GetClusterNodesRequestPBImpl) request).getProto();try {return new GetClusterNodesResponsePBImpl(proxy.getClusterNodes(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetQueueInfoResponse getQueueInfo(GetQueueInfoRequest request) {GetQueueInfoRequestProto requestProto = ((GetQueueInfoRequestPBImpl) request).getProto();try {return new GetQueueInfoResponsePBImpl(proxy.getQueueInfo(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetQueueUserAclsInfoResponse getQueueUserAcls(GetQueueUserAclsInfoRequest request) {GetQueueUserAclsInfoRequestProto requestProto = ((GetQueueUserAclsInfoRequestPBImpl) request).getProto();try {return new GetQueueUserAclsInfoResponsePBImpl(proxy.getQueueUserAcls(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetDelegationTokenResponse getDelegationToken(GetDelegationTokenRequest request) {GetDelegationTokenRequestProto requestProto = ((GetDelegationTokenRequestPBImpl) request).getProto();try {return new GetDelegationTokenResponsePBImpl(proxy.getDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public RenewDelegationTokenResponse renewDelegationToken(RenewDelegationTokenRequest request) {RenewDelegationTokenRequestProto requestProto = ((RenewDelegationTokenRequestPBImpl) request).getProto();try {return new RenewDelegationTokenResponsePBImpl(proxy.renewDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public CancelDelegationTokenResponse cancelDelegationToken(CancelDelegationTokenRequest request) {CancelDelegationTokenRequestProto requestProto = ((CancelDelegationTokenRequestPBImpl) request).getProto();try {return new CancelDelegationTokenResponsePBImpl(proxy.cancelDelegationToken(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public MoveApplicationAcrossQueuesResponse moveApplicationAcrossQueues(MoveApplicationAcrossQueuesRequest request) {MoveApplicationAcrossQueuesRequestProto requestProto = ((MoveApplicationAcrossQueuesRequestPBImpl) request).getProto();try {return new MoveApplicationAcrossQueuesResponsePBImpl(proxy.moveApplicationAcrossQueues(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationAttemptReportResponse getApplicationAttemptReport(GetApplicationAttemptReportRequest request) {GetApplicationAttemptReportRequestProto requestProto = ((GetApplicationAttemptReportRequestPBImpl) request).getProto();try {return new GetApplicationAttemptReportResponsePBImpl(proxy.getApplicationAttemptReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetApplicationAttemptsResponse getApplicationAttempts(GetApplicationAttemptsRequest request) {GetApplicationAttemptsRequestProto requestProto = ((GetApplicationAttemptsRequestPBImpl) request).getProto();try {return new GetApplicationAttemptsResponsePBImpl(proxy.getApplicationAttempts(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetContainerReportResponse getContainerReport(GetContainerReportRequest request) {GetContainerReportRequestProto requestProto = ((GetContainerReportRequestPBImpl) request).getProto();try {return new GetContainerReportResponsePBImpl(proxy.getContainerReport(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}
public GetContainersResponse getContainers(GetContainersRequest request) {GetContainersRequestProto requestProto = ((GetContainersRequestPBImpl) request).getProto();try {return new GetContainersResponsePBImpl(proxy.getContainers(null, requestProto));} catch (ServiceException e) {RPCUtil.unwrapAndThrowException(e);return null;}}public void testHamlet() {Hamlet h = newHamlet().title("test").h1("heading 1").p("#id.class").b("hello").em("world!")._().div("#footer")._("Brought to you by").a("http://hostname/", "Somebody")._();PrintWriter out = h.getWriter();out.flush();assertEquals(0, h.nestLevel);verify(out).print("<title");verify(out).print("test");verify(out).print("</title>");verify(out).print("<h1");verify(out).print("heading 1");verify(out).print("</h1>");verify(out).print("<p");verify(out).print(" id=\"id\"");verify(out).print(" class=\"class\"");verify(out).print("<b");verify(out).print("hello");verify(out).print("</b>");verify(out).print("<em");verify(out).print("world!");verify(out).print("</em>");verify(out).print("<div");verify(out).print(" id=\"footer\"");verify(out).print("Brought to you by");verify(out).print("<a");verify(out).print(" href=\"http://hostname/\"");verify(out).print("Somebody");verify(out).print("</a>");verify(out).print("</div>");verify(out, never()).print("</p>");}
public void testTable() {Hamlet h = newHamlet().title("test table").link("style.css");TABLE t = h.table("#id");for (int i = 0; i < 3; ++i) {t.tr().td("1").td("2")._();}t._();PrintWriter out = h.getWriter();out.flush();assertEquals(0, h.nestLevel);verify(out).print("<table");verify(out).print("</table>");verify(out, never()).print("</td>");verify(out, never()).print("</tr>");}
public void testEnumAttrs() {Hamlet h = newHamlet().meta_http("Content-type", "text/html; charset=utf-8").title("test enum attrs").link().$rel("stylesheet").$media(EnumSet.of(Media.screen, Media.print)).$type("text/css").$href("style.css")._().link().$rel(EnumSet.of(LinkType.index, LinkType.start)).$href("index.html")._();h.div("#content")._("content")._();PrintWriter out = h.getWriter();out.flush();assertEquals(0, h.nestLevel);verify(out).print(" media=\"screen, print\"");verify(out).print(" rel=\"start index\"");}
public void testScriptStyle() {Hamlet h = newHamlet().script("a.js").script("b.js").style("h1 { font-size: 1.2em }");PrintWriter out = h.getWriter();out.flush();assertEquals(0, h.nestLevel);verify(out, times(2)).print(" type=\"text/javascript\"");verify(out).print(" type=\"text/css\"");}
public void testPreformatted() {Hamlet h = newHamlet().div().i("inline before pre").pre()._("pre text1\npre text2").i("inline in pre")._("pre text after inline")._().i("inline after pre")._();PrintWriter out = h.getWriter();out.flush();assertEquals(5, h.indents);}
public void renderPartial() {}
public void renderPartial() {}
public void testSubViews() {Hamlet h = newHamlet().title("test sub-views").div("#view1")._(TestView1.class)._().div("#view2")._(TestView2.class)._();PrintWriter out = h.getWriter();out.flush();assertEquals(0, h.nestLevel);verify(out).print("[" + TestView1.class.getName() + "]");verify(out).print("[" + TestView2.class.getName() + "]");}
static Hamlet newHamlet() {PrintWriter out = spy(new PrintWriter(System.out));return new Hamlet(out, 0, false);}public void setupConfAndServices() {conf = new Configuration();conf.set(ZKFailoverController.ZK_ACL_KEY, TEST_ACL);conf.set(ZKFailoverController.ZK_AUTH_KEY, TEST_AUTH_GOOD);conf.set(ZKFailoverController.ZK_QUORUM_KEY, hostPort);this.cluster = new MiniZKFCCluster(conf, getServer(serverFactory));}
public void testFormatZK() {DummyHAService svc = cluster.getService(1);// should barfassertEquals(ZKFailoverController.ERR_CODE_NO_PARENT_ZNODE, runFC(svc));// Format the base dir, should succeedassertEquals(0, runFC(svc, "-formatZK"));// Should fail to format if already formattedassertEquals(ZKFailoverController.ERR_CODE_FORMAT_DENIED, runFC(svc, "-formatZK", "-nonInteractive"));// Unless '-force' is onassertEquals(0, runFC(svc, "-formatZK", "-force"));}
public void testNoZK() {stopServer();DummyHAService svc = cluster.getService(1);assertEquals(ZKFailoverController.ERR_CODE_NO_ZK, runFC(svc));}
public void testFormatOneClusterLeavesOtherClustersAlone() {DummyHAService svc = cluster.getService(1);DummyZKFC zkfcInOtherCluster = new DummyZKFC(conf, cluster.getService(1)) {
@Overrideprotected String getScopeInsideParentNode() {return "other-scope";}};// should barfassertEquals(ZKFailoverController.ERR_CODE_NO_PARENT_ZNODE, runFC(svc));// Format the base dir, should succeedassertEquals(0, runFC(svc, "-formatZK"));// it uses a different parent znodeassertEquals(ZKFailoverController.ERR_CODE_NO_PARENT_ZNODE, zkfcInOtherCluster.run(new String[] {}));// Should succeed in formatting the second clusterassertEquals(0, zkfcInOtherCluster.run(new String[] { "-formatZK" }));// clusterassertEquals(ZKFailoverController.ERR_CODE_FORMAT_DENIED, runFC(svc, "-formatZK", "-nonInteractive"));}
protected String getScopeInsideParentNode() {return "other-scope";}
public void testWontRunWhenAutoFailoverDisabled() {DummyHAService svc = cluster.getService(1);svc = Mockito.spy(svc);Mockito.doReturn(false).when(svc).isAutoFailoverEnabled();assertEquals(ZKFailoverController.ERR_CODE_AUTO_FAILOVER_NOT_ENABLED, runFC(svc, "-formatZK"));assertEquals(ZKFailoverController.ERR_CODE_AUTO_FAILOVER_NOT_ENABLED, runFC(svc));}
public void testFormatSetsAcls() {// Format the base dir, should succeedDummyHAService svc = cluster.getService(1);assertEquals(0, runFC(svc, "-formatZK"));ZooKeeper otherClient = createClient();try {// client without auth should not be able to read itStat stat = new Stat();otherClient.getData(ZKFailoverController.ZK_PARENT_ZNODE_DEFAULT, false, stat);fail("Was able to read data without authenticating!");} catch (KeeperException.NoAuthException nae) {}}
public void testFencingMustBeConfigured() {DummyHAService svc = Mockito.spy(cluster.getService(0));Mockito.doThrow(new BadFencingConfigurationException("no fencing")).when(svc).checkFencingConfigured();// Format the base dir, should succeedassertEquals(0, runFC(svc, "-formatZK"));// Try to run the actual FC, should fail without a fencerassertEquals(ZKFailoverController.ERR_CODE_NO_FENCER, runFC(svc));}
public void testAutoFailoverOnBadHealth() {try {cluster.start();DummyHAService svc1 = cluster.getService(1);LOG.info("Faking svc0 unhealthy, should failover to svc1");cluster.setHealthy(0, false);LOG.info("Waiting for svc0 to enter standby state");cluster.waitForHAState(0, HAServiceState.STANDBY);cluster.waitForHAState(1, HAServiceState.ACTIVE);LOG.info("Allowing svc0 to be healthy again, making svc1 unreachable " + "and fail to gracefully go to standby");cluster.setUnreachable(1, true);cluster.setHealthy(0, true);// Should fail back to svc0 at this pointcluster.waitForHAState(0, HAServiceState.ACTIVE);// and fence svc1Mockito.verify(svc1.fencer).fence(Mockito.same(svc1));} finally {cluster.stop();}}
public void testAutoFailoverOnBadState() {try {cluster.start();DummyHAService svc0 = cluster.getService(0);LOG.info("Faking svc0 to change the state, should failover to svc1");svc0.state = HAServiceState.STANDBY;// Should fail back to svc0 at this pointcluster.waitForHAState(1, HAServiceState.ACTIVE);} finally {cluster.stop();}}
public void testAutoFailoverOnLostZKSession() {try {cluster.start();// Expire svc0, it should fail over to svc1cluster.expireAndVerifyFailover(0, 1);// Expire svc1, it should fail back to svc0cluster.expireAndVerifyFailover(1, 0);LOG.info("======= Running test cases second time to test " + "re-establishment =========");// Expire svc0, it should fail over to svc1cluster.expireAndVerifyFailover(0, 1);// Expire svc1, it should fail back to svc0cluster.expireAndVerifyFailover(1, 0);} finally {cluster.stop();}}
public void testDontFailoverToUnhealthyNode() {try {cluster.start();// Make svc1 unhealthy, and wait for its FC to notice the bad health.cluster.setHealthy(1, false);cluster.waitForHealthState(1, HealthMonitor.State.SERVICE_UNHEALTHY);// Expire svc0cluster.getElector(0).preventSessionReestablishmentForTests();try {cluster.expireActiveLockHolder(0);LOG.info("Expired svc0's ZK session. Waiting a second to give svc1" + " a chance to take the lock, if it is ever going to.");Thread.sleep(1000);// Ensure that no one holds the lock.cluster.waitForActiveLockHolder(null);} finally {LOG.info("Allowing svc0's elector to re-establish its connection");cluster.getElector(0).allowSessionReestablishmentForTests();}// svc0 should get the lock againcluster.waitForActiveLockHolder(0);} finally {cluster.stop();}}
public void testBecomingActiveFails() {try {cluster.start();DummyHAService svc1 = cluster.getService(1);LOG.info("Making svc1 fail to become active");cluster.setFailToBecomeActive(1, true);LOG.info("Faking svc0 unhealthy, should NOT successfully " + "failover to svc1");cluster.setHealthy(0, false);cluster.waitForHealthState(0, State.SERVICE_UNHEALTHY);cluster.waitForActiveLockHolder(null);Mockito.verify(svc1.proxy, Mockito.timeout(2000).atLeastOnce()).transitionToActive(Mockito.<StateChangeRequestInfo>any());cluster.waitForHAState(0, HAServiceState.STANDBY);cluster.waitForHAState(1, HAServiceState.STANDBY);LOG.info("Faking svc0 healthy again, should go back to svc0");cluster.setHealthy(0, true);cluster.waitForHAState(0, HAServiceState.ACTIVE);cluster.waitForHAState(1, HAServiceState.STANDBY);cluster.waitForActiveLockHolder(0);// to become active (e.g the admin has restarted it)LOG.info("Allowing svc1 to become active, expiring svc0");svc1.failToBecomeActive = false;cluster.expireAndVerifyFailover(0, 1);} finally {cluster.stop();}}
public void testZooKeeperFailure() {try {cluster.start();// Record initial ZK sessionslong session0 = cluster.getElector(0).getZKSessionIdForTests();long session1 = cluster.getElector(1).getZKSessionIdForTests();LOG.info("====== Stopping ZK server");stopServer();waitForServerDown(hostPort, CONNECTION_TIMEOUT);LOG.info("====== Waiting for services to enter NEUTRAL mode");cluster.waitForElectorState(0, ActiveStandbyElector.State.NEUTRAL);cluster.waitForElectorState(1, ActiveStandbyElector.State.NEUTRAL);LOG.info("====== Checking that the services didn't change HA state");assertEquals(HAServiceState.ACTIVE, cluster.getService(0).state);assertEquals(HAServiceState.STANDBY, cluster.getService(1).state);LOG.info("====== Restarting server");startServer();waitForServerUp(hostPort, CONNECTION_TIMEOUT);// the same sessions.cluster.waitForElectorState(0, ActiveStandbyElector.State.ACTIVE);cluster.waitForElectorState(1, ActiveStandbyElector.State.STANDBY);// Check HA states didn't change.cluster.waitForHAState(0, HAServiceState.ACTIVE);cluster.waitForHAState(1, HAServiceState.STANDBY);// Check they re-used the same sessions and didn't spuriously reconnectassertEquals(session0, cluster.getElector(0).getZKSessionIdForTests());assertEquals(session1, cluster.getElector(1).getZKSessionIdForTests());} finally {cluster.stop();}}
public void testCedeActive() {try {cluster.start();DummyZKFC zkfc = cluster.getZkfc(0);// It should be in active to start.assertEquals(ActiveStandbyElector.State.ACTIVE, zkfc.getElectorForTests().getStateForTests());// (i.e. the RPC itself should not take 3 seconds!)ZKFCProtocol proxy = zkfc.getLocalTarget().getZKFCProxy(conf, 5000);long st = Time.now();proxy.cedeActive(3000);long et = Time.now();assertTrue("RPC to cedeActive took " + (et - st) + " ms", et - st < 1000);// at this point.assertEquals(ActiveStandbyElector.State.INIT, zkfc.getElectorForTests().getStateForTests());// since the other node in the cluster would have taken ACTIVE.cluster.waitForElectorState(0, ActiveStandbyElector.State.STANDBY);long et2 = Time.now();assertTrue("Should take ~3 seconds to rejoin. Only took " + (et2 - et) + "ms before rejoining.", et2 - et > 2800);} finally {cluster.stop();}}
public void testGracefulFailover() {try {cluster.start();cluster.waitForActiveLockHolder(0);cluster.getService(1).getZKFCProxy(conf, 5000).gracefulFailover();cluster.waitForActiveLockHolder(1);cluster.getService(0).getZKFCProxy(conf, 5000).gracefulFailover();cluster.waitForActiveLockHolder(0);// allow to quiesceThread.sleep(10000);assertEquals(0, cluster.getService(0).fenceCount);assertEquals(0, cluster.getService(1).fenceCount);assertEquals(2, cluster.getService(0).activeTransitionCount);assertEquals(1, cluster.getService(1).activeTransitionCount);} finally {cluster.stop();}}
public void testGracefulFailoverToUnhealthy() {try {cluster.start();cluster.waitForActiveLockHolder(0);// Mark it unhealthy, wait for it to exit electioncluster.setHealthy(1, false);cluster.waitForElectorState(1, ActiveStandbyElector.State.INIT);// Ask for failover, it should fail, because it's unhealthytry {cluster.getService(1).getZKFCProxy(conf, 5000).gracefulFailover();fail("Did not fail to graceful failover to unhealthy service!");} catch (ServiceFailedException sfe) {GenericTestUtils.assertExceptionContains(cluster.getService(1).toString() + " is not currently healthy.", sfe);}} finally {cluster.stop();}}
public void testGracefulFailoverFailBecomingActive() {try {cluster.start();cluster.waitForActiveLockHolder(0);cluster.setFailToBecomeActive(1, true);// Ask for failover, it should fail and report back to user.try {cluster.getService(1).getZKFCProxy(conf, 5000).gracefulFailover();fail("Did not fail to graceful failover when target failed " + "to become active!");} catch (ServiceFailedException sfe) {GenericTestUtils.assertExceptionContains("Couldn't make " + cluster.getService(1) + " active", sfe);GenericTestUtils.assertExceptionContains("injected failure", sfe);}// No fencingassertEquals(0, cluster.getService(0).fenceCount);assertEquals(0, cluster.getService(1).fenceCount);// Service 0 should go back to being active after the failed failovercluster.waitForActiveLockHolder(0);} finally {cluster.stop();}}
public void testGracefulFailoverFailBecomingStandby() {try {cluster.start();cluster.waitForActiveLockHolder(0);// still works, but leaves the breadcrumb in place.cluster.setFailToBecomeStandby(0, true);cluster.getService(1).getZKFCProxy(conf, 5000).gracefulFailover();// Check that the old node was fencedassertEquals(1, cluster.getService(0).fenceCount);} finally {cluster.stop();}}
public void testGracefulFailoverFailBecomingStandbyAndFailFence() {try {cluster.start();cluster.waitForActiveLockHolder(0);// still works, but leaves the breadcrumb in place.cluster.setFailToBecomeStandby(0, true);cluster.setFailToFence(0, true);try {cluster.getService(1).getZKFCProxy(conf, 5000).gracefulFailover();fail("Failover should have failed when old node wont fence");} catch (ServiceFailedException sfe) {GenericTestUtils.assertExceptionContains("Unable to fence " + cluster.getService(0), sfe);}} finally {cluster.stop();}}
public void testOneOfEverything() {try {cluster.start();// Failover by session expirationLOG.info("====== Failing over by session expiration");cluster.expireAndVerifyFailover(0, 1);cluster.expireAndVerifyFailover(1, 0);// Restart ZKLOG.info("====== Restarting server");stopServer();waitForServerDown(hostPort, CONNECTION_TIMEOUT);startServer();waitForServerUp(hostPort, CONNECTION_TIMEOUT);// Failover by bad healthcluster.setHealthy(0, false);cluster.waitForHAState(0, HAServiceState.STANDBY);cluster.waitForHAState(1, HAServiceState.ACTIVE);cluster.setHealthy(1, true);cluster.setHealthy(0, false);cluster.waitForHAState(1, HAServiceState.ACTIVE);cluster.waitForHAState(0, HAServiceState.STANDBY);cluster.setHealthy(0, true);cluster.waitForHealthState(0, State.SERVICE_HEALTHY);// Graceful failoverscluster.getZkfc(1).gracefulFailoverToYou();cluster.getZkfc(0).gracefulFailoverToYou();} finally {cluster.stop();}}
private int runFC(DummyHAService target, String... args) {DummyZKFC zkfc = new DummyZKFC(conf, target);return zkfc.run(args);}public void testRegisterNodeManagerResponsePBImpl() {RegisterNodeManagerResponsePBImpl original = new RegisterNodeManagerResponsePBImpl();original.setContainerTokenMasterKey(getMasterKey());original.setNMTokenMasterKey(getMasterKey());original.setNodeAction(NodeAction.NORMAL);original.setDiagnosticsMessage("testDiagnosticMessage");RegisterNodeManagerResponsePBImpl copy = new RegisterNodeManagerResponsePBImpl(original.getProto());assertEquals(1, copy.getContainerTokenMasterKey().getKeyId());assertEquals(1, copy.getNMTokenMasterKey().getKeyId());assertEquals(NodeAction.NORMAL, copy.getNodeAction());assertEquals("testDiagnosticMessage", copy.getDiagnosticsMessage());}
public void testNodeHeartbeatRequestPBImpl() {NodeHeartbeatRequestPBImpl original = new NodeHeartbeatRequestPBImpl();original.setLastKnownContainerTokenMasterKey(getMasterKey());original.setLastKnownNMTokenMasterKey(getMasterKey());original.setNodeStatus(getNodeStatus());NodeHeartbeatRequestPBImpl copy = new NodeHeartbeatRequestPBImpl(original.getProto());assertEquals(1, copy.getLastKnownContainerTokenMasterKey().getKeyId());assertEquals(1, copy.getLastKnownNMTokenMasterKey().getKeyId());assertEquals("localhost", copy.getNodeStatus().getNodeId().getHost());}
public void testNodeHeartbeatResponsePBImpl() {NodeHeartbeatResponsePBImpl original = new NodeHeartbeatResponsePBImpl();original.setDiagnosticsMessage("testDiagnosticMessage");original.setContainerTokenMasterKey(getMasterKey());original.setNMTokenMasterKey(getMasterKey());original.setNextHeartBeatInterval(1000);original.setNodeAction(NodeAction.NORMAL);original.setResponseId(100);NodeHeartbeatResponsePBImpl copy = new NodeHeartbeatResponsePBImpl(original.getProto());assertEquals(100, copy.getResponseId());assertEquals(NodeAction.NORMAL, copy.getNodeAction());assertEquals(1000, copy.getNextHeartBeatInterval());assertEquals(1, copy.getContainerTokenMasterKey().getKeyId());assertEquals(1, copy.getNMTokenMasterKey().getKeyId());assertEquals("testDiagnosticMessage", copy.getDiagnosticsMessage());}
public void testRegisterNodeManagerRequestPBImpl() {RegisterNodeManagerRequestPBImpl original = new RegisterNodeManagerRequestPBImpl();original.setHttpPort(8080);original.setNodeId(getNodeId());Resource resource = recordFactory.newRecordInstance(Resource.class);resource.setMemory(10000);resource.setVirtualCores(2);original.setResource(resource);RegisterNodeManagerRequestPBImpl copy = new RegisterNodeManagerRequestPBImpl(original.getProto());assertEquals(8080, copy.getHttpPort());assertEquals(9090, copy.getNodeId().getPort());assertEquals(10000, copy.getResource().getMemory());assertEquals(2, copy.getResource().getVirtualCores());}
public void testMasterKeyPBImpl() {MasterKeyPBImpl original = new MasterKeyPBImpl();original.setBytes(ByteBuffer.allocate(0));original.setKeyId(1);MasterKeyPBImpl copy = new MasterKeyPBImpl(original.getProto());assertEquals(1, copy.getKeyId());assertTrue(original.equals(copy));assertEquals(original.hashCode(), copy.hashCode());}
public void testSerializedExceptionPBImpl() {SerializedExceptionPBImpl original = new SerializedExceptionPBImpl();original.init("testMessage");SerializedExceptionPBImpl copy = new SerializedExceptionPBImpl(original.getProto());assertEquals("testMessage", copy.getMessage());original = new SerializedExceptionPBImpl();original.init("testMessage", new Throwable(new Throwable("parent")));copy = new SerializedExceptionPBImpl(original.getProto());assertEquals("testMessage", copy.getMessage());assertEquals("parent", copy.getCause().getMessage());assertTrue(copy.getRemoteTrace().startsWith("java.lang.Throwable: java.lang.Throwable: parent"));}
public void testNodeStatusPBImpl() {NodeStatusPBImpl original = new NodeStatusPBImpl();original.setContainersStatuses(Arrays.asList(getContainerStatus(1, 2, 1), getContainerStatus(2, 3, 1)));original.setKeepAliveApplications(Arrays.asList(getApplicationId(3), getApplicationId(4)));original.setNodeHealthStatus(getNodeHealthStatus());original.setNodeId(getNodeId());original.setResponseId(1);NodeStatusPBImpl copy = new NodeStatusPBImpl(original.getProto());assertEquals(3, copy.getContainersStatuses().get(1).getContainerId().getId());assertEquals(3, copy.getKeepAliveApplications().get(0).getId());assertEquals(1000, copy.getNodeHealthStatus().getLastHealthReportTime());assertEquals(9090, copy.getNodeId().getPort());assertEquals(1, copy.getResponseId());}
private ContainerStatus getContainerStatus(int applicationId, int containerID, int appAttemptId) {ContainerStatus status = recordFactory.newRecordInstance(ContainerStatus.class);status.setContainerId(getContainerId(containerID, appAttemptId));return status;}
private ApplicationAttemptId getApplicationAttemptId(int appAttemptId) {ApplicationAttemptId result = ApplicationAttemptIdPBImpl.newInstance(getApplicationId(appAttemptId), appAttemptId);return result;}
private ContainerId getContainerId(int containerID, int appAttemptId) {ContainerId containerId = ContainerIdPBImpl.newInstance(getApplicationAttemptId(appAttemptId), containerID);return containerId;}
private ApplicationId getApplicationId(int applicationId) {ApplicationIdPBImpl appId = new ApplicationIdPBImpl() {
public ApplicationIdPBImpl setParameters(int id, long timestamp) {setClusterTimestamp(timestamp);setId(id);build();return this;}}.setParameters(applicationId, 1000);return new ApplicationIdPBImpl(appId.getProto());}
public ApplicationIdPBImpl setParameters(int id, long timestamp) {setClusterTimestamp(timestamp);setId(id);build();return this;}
private NodeStatus getNodeStatus() {NodeStatus status = recordFactory.newRecordInstance(NodeStatus.class);status.setContainersStatuses(new ArrayList<ContainerStatus>());status.setKeepAliveApplications(new ArrayList<ApplicationId>());status.setNodeHealthStatus(getNodeHealthStatus());status.setNodeId(getNodeId());status.setResponseId(1);return status;}
private NodeId getNodeId() {return NodeId.newInstance("localhost", 9090);}
private NodeHealthStatus getNodeHealthStatus() {NodeHealthStatus healStatus = recordFactory.newRecordInstance(NodeHealthStatus.class);healStatus.setHealthReport("healthReport");healStatus.setIsNodeHealthy(true);healStatus.setLastHealthReportTime(1000);return healStatus;}
private MasterKey getMasterKey() {MasterKey key = recordFactory.newRecordInstance(MasterKey.class);key.setBytes(ByteBuffer.allocate(0));key.setKeyId(1);return key;}public void handle(SchedulerEvent event) {scheduler.handle(event);}
public void handle(NodesListManagerEvent event) {nodesListManagerEvent = event;}
public void setUp() {InlineDispatcher rmDispatcher = new InlineDispatcher();rmContext = new RMContextImpl(rmDispatcher, null, null, null, mock(DelegationTokenRenewer.class), null, null, null, null, null);NodesListManager nodesListManager = mock(NodesListManager.class);HostsFileReader reader = mock(HostsFileReader.class);when(nodesListManager.getHostsReader()).thenReturn(reader);((RMContextImpl) rmContext).setNodesListManager(nodesListManager);scheduler = mock(YarnScheduler.class);doAnswer(new Answer<Void>() {
@Overridepublic Void answer(InvocationOnMock invocation) throws Throwable {final SchedulerEvent event = (SchedulerEvent) (invocation.getArguments()[0]);eventType = event.getType();if (eventType == SchedulerEventType.NODE_UPDATE) {List<UpdatedContainerInfo> lastestContainersInfoList = ((NodeUpdateSchedulerEvent) event).getRMNode().pullContainerUpdates();for (UpdatedContainerInfo lastestContainersInfo : lastestContainersInfoList) {completedContainers.addAll(lastestContainersInfo.getCompletedContainers());}}return null;}}).when(scheduler).handle(any(SchedulerEvent.class));rmDispatcher.register(SchedulerEventType.class, new TestSchedulerEventDispatcher());rmDispatcher.register(NodesListManagerEventType.class, new TestNodeListManagerEventDispatcher());NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);nodesListManagerEvent = null;}
public Void answer(InvocationOnMock invocation) {final SchedulerEvent event = (SchedulerEvent) (invocation.getArguments()[0]);eventType = event.getType();if (eventType == SchedulerEventType.NODE_UPDATE) {List<UpdatedContainerInfo> lastestContainersInfoList = ((NodeUpdateSchedulerEvent) event).getRMNode().pullContainerUpdates();for (UpdatedContainerInfo lastestContainersInfo : lastestContainersInfoList) {completedContainers.addAll(lastestContainersInfo.getCompletedContainers());}}return null;}
public void tearDown() {}
private RMNodeStatusEvent getMockRMNodeStatusEvent() {NodeHeartbeatResponse response = mock(NodeHeartbeatResponse.class);NodeHealthStatus healthStatus = mock(NodeHealthStatus.class);Boolean yes = new Boolean(true);doReturn(yes).when(healthStatus).getIsNodeHealthy();RMNodeStatusEvent event = mock(RMNodeStatusEvent.class);doReturn(healthStatus).when(event).getNodeHealthStatus();doReturn(response).when(event).getLatestResponse();doReturn(RMNodeEventType.STATUS_UPDATE).when(event).getType();return event;}
public void testExpiredContainer() {// Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));verify(scheduler).handle(any(NodeAddedSchedulerEvent.class));// Expire a containerContainerId completedContainerId = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);node.handle(new RMNodeCleanContainerEvent(null, completedContainerId));Assert.assertEquals(1, node.getContainersToCleanUp().size());// by checking number of 'completedContainers' it got in the previous eventRMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();ContainerStatus containerStatus = mock(ContainerStatus.class);doReturn(completedContainerId).when(containerStatus).getContainerId();doReturn(Collections.singletonList(containerStatus)).when(statusEvent).getContainers();node.handle(statusEvent);/* Expect the scheduler call handle function 2 times * 1. RMNode status from new to Running, handle the add_node event * 2. handle the node update event */verify(scheduler, times(2)).handle(any(NodeUpdateSchedulerEvent.class));}
public void testContainerUpdate() {//Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));NodeId nodeId = BuilderUtils.newNodeId("localhost:1", 1);RMNodeImpl node2 = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);node2.handle(new RMNodeStartedEvent(null, null, null));ContainerId completedContainerIdFromNode1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);ContainerId completedContainerIdFromNode2_1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 1);ContainerId completedContainerIdFromNode2_2 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 2);RMNodeStatusEvent statusEventFromNode1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEventFromNode2_1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEventFromNode2_2 = getMockRMNodeStatusEvent();ContainerStatus containerStatusFromNode1 = mock(ContainerStatus.class);ContainerStatus containerStatusFromNode2_1 = mock(ContainerStatus.class);ContainerStatus containerStatusFromNode2_2 = mock(ContainerStatus.class);doReturn(completedContainerIdFromNode1).when(containerStatusFromNode1).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode1)).when(statusEventFromNode1).getContainers();node.handle(statusEventFromNode1);Assert.assertEquals(1, completedContainers.size());Assert.assertEquals(completedContainerIdFromNode1, completedContainers.get(0).getContainerId());completedContainers.clear();doReturn(completedContainerIdFromNode2_1).when(containerStatusFromNode2_1).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode2_1)).when(statusEventFromNode2_1).getContainers();doReturn(completedContainerIdFromNode2_2).when(containerStatusFromNode2_2).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode2_2)).when(statusEventFromNode2_2).getContainers();node2.setNextHeartBeat(false);node2.handle(statusEventFromNode2_1);node2.setNextHeartBeat(true);node2.handle(statusEventFromNode2_2);Assert.assertEquals(2, completedContainers.size());Assert.assertEquals(completedContainerIdFromNode2_1, completedContainers.get(0).getContainerId());Assert.assertEquals(completedContainerIdFromNode2_2, completedContainers.get(1).getContainerId());}
public void testStatusChange() {//Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));//Add info to the queue firstnode.setNextHeartBeat(false);ContainerId completedContainerId1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);ContainerId completedContainerId2 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 1);RMNodeStatusEvent statusEvent1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEvent2 = getMockRMNodeStatusEvent();ContainerStatus containerStatus1 = mock(ContainerStatus.class);ContainerStatus containerStatus2 = mock(ContainerStatus.class);doReturn(completedContainerId1).when(containerStatus1).getContainerId();doReturn(Collections.singletonList(containerStatus1)).when(statusEvent1).getContainers();doReturn(completedContainerId2).when(containerStatus2).getContainerId();doReturn(Collections.singletonList(containerStatus2)).when(statusEvent2).getContainers();verify(scheduler, times(1)).handle(any(NodeUpdateSchedulerEvent.class));node.handle(statusEvent1);node.handle(statusEvent2);verify(scheduler, times(1)).handle(any(NodeUpdateSchedulerEvent.class));Assert.assertEquals(2, node.getQueueSize());node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals(0, node.getQueueSize());}
public void testRunningExpire() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost + 1, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.LOST, node.getState());}
public void testUnhealthyExpire() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost + 1, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.LOST, node.getState());}
public void testUnhealthyExpireForSchedulerRemove() {RMNodeImpl node = getUnhealthyNode();verify(scheduler, times(2)).handle(any(NodeRemovedSchedulerEvent.class));node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));verify(scheduler, times(2)).handle(any(NodeRemovedSchedulerEvent.class));Assert.assertEquals(NodeState.LOST, node.getState());}
public void testRunningDecommission() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.DECOMMISSION));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned + 1, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.DECOMMISSIONED, node.getState());}
public void testUnhealthyDecommission() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.DECOMMISSION));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned + 1, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.DECOMMISSIONED, node.getState());}
public void testRunningRebooting() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.REBOOTING));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted + 1, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.REBOOTED, node.getState());}
public void testUnhealthyRebooting() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.REBOOTING));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted + 1, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.REBOOTED, node.getState());}
public void testUpdateHeartbeatResponseForCleanup() {RMNodeImpl node = getRunningNode();NodeId nodeId = node.getNodeID();// Expire a containerContainerId completedContainerId = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);node.handle(new RMNodeCleanContainerEvent(nodeId, completedContainerId));Assert.assertEquals(1, node.getContainersToCleanUp().size());// Finish an applicationApplicationId finishedAppId = BuilderUtils.newApplicationId(0, 1);node.handle(new RMNodeCleanAppEvent(nodeId, finishedAppId));Assert.assertEquals(1, node.getAppsToCleanup().size());// but updating heartbeat response for cleanup doesRMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();node.handle(statusEvent);Assert.assertEquals(1, node.getContainersToCleanUp().size());Assert.assertEquals(1, node.getAppsToCleanup().size());NodeHeartbeatResponse hbrsp = Records.newRecord(NodeHeartbeatResponse.class);node.updateNodeHeartbeatResponseForCleanup(hbrsp);Assert.assertEquals(0, node.getContainersToCleanUp().size());Assert.assertEquals(0, node.getAppsToCleanup().size());Assert.assertEquals(1, hbrsp.getContainersToCleanup().size());Assert.assertEquals(completedContainerId, hbrsp.getContainersToCleanup().get(0));Assert.assertEquals(1, hbrsp.getApplicationsToCleanup().size());Assert.assertEquals(finishedAppId, hbrsp.getApplicationsToCleanup().get(0));}
private RMNodeImpl getRunningNode() {return getRunningNode(null);}
private RMNodeImpl getRunningNode(String nmVersion) {NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);Resource capability = Resource.newInstance(4096, 4);RMNodeImpl node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, ResourceOption.newInstance(capability, RMNode.OVER_COMMIT_TIMEOUT_MILLIS_DEFAULT), nmVersion);node.handle(new RMNodeStartedEvent(node.getNodeID(), null, null));Assert.assertEquals(NodeState.RUNNING, node.getState());return node;}
private RMNodeImpl getUnhealthyNode() {RMNodeImpl node = getRunningNode();NodeHealthStatus status = NodeHealthStatus.newInstance(false, "sick", System.currentTimeMillis());node.handle(new RMNodeStatusEvent(node.getNodeID(), status, new ArrayList<ContainerStatus>(), null, null));Assert.assertEquals(NodeState.UNHEALTHY, node.getState());return node;}
private RMNodeImpl getNewNode() {NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);RMNodeImpl node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);return node;}
public void testAdd() {RMNodeImpl node = getNewNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeStartedEvent(node.getNodeID(), null, null));Assert.assertEquals("Active Nodes", initialActive + 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.RUNNING, node.getState());Assert.assertNotNull(nodesListManagerEvent);Assert.assertEquals(NodesListManagerEventType.NODE_USABLE, nodesListManagerEvent.getType());}
public void testReconnect() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeReconnectEvent(node.getNodeID(), node, null));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.RUNNING, node.getState());Assert.assertNotNull(nodesListManagerEvent);Assert.assertEquals(NodesListManagerEventType.NODE_USABLE, nodesListManagerEvent.getType());}
public void testReconnnectUpdate() {final String nmVersion1 = "nm version 1";final String nmVersion2 = "nm version 2";RMNodeImpl node = getRunningNode(nmVersion1);Assert.assertEquals(nmVersion1, node.getNodeManagerVersion());RMNodeImpl reconnectingNode = getRunningNode(nmVersion2);node.handle(new RMNodeReconnectEvent(node.getNodeID(), reconnectingNode, null));Assert.assertEquals(nmVersion2, node.getNodeManagerVersion());}public void setUp() {Configuration conf = new Configuration();FSNamesystem fsn = mock(FSNamesystem.class);doAnswer(new Answer() {
@Overridepublic Object answer(InvocationOnMock invocation) throws Throwable {Object[] args = invocation.getArguments();FsPermission perm = (FsPermission) args[0];return new PermissionStatus(SUPERUSER, SUPERGROUP, perm);}}).when(fsn).createFsOwnerPermissions(any(FsPermission.class));dir = new FSDirectory(fsn, conf);inodeRoot = dir.getRoot();}
public Object answer(InvocationOnMock invocation) {Object[] args = invocation.getArguments();FsPermission perm = (FsPermission) args[0];return new PermissionStatus(SUPERUSER, SUPERGROUP, perm);}
public void testAclOwner() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0640);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, NONE));assertPermissionGranted(BRUCE, "/file1", READ);assertPermissionGranted(BRUCE, "/file1", WRITE);assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionDenied(BRUCE, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);}
public void testAclNamedUser() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0640);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, "diana", READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, NONE));assertPermissionGranted(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ_WRITE);assertPermissionDenied(DIANA, "/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/file1", ALL);}
public void testAclNamedUserDeny() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0644);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, "diana", NONE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionDenied(DIANA, "/file1", READ);}
public void testAclNamedUserTraverseDeny() {INodeDirectory inodeDir = createINodeDirectory(inodeRoot, "dir1", "bruce", "execs", (short) 0755);INodeFile inodeFile = createINodeFile(inodeDir, "file1", "bruce", "execs", (short) 0644);addAcl(inodeDir, aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, "diana", NONE), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, MASK, READ_EXECUTE), aclEntry(ACCESS, OTHER, READ_EXECUTE));assertPermissionGranted(BRUCE, "/dir1/file1", READ_WRITE);assertPermissionGranted(CLARK, "/dir1/file1", READ);assertPermissionDenied(DIANA, "/dir1/file1", READ);assertPermissionDenied(DIANA, "/dir1/file1", WRITE);assertPermissionDenied(DIANA, "/dir1/file1", EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", READ_WRITE);assertPermissionDenied(DIANA, "/dir1/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", ALL);}
public void testAclNamedUserMask() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0620);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, "diana", READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, WRITE), aclEntry(ACCESS, OTHER, NONE));assertPermissionDenied(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ_WRITE);assertPermissionDenied(DIANA, "/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/file1", ALL);}
public void testAclGroup() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0640);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, NONE));assertPermissionGranted(CLARK, "/file1", READ);assertPermissionDenied(CLARK, "/file1", WRITE);assertPermissionDenied(CLARK, "/file1", EXECUTE);assertPermissionDenied(CLARK, "/file1", READ_WRITE);assertPermissionDenied(CLARK, "/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/file1", ALL);}
public void testAclGroupDeny() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "sales", (short) 0604);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, MASK, NONE), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionDenied(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ_WRITE);assertPermissionDenied(DIANA, "/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/file1", ALL);}
public void testAclGroupTraverseDeny() {INodeDirectory inodeDir = createINodeDirectory(inodeRoot, "dir1", "bruce", "execs", (short) 0755);INodeFile inodeFile = createINodeFile(inodeDir, "file1", "bruce", "execs", (short) 0644);addAcl(inodeDir, aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, MASK, NONE), aclEntry(ACCESS, OTHER, READ_EXECUTE));assertPermissionGranted(BRUCE, "/dir1/file1", READ_WRITE);assertPermissionGranted(DIANA, "/dir1/file1", READ);assertPermissionDenied(CLARK, "/dir1/file1", READ);assertPermissionDenied(CLARK, "/dir1/file1", WRITE);assertPermissionDenied(CLARK, "/dir1/file1", EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", READ_WRITE);assertPermissionDenied(CLARK, "/dir1/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", ALL);}
public void testAclGroupTraverseDenyOnlyDefaultEntries() {INodeDirectory inodeDir = createINodeDirectory(inodeRoot, "dir1", "bruce", "execs", (short) 0755);INodeFile inodeFile = createINodeFile(inodeDir, "file1", "bruce", "execs", (short) 0644);addAcl(inodeDir, aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, "sales", NONE), aclEntry(DEFAULT, GROUP, NONE), aclEntry(DEFAULT, OTHER, READ_EXECUTE));assertPermissionGranted(BRUCE, "/dir1/file1", READ_WRITE);assertPermissionGranted(DIANA, "/dir1/file1", READ);assertPermissionDenied(CLARK, "/dir1/file1", READ);assertPermissionDenied(CLARK, "/dir1/file1", WRITE);assertPermissionDenied(CLARK, "/dir1/file1", EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", READ_WRITE);assertPermissionDenied(CLARK, "/dir1/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/dir1/file1", ALL);}
public void testAclGroupMask() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0644);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ_WRITE), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionDenied(CLARK, "/file1", WRITE);assertPermissionDenied(CLARK, "/file1", EXECUTE);assertPermissionDenied(CLARK, "/file1", READ_WRITE);assertPermissionDenied(CLARK, "/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/file1", ALL);}
public void testAclNamedGroup() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0640);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, GROUP, "sales", READ), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, NONE));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionGranted(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ_WRITE);assertPermissionDenied(DIANA, "/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/file1", ALL);}
public void testAclNamedGroupDeny() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "sales", (short) 0644);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, GROUP, "execs", NONE), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(DIANA, "/file1", READ);assertPermissionDenied(CLARK, "/file1", READ);assertPermissionDenied(CLARK, "/file1", WRITE);assertPermissionDenied(CLARK, "/file1", EXECUTE);assertPermissionDenied(CLARK, "/file1", READ_WRITE);assertPermissionDenied(CLARK, "/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/file1", ALL);}
public void testAclNamedGroupTraverseDeny() {INodeDirectory inodeDir = createINodeDirectory(inodeRoot, "dir1", "bruce", "execs", (short) 0755);INodeFile inodeFile = createINodeFile(inodeDir, "file1", "bruce", "execs", (short) 0644);addAcl(inodeDir, aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, GROUP, "sales", NONE), aclEntry(ACCESS, MASK, READ_EXECUTE), aclEntry(ACCESS, OTHER, READ_EXECUTE));assertPermissionGranted(BRUCE, "/dir1/file1", READ_WRITE);assertPermissionGranted(CLARK, "/dir1/file1", READ);assertPermissionDenied(DIANA, "/dir1/file1", READ);assertPermissionDenied(DIANA, "/dir1/file1", WRITE);assertPermissionDenied(DIANA, "/dir1/file1", EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", READ_WRITE);assertPermissionDenied(DIANA, "/dir1/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/dir1/file1", ALL);}
public void testAclNamedGroupMask() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "execs", (short) 0644);addAcl(inodeFile, aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, GROUP, "sales", READ_WRITE), aclEntry(ACCESS, MASK, READ), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", READ_WRITE);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionGranted(DIANA, "/file1", READ);assertPermissionDenied(DIANA, "/file1", WRITE);assertPermissionDenied(DIANA, "/file1", EXECUTE);assertPermissionDenied(DIANA, "/file1", READ_WRITE);assertPermissionDenied(DIANA, "/file1", READ_EXECUTE);assertPermissionDenied(DIANA, "/file1", WRITE_EXECUTE);assertPermissionDenied(DIANA, "/file1", ALL);}
public void testAclOther() {INodeFile inodeFile = createINodeFile(inodeRoot, "file1", "bruce", "sales", (short) 0774);addAcl(inodeFile, aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, "diana", ALL), aclEntry(ACCESS, GROUP, READ_WRITE), aclEntry(ACCESS, MASK, ALL), aclEntry(ACCESS, OTHER, READ));assertPermissionGranted(BRUCE, "/file1", ALL);assertPermissionGranted(DIANA, "/file1", ALL);assertPermissionGranted(CLARK, "/file1", READ);assertPermissionDenied(CLARK, "/file1", WRITE);assertPermissionDenied(CLARK, "/file1", EXECUTE);assertPermissionDenied(CLARK, "/file1", READ_WRITE);assertPermissionDenied(CLARK, "/file1", READ_EXECUTE);assertPermissionDenied(CLARK, "/file1", WRITE_EXECUTE);assertPermissionDenied(CLARK, "/file1", ALL);}
private void addAcl(INodeWithAdditionalFields inode, AclEntry... acl) {AclStorage.updateINodeAcl(inode, Arrays.asList(acl), Snapshot.CURRENT_STATE_ID);}
private void assertPermissionGranted(UserGroupInformation user, String path, FsAction access) {new FSPermissionChecker(SUPERUSER, SUPERGROUP, user).checkPermission(path, dir, false, null, null, access, null, false, true);}
private void assertPermissionDenied(UserGroupInformation user, String path, FsAction access) {try {new FSPermissionChecker(SUPERUSER, SUPERGROUP, user).checkPermission(path, dir, false, null, null, access, null, false, true);fail("expected AccessControlException for user + " + user + ", path = " + path + ", access = " + access);} catch (AccessControlException e) {}}
private static INodeDirectory createINodeDirectory(INodeDirectory parent, String name, String owner, String group, short perm) {PermissionStatus permStatus = PermissionStatus.createImmutable(owner, group, FsPermission.createImmutable(perm));INodeDirectory inodeDirectory = new INodeDirectory(INodeId.GRANDFATHER_INODE_ID, name.getBytes("UTF-8"), permStatus, 0L);parent.addChild(inodeDirectory);return inodeDirectory;}
private static INodeFile createINodeFile(INodeDirectory parent, String name, String owner, String group, short perm) {PermissionStatus permStatus = PermissionStatus.createImmutable(owner, group, FsPermission.createImmutable(perm));INodeFile inodeFile = new INodeFile(INodeId.GRANDFATHER_INODE_ID, name.getBytes("UTF-8"), permStatus, 0L, 0L, null, REPLICATION, PREFERRED_BLOCK_SIZE);parent.addChild(inodeFile);return inodeFile;}public void testAbsoluteMaxAvailCapacityInvalidDivisor() {runInvalidDivisorTest(false);runInvalidDivisorTest(true);}
public void runInvalidDivisorTest(boolean useDominant) {ResourceCalculator resourceCalculator;Resource clusterResource;if (useDominant) {resourceCalculator = new DominantResourceCalculator();clusterResource = Resources.createResource(10, 0);} else {resourceCalculator = new DefaultResourceCalculator();clusterResource = Resources.createResource(0, 99);}YarnConfiguration conf = new YarnConfiguration();CapacitySchedulerConfiguration csConf = new CapacitySchedulerConfiguration();CapacitySchedulerContext csContext = mock(CapacitySchedulerContext.class);when(csContext.getConf()).thenReturn(conf);when(csContext.getConfiguration()).thenReturn(csConf);when(csContext.getClusterResource()).thenReturn(clusterResource);when(csContext.getResourceCalculator()).thenReturn(resourceCalculator);when(csContext.getMinimumResourceCapability()).thenReturn(Resources.createResource(GB, 1));when(csContext.getMaximumResourceCapability()).thenReturn(Resources.createResource(0, 0));final String L1Q1 = "L1Q1";csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { L1Q1 });final String L1Q1P = CapacitySchedulerConfiguration.ROOT + "." + L1Q1;csConf.setCapacity(L1Q1P, 90);csConf.setMaximumCapacity(L1Q1P, 90);ParentQueue root = new ParentQueue(csContext, CapacitySchedulerConfiguration.ROOT, null, null);LeafQueue l1q1 = new LeafQueue(csContext, L1Q1, root, null);LOG.info("t1 root " + CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, root));LOG.info("t1 l1q1 " + CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l1q1));assertEquals(0.0f, CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l1q1), 0.000001f);}
public void testAbsoluteMaxAvailCapacityNoUse() {ResourceCalculator resourceCalculator = new DefaultResourceCalculator();Resource clusterResource = Resources.createResource(100 * 16 * GB, 100 * 32);YarnConfiguration conf = new YarnConfiguration();CapacitySchedulerConfiguration csConf = new CapacitySchedulerConfiguration();CapacitySchedulerContext csContext = mock(CapacitySchedulerContext.class);when(csContext.getConf()).thenReturn(conf);when(csContext.getConfiguration()).thenReturn(csConf);when(csContext.getClusterResource()).thenReturn(clusterResource);when(csContext.getResourceCalculator()).thenReturn(resourceCalculator);when(csContext.getMinimumResourceCapability()).thenReturn(Resources.createResource(GB, 1));when(csContext.getMaximumResourceCapability()).thenReturn(Resources.createResource(16 * GB, 32));final String L1Q1 = "L1Q1";csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { L1Q1 });final String L1Q1P = CapacitySchedulerConfiguration.ROOT + "." + L1Q1;csConf.setCapacity(L1Q1P, 90);csConf.setMaximumCapacity(L1Q1P, 90);ParentQueue root = new ParentQueue(csContext, CapacitySchedulerConfiguration.ROOT, null, null);LeafQueue l1q1 = new LeafQueue(csContext, L1Q1, root, null);LOG.info("t1 root " + CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, root));LOG.info("t1 l1q1 " + CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l1q1));assertEquals(1.0f, CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, root), 0.000001f);assertEquals(0.9f, CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l1q1), 0.000001f);}
public void testAbsoluteMaxAvailCapacityWithUse() {ResourceCalculator resourceCalculator = new DefaultResourceCalculator();Resource clusterResource = Resources.createResource(100 * 16 * GB, 100 * 32);YarnConfiguration conf = new YarnConfiguration();CapacitySchedulerConfiguration csConf = new CapacitySchedulerConfiguration();CapacitySchedulerContext csContext = mock(CapacitySchedulerContext.class);when(csContext.getConf()).thenReturn(conf);when(csContext.getConfiguration()).thenReturn(csConf);when(csContext.getClusterResource()).thenReturn(clusterResource);when(csContext.getResourceCalculator()).thenReturn(resourceCalculator);when(csContext.getMinimumResourceCapability()).thenReturn(Resources.createResource(GB, 1));when(csContext.getMaximumResourceCapability()).thenReturn(Resources.createResource(16 * GB, 32));final String L1Q1 = "L1Q1";final String L1Q2 = "L1Q2";final String L2Q1 = "L2Q1";final String L2Q2 = "L2Q2";csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { L1Q1, L1Q2, L2Q1, L2Q2 });final String L1Q1P = CapacitySchedulerConfiguration.ROOT + "." + L1Q1;csConf.setCapacity(L1Q1P, 80);csConf.setMaximumCapacity(L1Q1P, 80);final String L1Q2P = CapacitySchedulerConfiguration.ROOT + "." + L1Q2;csConf.setCapacity(L1Q2P, 20);csConf.setMaximumCapacity(L1Q2P, 100);final String L2Q1P = L1Q1P + "." + L2Q1;csConf.setCapacity(L2Q1P, 50);csConf.setMaximumCapacity(L2Q1P, 50);final String L2Q2P = L1Q1P + "." + L2Q2;csConf.setCapacity(L2Q2P, 50);csConf.setMaximumCapacity(L2Q2P, 50);float result;ParentQueue root = new ParentQueue(csContext, CapacitySchedulerConfiguration.ROOT, null, null);LeafQueue l1q1 = new LeafQueue(csContext, L1Q1, root, null);LeafQueue l1q2 = new LeafQueue(csContext, L1Q2, root, null);LeafQueue l2q2 = new LeafQueue(csContext, L2Q2, l1q1, null);LeafQueue l2q1 = new LeafQueue(csContext, L2Q1, l1q1, null);//no usage, all based on maxCapacity (prior behavior)result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.4f, result, 0.000001f);LOG.info("t2 l2q2 " + result);//some usage, but below the base capacityResources.addTo(root.getUsedResources(), Resources.multiply(clusterResource, 0.1f));Resources.addTo(l1q2.getUsedResources(), Resources.multiply(clusterResource, 0.1f));result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.4f, result, 0.000001f);LOG.info("t2 l2q2 " + result);//usage gt base on parent siblingResources.addTo(root.getUsedResources(), Resources.multiply(clusterResource, 0.3f));Resources.addTo(l1q2.getUsedResources(), Resources.multiply(clusterResource, 0.3f));result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.3f, result, 0.000001f);LOG.info("t2 l2q2 " + result);//same as last, but with usage also on direct parentResources.addTo(root.getUsedResources(), Resources.multiply(clusterResource, 0.1f));Resources.addTo(l1q1.getUsedResources(), Resources.multiply(clusterResource, 0.1f));result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.3f, result, 0.000001f);LOG.info("t2 l2q2 " + result);//add to direct sibling, below the threshold of effect at presentResources.addTo(root.getUsedResources(), Resources.multiply(clusterResource, 0.2f));Resources.addTo(l1q1.getUsedResources(), Resources.multiply(clusterResource, 0.2f));Resources.addTo(l2q1.getUsedResources(), Resources.multiply(clusterResource, 0.2f));result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.3f, result, 0.000001f);LOG.info("t2 l2q2 " + result);//(it's cumulative with prior tests)Resources.addTo(root.getUsedResources(), Resources.multiply(clusterResource, 0.2f));Resources.addTo(l1q1.getUsedResources(), Resources.multiply(clusterResource, 0.2f));Resources.addTo(l2q1.getUsedResources(), Resources.multiply(clusterResource, 0.2f));result = CSQueueUtils.getAbsoluteMaxAvailCapacity(resourceCalculator, clusterResource, l2q2);assertEquals(0.1f, result, 0.000001f);LOG.info("t2 l2q2 " + result);}public void setUp() {faultInjector = Mockito.mock(DFSClientFaultInjector.class);DFSClientFaultInjector.instance = faultInjector;}
public void testCorruptionDuringWrt() {Configuration conf = new HdfsConfiguration();// Set short retry timeouts so this test runs fasterconf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(10).build();cluster.waitActive();FileSystem fs = cluster.getFileSystem();Path file = new Path("/test_corruption_file");FSDataOutputStream out = fs.create(file, true, 8192, (short) 3, (long) (128 * 1024 * 1024));byte[] data = new byte[65536];for (int i = 0; i < 65536; i++) {data[i] = (byte) (i % 256);}for (int i = 0; i < 5; i++) {out.write(data, 0, 65535);}out.hflush();// corrupt the packet onceMockito.when(faultInjector.corruptPacket()).thenReturn(true, false);Mockito.when(faultInjector.uncorruptPacket()).thenReturn(true, false);for (int i = 0; i < 5; i++) {out.write(data, 0, 65535);}out.close();// read should succeedFSDataInputStream in = fs.open(file);for (int c; (c = in.read()) != -1; ) ;in.close();// test the retry limitout = fs.create(file, true, 8192, (short) 3, (long) (128 * 1024 * 1024));// corrupt the packet once and never fix it.Mockito.when(faultInjector.corruptPacket()).thenReturn(true, false);Mockito.when(faultInjector.uncorruptPacket()).thenReturn(false);// the client should give up pipeline reconstruction after retries.try {for (int i = 0; i < 5; i++) {out.write(data, 0, 65535);}out.close();fail("Write did not fail");} catch (IOException ioe) {DFSClient.LOG.info("Got expected exception", ioe);}} finally {if (cluster != null) {cluster.shutdown();}Mockito.when(faultInjector.corruptPacket()).thenReturn(false);Mockito.when(faultInjector.uncorruptPacket()).thenReturn(false);}}
private void thistest(Configuration conf, DFSTestUtil util) {MiniDFSCluster cluster = null;int numDataNodes = 2;short replFactor = 2;Random random = new Random();// Set short retry timeouts so this test runs fasterconf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fs = cluster.getFileSystem();util.createFiles(fs, "/srcdat", replFactor);util.waitReplication(fs, "/srcdat", (short) 2);File storageDir = cluster.getInstanceStorageDir(0, 1);String bpid = cluster.getNamesystem().getBlockPoolId();File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);assertTrue("data directory does not exist", data_dir.exists());File[] blocks = data_dir.listFiles();assertTrue("Blocks do not exist in data-dir", (blocks != null) && (blocks.length > 0));int num = 0;for (int idx = 0; idx < blocks.length; idx++) {if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {num++;if (num % 3 == 0) {//System.out.println("Deliberately removing file " + blocks[idx].getName());assertTrue("Cannot remove file.", blocks[idx].delete());} else if (num % 3 == 1) {//RandomAccessFile file = new RandomAccessFile(blocks[idx], "rw");FileChannel channel = file.getChannel();int newsize = random.nextInt((int) channel.size() / 2);System.out.println("Deliberately truncating file " + blocks[idx].getName() + " to size " + newsize + " bytes.");channel.truncate(newsize);file.close();} else {RandomAccessFile file = new RandomAccessFile(blocks[idx], "rw");FileChannel channel = file.getChannel();long position = 0;if (num != 2) {position = (long) random.nextInt((int) channel.size());}int length = random.nextInt((int) (channel.size() - position + 1));byte[] buffer = new byte[length];random.nextBytes(buffer);channel.write(ByteBuffer.wrap(buffer), position);System.out.println("Deliberately corrupting file " + blocks[idx].getName() + " at offset " + position + " length " + length);file.close();}}}//storageDir = cluster.getInstanceStorageDir(0, 1);data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);assertTrue("data directory does not exist", data_dir.exists());blocks = data_dir.listFiles();assertTrue("Blocks do not exist in data-dir", (blocks != null) && (blocks.length > 0));int count = 0;File previous = null;for (int idx = 0; idx < blocks.length; idx++) {if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {//count++;if (count % 2 == 0) {System.out.println("Deliberately insertimg bad crc into files " + blocks[idx].getName() + " " + previous.getName());assertTrue("Cannot remove file.", blocks[idx].delete());assertTrue("Cannot corrupt meta file.", previous.renameTo(blocks[idx]));assertTrue("Cannot recreate empty meta file.", previous.createNewFile());previous = null;} else {previous = blocks[idx];}}}//assertTrue("Corrupted replicas not handled properly.", util.checkFiles(fs, "/srcdat"));System.out.println("All File still have a valid replica");//util.setReplication(fs, "/srcdat", (short) 1);//   util.checkFiles(fs, "/srcdat"));System.out.println("The excess-corrupted-replica test is disabled " + " pending HADOOP-1557");util.cleanup(fs, "/srcdat");} finally {if (cluster != null) {cluster.shutdown();}}}
public void testCrcCorruption() {//System.out.println("TestCrcCorruption with default parameters");Configuration conf1 = new HdfsConfiguration();conf1.setInt(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 3 * 1000);DFSTestUtil util1 = new DFSTestUtil.Builder().setName("TestCrcCorruption").setNumFiles(40).build();thistest(conf1, util1);//System.out.println("TestCrcCorruption with specific parameters");Configuration conf2 = new HdfsConfiguration();conf2.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, 17);conf2.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 34);DFSTestUtil util2 = new DFSTestUtil.Builder().setName("TestCrcCorruption").setNumFiles(40).setMaxSize(400).build();thistest(conf2, util2);}
public void testEntirelyCorruptFileOneNode() {doTestEntirelyCorruptFile(1);}
public void testEntirelyCorruptFileThreeNodes() {doTestEntirelyCorruptFile(3);}
private void doTestEntirelyCorruptFile(int numDataNodes) {long fileSize = 4096;Path file = new Path("/testFile");short replFactor = (short) numDataNodes;Configuration conf = new Configuration();conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, numDataNodes);// Set short retry timeouts so this test runs fasterconf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();try {cluster.waitActive();FileSystem fs = cluster.getFileSystem();DFSTestUtil.createFile(fs, file, fileSize, replFactor, 12345L);DFSTestUtil.waitReplication(fs, file, replFactor);ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, file);int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);assertEquals("All replicas not corrupted", replFactor, blockFilesCorrupted);try {IOUtils.copyBytes(fs.open(file), new IOUtils.NullOutputStream(), conf, true);fail("Didn't get exception");} catch (IOException ioe) {DFSClient.LOG.info("Got expected exception", ioe);}} finally {cluster.shutdown();}}protected void configureServlets() {bind(JAXBContextResolver.class);bind(RMWebServices.class);bind(GenericExceptionHandler.class);csConf = new CapacitySchedulerConfiguration();setupQueueConfiguration(csConf);conf = new YarnConfiguration(csConf);conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class, ResourceScheduler.class);rm = new MockRM(conf);bind(ResourceManager.class).toInstance(rm);bind(RMContext.class).toInstance(rm.getRMContext());bind(ApplicationACLsManager.class).toInstance(rm.getApplicationACLsManager());bind(QueueACLsManager.class).toInstance(rm.getQueueACLsManager());serve("/*").with(GuiceContainer.class);}
protected Injector getInjector() {return injector;}
private static void setupQueueConfiguration(CapacitySchedulerConfiguration conf) {// Define top-level queuesconf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { "a", "b" });final String A = CapacitySchedulerConfiguration.ROOT + ".a";conf.setCapacity(A, 10.5f);conf.setMaximumCapacity(A, 50);final String B = CapacitySchedulerConfiguration.ROOT + ".b";conf.setCapacity(B, 89.5f);// Define 2nd-level queuesfinal String A1 = A + ".a1";final String A2 = A + ".a2";conf.setQueues(A, new String[] { "a1", "a2" });conf.setCapacity(A1, 30);conf.setMaximumCapacity(A1, 50);conf.setUserLimitFactor(A1, 100.0f);conf.setCapacity(A2, 70);conf.setUserLimitFactor(A2, 100.0f);final String B1 = B + ".b1";final String B2 = B + ".b2";final String B3 = B + ".b3";conf.setQueues(B, new String[] { "b1", "b2", "b3" });conf.setCapacity(B1, 60);conf.setUserLimitFactor(B1, 100.0f);conf.setCapacity(B2, 39.5f);conf.setUserLimitFactor(B2, 100.0f);conf.setCapacity(B3, 0.5f);conf.setUserLimitFactor(B3, 100.0f);conf.setQueues(A1, new String[] { "a1a", "a1b" });final String A1A = A1 + ".a1a";conf.setCapacity(A1A, 85);final String A1B = A1 + ".a1b";conf.setCapacity(A1B, 15);}
public void setUp() {super.setUp();}
public void testClusterScheduler() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterScheduler(json);}
public void testClusterSchedulerSlash() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterScheduler(json);}
public void testClusterSchedulerDefault() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterScheduler(json);}
public void testClusterSchedulerXML() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler/").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_XML_TYPE, response.getType());String xml = response.getEntity(String.class);DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();DocumentBuilder db = dbf.newDocumentBuilder();InputSource is = new InputSource();is.setCharacterStream(new StringReader(xml));Document dom = db.parse(is);NodeList scheduler = dom.getElementsByTagName("scheduler");assertEquals("incorrect number of elements", 1, scheduler.getLength());NodeList schedulerInfo = dom.getElementsByTagName("schedulerInfo");assertEquals("incorrect number of elements", 1, schedulerInfo.getLength());verifyClusterSchedulerXML(schedulerInfo);}
public void verifyClusterSchedulerXML(NodeList nodes) {for (int i = 0; i < nodes.getLength(); i++) {Element element = (Element) nodes.item(i);verifyClusterSchedulerGeneric(WebServicesTestUtils.getXmlAttrString(element, "xsi:type"), WebServicesTestUtils.getXmlFloat(element, "usedCapacity"), WebServicesTestUtils.getXmlFloat(element, "capacity"), WebServicesTestUtils.getXmlFloat(element, "maxCapacity"), WebServicesTestUtils.getXmlString(element, "queueName"));NodeList children = element.getChildNodes();for (int j = 0; j < children.getLength(); j++) {Element qElem = (Element) children.item(j);if (qElem.getTagName().equals("queues")) {NodeList qListInfos = qElem.getChildNodes();for (int k = 0; k < qListInfos.getLength(); k++) {Element qElem2 = (Element) qListInfos.item(k);String qName2 = WebServicesTestUtils.getXmlString(qElem2, "queueName");String q2 = CapacitySchedulerConfiguration.ROOT + "." + qName2;verifySubQueueXML(qElem2, q2, 100, 100);}}}}}
public void verifySubQueueXML(Element qElem, String q, float parentAbsCapacity, float parentAbsMaxCapacity) {NodeList children = qElem.getChildNodes();boolean hasSubQueues = false;for (int j = 0; j < children.getLength(); j++) {Element qElem2 = (Element) children.item(j);if (qElem2.getTagName().equals("queues")) {NodeList qListInfos = qElem2.getChildNodes();if (qListInfos.getLength() > 0) {hasSubQueues = true;}}}QueueInfo qi = (hasSubQueues) ? new QueueInfo() : new LeafQueueInfo();qi.capacity = WebServicesTestUtils.getXmlFloat(qElem, "capacity");qi.usedCapacity = WebServicesTestUtils.getXmlFloat(qElem, "usedCapacity");qi.maxCapacity = WebServicesTestUtils.getXmlFloat(qElem, "maxCapacity");qi.absoluteCapacity = WebServicesTestUtils.getXmlFloat(qElem, "absoluteCapacity");qi.absoluteMaxCapacity = WebServicesTestUtils.getXmlFloat(qElem, "absoluteMaxCapacity");qi.absoluteUsedCapacity = WebServicesTestUtils.getXmlFloat(qElem, "absoluteUsedCapacity");qi.numApplications = WebServicesTestUtils.getXmlInt(qElem, "numApplications");qi.queueName = WebServicesTestUtils.getXmlString(qElem, "queueName");qi.state = WebServicesTestUtils.getXmlString(qElem, "state");verifySubQueueGeneric(q, qi, parentAbsCapacity, parentAbsMaxCapacity);if (hasSubQueues) {for (int j = 0; j < children.getLength(); j++) {Element qElem2 = (Element) children.item(j);if (qElem2.getTagName().equals("queues")) {NodeList qListInfos = qElem2.getChildNodes();for (int k = 0; k < qListInfos.getLength(); k++) {Element qElem3 = (Element) qListInfos.item(k);String qName3 = WebServicesTestUtils.getXmlString(qElem3, "queueName");String q3 = q + "." + qName3;verifySubQueueXML(qElem3, q3, qi.absoluteCapacity, qi.absoluteMaxCapacity);}}}} else {LeafQueueInfo lqi = (LeafQueueInfo) qi;lqi.numActiveApplications = WebServicesTestUtils.getXmlInt(qElem, "numActiveApplications");lqi.numPendingApplications = WebServicesTestUtils.getXmlInt(qElem, "numPendingApplications");lqi.numContainers = WebServicesTestUtils.getXmlInt(qElem, "numContainers");lqi.maxApplications = WebServicesTestUtils.getXmlInt(qElem, "maxApplications");lqi.maxApplicationsPerUser = WebServicesTestUtils.getXmlInt(qElem, "maxApplicationsPerUser");lqi.maxActiveApplications = WebServicesTestUtils.getXmlInt(qElem, "maxActiveApplications");lqi.maxActiveApplicationsPerUser = WebServicesTestUtils.getXmlInt(qElem, "maxActiveApplicationsPerUser");lqi.userLimit = WebServicesTestUtils.getXmlInt(qElem, "userLimit");lqi.userLimitFactor = WebServicesTestUtils.getXmlFloat(qElem, "userLimitFactor");verifyLeafQueueGeneric(q, lqi);}}
private void verifyClusterScheduler(JSONObject json) {assertEquals("incorrect number of elements", 1, json.length());JSONObject info = json.getJSONObject("scheduler");assertEquals("incorrect number of elements", 1, info.length());info = info.getJSONObject("schedulerInfo");assertEquals("incorrect number of elements", 6, info.length());verifyClusterSchedulerGeneric(info.getString("type"), (float) info.getDouble("usedCapacity"), (float) info.getDouble("capacity"), (float) info.getDouble("maxCapacity"), info.getString("queueName"));JSONArray arr = info.getJSONObject("queues").getJSONArray("queue");assertEquals("incorrect number of elements", 2, arr.length());// test subqueuesfor (int i = 0; i < arr.length(); i++) {JSONObject obj = arr.getJSONObject(i);String q = CapacitySchedulerConfiguration.ROOT + "." + obj.getString("queueName");verifySubQueue(obj, q, 100, 100);}}
private void verifyClusterSchedulerGeneric(String type, float usedCapacity, float capacity, float maxCapacity, String queueName) {assertTrue("type doesn't match", "capacityScheduler".matches(type));assertEquals("usedCapacity doesn't match", 0, usedCapacity, 1e-3f);assertEquals("capacity doesn't match", 100, capacity, 1e-3f);assertEquals("maxCapacity doesn't match", 100, maxCapacity, 1e-3f);assertTrue("queueName doesn't match", "root".matches(queueName));}
private void verifySubQueue(JSONObject info, String q, float parentAbsCapacity, float parentAbsMaxCapacity) {int numExpectedElements = 11;boolean isParentQueue = true;if (!info.has("queues")) {numExpectedElements = 21;isParentQueue = false;}assertEquals("incorrect number of elements", numExpectedElements, info.length());QueueInfo qi = isParentQueue ? new QueueInfo() : new LeafQueueInfo();qi.capacity = (float) info.getDouble("capacity");qi.usedCapacity = (float) info.getDouble("usedCapacity");qi.maxCapacity = (float) info.getDouble("maxCapacity");qi.absoluteCapacity = (float) info.getDouble("absoluteCapacity");qi.absoluteMaxCapacity = (float) info.getDouble("absoluteMaxCapacity");qi.absoluteUsedCapacity = (float) info.getDouble("absoluteUsedCapacity");qi.numApplications = info.getInt("numApplications");qi.queueName = info.getString("queueName");qi.state = info.getString("state");verifySubQueueGeneric(q, qi, parentAbsCapacity, parentAbsMaxCapacity);if (isParentQueue) {JSONArray arr = info.getJSONObject("queues").getJSONArray("queue");// test subqueuesfor (int i = 0; i < arr.length(); i++) {JSONObject obj = arr.getJSONObject(i);String q2 = q + "." + obj.getString("queueName");verifySubQueue(obj, q2, qi.absoluteCapacity, qi.absoluteMaxCapacity);}} else {LeafQueueInfo lqi = (LeafQueueInfo) qi;lqi.numActiveApplications = info.getInt("numActiveApplications");lqi.numPendingApplications = info.getInt("numPendingApplications");lqi.numContainers = info.getInt("numContainers");lqi.maxApplications = info.getInt("maxApplications");lqi.maxApplicationsPerUser = info.getInt("maxApplicationsPerUser");lqi.maxActiveApplications = info.getInt("maxActiveApplications");lqi.maxActiveApplicationsPerUser = info.getInt("maxActiveApplicationsPerUser");lqi.userLimit = info.getInt("userLimit");lqi.userLimitFactor = (float) info.getDouble("userLimitFactor");verifyLeafQueueGeneric(q, lqi);}}
private void verifySubQueueGeneric(String q, QueueInfo info, float parentAbsCapacity, float parentAbsMaxCapacity) {String[] qArr = q.split("\\.");assertTrue("q name invalid: " + q, qArr.length > 1);String qshortName = qArr[qArr.length - 1];assertEquals("usedCapacity doesn't match", 0, info.usedCapacity, 1e-3f);assertEquals("capacity doesn't match", csConf.getCapacity(q), info.capacity, 1e-3f);float expectCapacity = csConf.getMaximumCapacity(q);float expectAbsMaxCapacity = parentAbsMaxCapacity * (info.maxCapacity / 100);if (CapacitySchedulerConfiguration.UNDEFINED == expectCapacity) {expectCapacity = 100;expectAbsMaxCapacity = 100;}assertEquals("maxCapacity doesn't match", expectCapacity, info.maxCapacity, 1e-3f);assertEquals("absoluteCapacity doesn't match", parentAbsCapacity * (info.capacity / 100), info.absoluteCapacity, 1e-3f);assertEquals("absoluteMaxCapacity doesn't match", expectAbsMaxCapacity, info.absoluteMaxCapacity, 1e-3f);assertEquals("absoluteUsedCapacity doesn't match", 0, info.absoluteUsedCapacity, 1e-3f);assertEquals("numApplications doesn't match", 0, info.numApplications);assertTrue("queueName doesn't match, got: " + info.queueName + " expected: " + q, qshortName.matches(info.queueName));assertTrue("state doesn't match", (csConf.getState(q).toString()).matches(info.state));}
private void verifyLeafQueueGeneric(String q, LeafQueueInfo info) {assertEquals("numActiveApplications doesn't match", 0, info.numActiveApplications);assertEquals("numPendingApplications doesn't match", 0, info.numPendingApplications);assertEquals("numContainers doesn't match", 0, info.numContainers);int maxSystemApps = csConf.getMaximumSystemApplications();int expectedMaxApps = (int) (maxSystemApps * (info.absoluteCapacity / 100));int expectedMaxAppsPerUser = (int) (expectedMaxApps * (info.userLimit / 100.0f) * info.userLimitFactor);//   roundoff errors in absolute capacity calculationsassertEquals("maxApplications doesn't match", (float) expectedMaxApps, (float) info.maxApplications, 1.0f);assertEquals("maxApplicationsPerUser doesn't match", (float) expectedMaxAppsPerUser, (float) info.maxApplicationsPerUser, info.userLimitFactor);assertTrue("maxActiveApplications doesn't match", info.maxActiveApplications > 0);assertTrue("maxActiveApplicationsPerUser doesn't match", info.maxActiveApplicationsPerUser > 0);assertEquals("userLimit doesn't match", csConf.getUserLimit(q), info.userLimit);assertEquals("userLimitFactor doesn't match", csConf.getUserLimitFactor(q), info.userLimitFactor, 1e-3f);}
private Node getChildNodeByName(Node node, String tagname) {NodeList nodeList = node.getChildNodes();for (int i = 0; i < nodeList.getLength(); ++i) {if (nodeList.item(i).getNodeName().equals(tagname)) {return nodeList.item(i);}}return null;}
public void testPerUserResourcesXML() {//Start RM so that it accepts app submissionsrm.start();try {rm.submitApp(10, "app1", "user1", null, "b1");rm.submitApp(20, "app2", "user2", null, "b1");//Get the XML from ws/v1/cluster/schedulerWebResource r = resource();ClientResponse response = r.path("ws/v1/cluster/scheduler").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_XML_TYPE, response.getType());String xml = response.getEntity(String.class);DocumentBuilder db = DocumentBuilderFactory.newInstance().newDocumentBuilder();InputSource is = new InputSource();is.setCharacterStream(new StringReader(xml));//Parse the XML we gotDocument dom = db.parse(is);//Get all users elements (1 for each leaf queue)NodeList allUsers = dom.getElementsByTagName("users");for (int i = 0; i < allUsers.getLength(); ++i) {Node perUserResources = allUsers.item(i);String queueName = getChildNodeByName(perUserResources.getParentNode(), "queueName").getTextContent();if (queueName.equals("b1")) {//b1 should have two users (user1 and user2) which submitted jobsassertEquals(2, perUserResources.getChildNodes().getLength());NodeList users = perUserResources.getChildNodes();for (int j = 0; j < users.getLength(); ++j) {Node user = users.item(j);String username = getChildNodeByName(user, "username").getTextContent();assertTrue(username.equals("user1") || username.equals("user2"));//Should be a parsable integerInteger.parseInt(getChildNodeByName(getChildNodeByName(user, "resourcesUsed"), "memory").getTextContent());Integer.parseInt(getChildNodeByName(user, "numActiveApplications").getTextContent());Integer.parseInt(getChildNodeByName(user, "numPendingApplications").getTextContent());}} else {//Queues other than b1 should have 0 usersassertEquals(0, perUserResources.getChildNodes().getLength());}}NodeList allResourcesUsed = dom.getElementsByTagName("resourcesUsed");for (int i = 0; i < allResourcesUsed.getLength(); ++i) {Node resourcesUsed = allResourcesUsed.item(i);Integer.parseInt(getChildNodeByName(resourcesUsed, "memory").getTextContent());Integer.parseInt(getChildNodeByName(resourcesUsed, "vCores").getTextContent());}} finally {rm.stop();}}
private void checkResourcesUsed(JSONObject queue) {queue.getJSONObject("resourcesUsed").getInt("memory");queue.getJSONObject("resourcesUsed").getInt("vCores");}
private JSONObject getSubQueue(JSONObject queue, String subQueue) {JSONArray queues = queue.getJSONObject("queues").getJSONArray("queue");for (int i = 0; i < queues.length(); ++i) {checkResourcesUsed(queues.getJSONObject(i));if (queues.getJSONObject(i).getString("queueName").equals(subQueue)) {return queues.getJSONObject(i);}}return null;}
public void testPerUserResourcesJSON() {//Start RM so that it accepts app submissionsrm.start();try {rm.submitApp(10, "app1", "user1", null, "b1");rm.submitApp(20, "app2", "user2", null, "b1");//Get JSONWebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);JSONObject schedulerInfo = json.getJSONObject("scheduler").getJSONObject("schedulerInfo");JSONObject b1 = getSubQueue(getSubQueue(schedulerInfo, "b"), "b1");//Check users user1 and user2 exist in b1JSONArray users = b1.getJSONObject("users").getJSONArray("user");for (int i = 0; i < 2; ++i) {JSONObject user = users.getJSONObject(i);assertTrue("User isn't user1 or user2", user.getString("username").equals("user1") || user.getString("username").equals("user2"));user.getInt("numActiveApplications");user.getInt("numPendingApplications");checkResourcesUsed(user);}} finally {rm.stop();}}
public void testResourceInfo() {Resource res = Resources.createResource(10, 1);// eg. ResourceInfoassertEquals("<memory:10, vCores:1>", res.toString());}public static void setNodeGroups(String[] nodeGroups) {NODE_GROUPS = nodeGroups;}
public synchronized void startDataNodes(Configuration conf, int numDataNodes, StorageType[][] storageTypes, boolean manageDfsDirs, StartupOption operation, String[] racks, String[] nodeGroups, String[] hosts, long[][] storageCapacities, long[] simulatedCapacities, boolean setupHostsFile, boolean checkDataNodeAddrConfig, boolean checkDataNodeHostConfig) {assert storageCapacities == null || simulatedCapacities == null;assert storageTypes == null || storageTypes.length == numDataNodes;assert storageCapacities == null || storageCapacities.length == numDataNodes;if (operation == StartupOption.RECOVER) {return;}if (checkDataNodeHostConfig) {conf.setIfUnset(DFS_DATANODE_HOST_NAME_KEY, "127.0.0.1");} else {conf.set(DFS_DATANODE_HOST_NAME_KEY, "127.0.0.1");}conf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, "127.0.0.1");int curDatanodesNum = dataNodes.size();if (conf.get(DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY) == null) {conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY, 0);}if (racks != null && numDataNodes > racks.length) {throw new IllegalArgumentException("The length of racks [" + racks.length + "] is less than the number of datanodes [" + numDataNodes + "].");}if (nodeGroups != null && numDataNodes > nodeGroups.length) {throw new IllegalArgumentException("The length of nodeGroups [" + nodeGroups.length + "] is less than the number of datanodes [" + numDataNodes + "].");}if (hosts != null && numDataNodes > hosts.length) {throw new IllegalArgumentException("The length of hosts [" + hosts.length + "] is less than the number of datanodes [" + numDataNodes + "].");}if (racks != null && hosts == null) {hosts = new String[numDataNodes];for (int i = curDatanodesNum; i < curDatanodesNum + numDataNodes; i++) {hosts[i - curDatanodesNum] = "host" + i + ".foo.com";}}if (simulatedCapacities != null && numDataNodes > simulatedCapacities.length) {throw new IllegalArgumentException("The length of simulatedCapacities [" + simulatedCapacities.length + "] is less than the number of datanodes [" + numDataNodes + "].");}String[] dnArgs = (operation == null || operation != StartupOption.ROLLBACK) ? null : new String[] { operation.getName() };DataNode[] dns = new DataNode[numDataNodes];for (int i = curDatanodesNum; i < curDatanodesNum + numDataNodes; i++) {Configuration dnConf = new HdfsConfiguration(conf);// Set up datanode addresssetupDatanodeAddress(dnConf, setupHostsFile, checkDataNodeAddrConfig);if (manageDfsDirs) {String dirs = makeDataNodeDirs(i, storageTypes == null ? null : storageTypes[i]);dnConf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);conf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY, dirs);}if (simulatedCapacities != null) {SimulatedFSDataset.setFactory(dnConf);dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY, simulatedCapacities[i - curDatanodesNum]);}LOG.info("Starting DataNode " + i + " with " + DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY + ": " + dnConf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));if (hosts != null) {dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY, hosts[i - curDatanodesNum]);LOG.info("Starting DataNode " + i + " with hostname set to: " + dnConf.get(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY));}if (racks != null) {String name = hosts[i - curDatanodesNum];if (nodeGroups == null) {LOG.info("Adding node with hostname : " + name + " to rack " + racks[i - curDatanodesNum]);StaticMapping.addNodeToRack(name, racks[i - curDatanodesNum]);} else {LOG.info("Adding node with hostname : " + name + " to serverGroup " + nodeGroups[i - curDatanodesNum] + " and rack " + racks[i - curDatanodesNum]);StaticMapping.addNodeToRack(name, racks[i - curDatanodesNum] + nodeGroups[i - curDatanodesNum]);}}// save configConfiguration newconf = new HdfsConfiguration(dnConf);if (hosts != null) {NetUtils.addStaticResolution(hosts[i - curDatanodesNum], "localhost");}SecureResources secureResources = null;if (UserGroupInformation.isSecurityEnabled()) {try {secureResources = SecureDataNodeStarter.getSecureResources(dnConf);} catch (Exception ex) {ex.printStackTrace();}}DataNode dn = DataNode.instantiateDataNode(dnArgs, dnConf, secureResources);if (dn == null)throw new IOException("Cannot start DataNode in " + dnConf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));//for IP:port to rackIdString ipAddr = dn.getXferAddress().getAddress().getHostAddress();if (racks != null) {int port = dn.getXferAddress().getPort();if (nodeGroups == null) {LOG.info("Adding node with IP:port : " + ipAddr + ":" + port + " to rack " + racks[i - curDatanodesNum]);StaticMapping.addNodeToRack(ipAddr + ":" + port, racks[i - curDatanodesNum]);} else {LOG.info("Adding node with IP:port : " + ipAddr + ":" + port + " to nodeGroup " + nodeGroups[i - curDatanodesNum] + " and rack " + racks[i - curDatanodesNum]);StaticMapping.addNodeToRack(ipAddr + ":" + port, racks[i - curDatanodesNum] + nodeGroups[i - curDatanodesNum]);}}dn.runDatanodeDaemon();dataNodes.add(new DataNodeProperties(dn, newconf, dnArgs, secureResources, dn.getIpcPort()));dns[i - curDatanodesNum] = dn;}curDatanodesNum += numDataNodes;this.numDataNodes += numDataNodes;waitActive();if (storageCapacities != null) {for (int i = curDatanodesNum; i < curDatanodesNum + numDataNodes; ++i) {List<? extends FsVolumeSpi> volumes = dns[i].getFSDataset().getVolumes();assert volumes.size() == storagesPerDatanode;for (int j = 0; j < volumes.size(); ++j) {FsVolumeImpl volume = (FsVolumeImpl) volumes.get(j);volume.setCapacityForTesting(storageCapacities[i][j]);}}}}
public synchronized void startDataNodes(Configuration conf, int numDataNodes, boolean manageDfsDirs, StartupOption operation, String[] racks, String[] nodeGroups, String[] hosts, long[] simulatedCapacities, boolean setupHostsFile) {startDataNodes(conf, numDataNodes, null, manageDfsDirs, operation, racks, nodeGroups, hosts, null, simulatedCapacities, setupHostsFile, false, false);}
public void startDataNodes(Configuration conf, int numDataNodes, boolean manageDfsDirs, StartupOption operation, String[] racks, long[] simulatedCapacities, String[] nodeGroups) {startDataNodes(conf, numDataNodes, manageDfsDirs, operation, racks, nodeGroups, null, simulatedCapacities, false);}
public synchronized void startDataNodes(Configuration conf, int numDataNodes, StorageType[][] storageTypes, boolean manageDfsDirs, StartupOption operation, String[] racks, String[] hosts, long[][] storageCapacities, long[] simulatedCapacities, boolean setupHostsFile, boolean checkDataNodeAddrConfig, boolean checkDataNodeHostConfig, Configuration[] dnConfOverlays) {startDataNodes(conf, numDataNodes, storageTypes, manageDfsDirs, operation, racks, NODE_GROUPS, hosts, storageCapacities, simulatedCapacities, setupHostsFile, checkDataNodeAddrConfig, checkDataNodeHostConfig);}private Job runJob(Configuration conf) {String input = "hello1\nhello2\nhello3\n";Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, input);job.setJobName("mr");job.setPriority(JobPriority.NORMAL);job.waitForCompletion(true);return job;}
private Job runJobInBackGround(Configuration conf) {String input = "hello1\nhello2\nhello3\n";Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, input);job.setJobName("mr");job.setPriority(JobPriority.NORMAL);job.submit();int i = 0;while (i++ < 200 && job.getJobID() == null) {LOG.info("waiting for jobId...");Thread.sleep(100);}return job;}
public static int runTool(Configuration conf, Tool tool, String[] args, OutputStream out) {PrintStream oldOut = System.out;PrintStream newOut = new PrintStream(out, true);try {System.setOut(newOut);return ToolRunner.run(conf, tool, args);} finally {System.setOut(oldOut);}}
public void checkOutputSpecs(JobContext job) {throw new IOException();}
public void testJobSubmissionSpecsAndFiles() {Configuration conf = createJobConf();Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1);job.setOutputFormatClass(BadOutputFormat.class);try {job.submit();fail("Should've thrown an exception while checking output specs.");} catch (Exception e) {assertTrue(e instanceof IOException);}Cluster cluster = new Cluster(conf);Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, job.getConfiguration());Path submitJobDir = new Path(jobStagingArea, "JobId");Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);assertFalse("Shouldn't have created a job file if job specs failed.", FileSystem.get(conf).exists(submitJobFile));}
public void testJobClient() {Configuration conf = createJobConf();Job job = runJob(conf);String jobId = job.getJobID().toString();// test all jobs listtestAllJobList(jobId, conf);// test only submitted jobs listtestSubmittedJobList(conf);// test job countertestGetCounter(jobId, conf);// statustestJobStatus(jobId, conf);// test list of eventstestJobEvents(jobId, conf);// test job historytestJobHistory(conf);// test tracker listtestListTrackers(conf);// attempts listtestListAttemptIds(jobId, conf);// black listtestListBlackList(conf);// test method main and help screenstartStop();// test a change job priority .testChangingJobPriority(jobId, conf);// submit job from filetestSubmit(conf);// kill a tasktestKillTask(conf);// fail a tasktestfailTask(conf);// kill jobtestKillJob(conf);}
private void testfailTask(Configuration conf) {Job job = runJobInBackGround(conf);CLI jc = createJobClient();TaskID tid = new TaskID(job.getJobID(), TaskType.MAP, 0);TaskAttemptID taid = new TaskAttemptID(tid, 1);ByteArrayOutputStream out = new ByteArrayOutputStream();// TaskAttemptId is not setint exitCode = runTool(conf, jc, new String[] { "-fail-task" }, out);assertEquals("Exit code", -1, exitCode);runTool(conf, jc, new String[] { "-fail-task", taid.toString() }, out);String answer = new String(out.toByteArray(), "UTF-8");Assert.assertTrue(answer.contains("Killed task " + taid + " by failing it"));}
private void testKillTask(Configuration conf) {Job job = runJobInBackGround(conf);CLI jc = createJobClient();TaskID tid = new TaskID(job.getJobID(), TaskType.MAP, 0);TaskAttemptID taid = new TaskAttemptID(tid, 1);ByteArrayOutputStream out = new ByteArrayOutputStream();// bad parametersint exitCode = runTool(conf, jc, new String[] { "-kill-task" }, out);assertEquals("Exit code", -1, exitCode);runTool(conf, jc, new String[] { "-kill-task", taid.toString() }, out);String answer = new String(out.toByteArray(), "UTF-8");Assert.assertTrue(answer.contains("Killed task " + taid));}
private void testKillJob(Configuration conf) {Job job = runJobInBackGround(conf);String jobId = job.getJobID().toString();CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();// without jobIdint exitCode = runTool(conf, jc, new String[] { "-kill" }, out);assertEquals("Exit code", -1, exitCode);// good parametersexitCode = runTool(conf, jc, new String[] { "-kill", jobId }, out);assertEquals("Exit code", 0, exitCode);String answer = new String(out.toByteArray(), "UTF-8");assertTrue(answer.contains("Killed job " + jobId));}
private void testSubmit(Configuration conf) {CLI jc = createJobClient();Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, "ping");job.setJobName("mr");job.setPriority(JobPriority.NORMAL);File fcon = File.createTempFile("config", ".xml");FileSystem localFs = FileSystem.getLocal(conf);String fconUri = new Path(fcon.getAbsolutePath()).makeQualified(localFs.getUri(), localFs.getWorkingDirectory()).toUri().toString();job.getConfiguration().writeXml(new FileOutputStream(fcon));ByteArrayOutputStream out = new ByteArrayOutputStream();// bad parametersint exitCode = runTool(conf, jc, new String[] { "-submit" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-submit", fconUri }, out);assertEquals("Exit code", 0, exitCode);String answer = new String(out.toByteArray());// in console was writtenassertTrue(answer.contains("Created job "));}
private void startStop() {ByteArrayOutputStream data = new ByteArrayOutputStream();PrintStream error = System.err;System.setErr(new PrintStream(data));ExitUtil.disableSystemExit();try {CLI.main(new String[0]);fail(" CLI.main should call System.exit");} catch (ExitUtil.ExitException e) {ExitUtil.resetFirstExitException();assertEquals(-1, e.status);} catch (Exception e) {} finally {System.setErr(error);}// in console should be written help text String s = new String(data.toByteArray());assertTrue(s.contains("-submit"));assertTrue(s.contains("-status"));assertTrue(s.contains("-kill"));assertTrue(s.contains("-set-priority"));assertTrue(s.contains("-events"));assertTrue(s.contains("-history"));assertTrue(s.contains("-list"));assertTrue(s.contains("-list-active-trackers"));assertTrue(s.contains("-list-blacklisted-trackers"));assertTrue(s.contains("-list-attempt-ids"));assertTrue(s.contains("-kill-task"));assertTrue(s.contains("-fail-task"));assertTrue(s.contains("-logs"));}
private void testListBlackList(Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();int exitCode = runTool(conf, jc, new String[] { "-list-blacklisted-trackers", "second in" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-list-blacklisted-trackers" }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));int counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);counter++;}assertEquals(0, counter);}
private void testListAttemptIds(String jobId, Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();int exitCode = runTool(conf, jc, new String[] { "-list-attempt-ids" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-list-attempt-ids", jobId, "MAP", "completed" }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));int counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);counter++;}assertEquals(1, counter);}
private void testListTrackers(Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();int exitCode = runTool(conf, jc, new String[] { "-list-active-trackers", "second parameter" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-list-active-trackers" }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));int counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);counter++;}assertEquals(2, counter);}
private void testJobHistory(Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();File f = new File("src/test/resources/job_1329348432655_0001-10.jhist");FileSystem localFs = FileSystem.getLocal(conf);String historyFileUri = new Path(f.getAbsolutePath()).makeQualified(localFs.getUri(), localFs.getWorkingDirectory()).toUri().toString();// bad commandint exitCode = runTool(conf, jc, new String[] { "-history", "pul", historyFileUri }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-history", "all", historyFileUri }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));int counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (line.startsWith("task_")) {counter++;}}assertEquals(23, counter);}
private void testJobEvents(String jobId, Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();int exitCode = runTool(conf, jc, new String[] { "-events" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-events", jobId, "0", "100" }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));int counter = 0;String attemptId = ("attempt" + jobId.substring(3));while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (line.contains(attemptId)) {counter++;}}assertEquals(2, counter);}
private void testJobStatus(String jobId, Configuration conf) {CLI jc = createJobClient();ByteArrayOutputStream out = new ByteArrayOutputStream();// bad optionsint exitCode = runTool(conf, jc, new String[] { "-status" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, jc, new String[] { "-status", jobId }, out);assertEquals("Exit code", 0, exitCode);String line;BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (!line.contains("Job state:")) {continue;}break;}assertNotNull(line);assertTrue(line.contains("SUCCEEDED"));}
public void testGetCounter(String jobId, Configuration conf) {ByteArrayOutputStream out = new ByteArrayOutputStream();// bad command int exitCode = runTool(conf, createJobClient(), new String[] { "-counter" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, createJobClient(), new String[] { "-counter", jobId, "org.apache.hadoop.mapreduce.TaskCounter", "MAP_INPUT_RECORDS" }, out);assertEquals("Exit code", 0, exitCode);assertEquals("Counter", "3", out.toString().trim());}
protected void testAllJobList(String jobId, Configuration conf) {ByteArrayOutputStream out = new ByteArrayOutputStream();int exitCode = runTool(conf, createJobClient(), new String[] { "-list", "alldata" }, out);assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, createJobClient(), // all jobsnew String[] { "-list", "all" }, out);assertEquals("Exit code", 0, exitCode);BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));String line;int counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (line.contains(jobId)) {counter++;}}assertEquals(1, counter);out.reset();}
protected void testSubmittedJobList(Configuration conf) {Job job = runJobInBackGround(conf);ByteArrayOutputStream out = new ByteArrayOutputStream();String line;int counter = 0;// only submittedint exitCode = runTool(conf, createJobClient(), new String[] { "-list" }, out);assertEquals("Exit code", 0, exitCode);BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));counter = 0;while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (line.contains(job.getJobID().toString())) {counter++;}}// all jobs submitted! no currentassertEquals(1, counter);}
protected void verifyJobPriority(String jobId, String priority, Configuration conf, CLI jc) {PipedInputStream pis = new PipedInputStream();PipedOutputStream pos = new PipedOutputStream(pis);int exitCode = runTool(conf, jc, new String[] { "-list", "all" }, pos);assertEquals("Exit code", 0, exitCode);BufferedReader br = new BufferedReader(new InputStreamReader(pis));String line;while ((line = br.readLine()) != null) {LOG.info("line = " + line);if (!line.contains(jobId)) {continue;}assertTrue(line.contains(priority));break;}pis.close();}
public void testChangingJobPriority(String jobId, Configuration conf) {int exitCode = runTool(conf, createJobClient(), new String[] { "-set-priority" }, new ByteArrayOutputStream());assertEquals("Exit code", -1, exitCode);exitCode = runTool(conf, createJobClient(), new String[] { "-set-priority", jobId, "VERY_LOW" }, new ByteArrayOutputStream());assertEquals("Exit code", 0, exitCode);// because this method does not implemented still.verifyJobPriority(jobId, "NORMAL", conf, createJobClient());}
protected CLI createJobClient() {return new CLI();}public void testparseOption() {KeyFieldHelper helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");String keySpecs = "-k1.2,3.4";String eKeySpecs = keySpecs;helper.parseOption(keySpecs);String actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);// test -k a.bkeySpecs = "-k 1.2";eKeySpecs = "-k1.2,0.0";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr -k1.2,3.4";eKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr -k1.2,3.4n";eKeySpecs = "-k1.2,3.4n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr -k1.2,3.4r";eKeySpecs = "-k1.2,3.4r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr -k1.2,3.4 -k5.6,7.8n -k9.10,11.12r -k13.14,15.16nr";//1steKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);// 2ndeKeySpecs = "-k5.6,7.8n";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);//3rdeKeySpecs = "-k9.10,11.12r";actKeySpecs = helper.keySpecs().get(2).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);//4theKeySpecs = "-k13.14,15.16nr";actKeySpecs = helper.keySpecs().get(3).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2n,3.4";eKeySpecs = "-k1.2,3.4n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2r,3.4";eKeySpecs = "-k1.2,3.4r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2nr,3.4";eKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4n";eKeySpecs = "-k1.2,3.4n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4r";eKeySpecs = "-k1.2,3.4r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4nr";eKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr -k1.2,3.4 -k5.6,7.8";eKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8nr";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-n -k1.2,3.4 -k5.6,7.8";eKeySpecs = "-k1.2,3.4n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8n";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-r -k1.2,3.4 -k5.6,7.8";eKeySpecs = "-k1.2,3.4r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8r";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4n -k5.6,7.8";eKeySpecs = "-k1.2,3.4n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4r -k5.6,7.8";eKeySpecs = "-k1.2,3.4r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-k1.2,3.4nr -k5.6,7.8";eKeySpecs = "-k1.2,3.4nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);eKeySpecs = "-k5.6,7.8";actKeySpecs = helper.keySpecs().get(1).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-n";eKeySpecs = "-k1.1,0.0n";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-r";eKeySpecs = "-k1.1,0.0r";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);keySpecs = "-nr";eKeySpecs = "-k1.1,0.0nr";helper = new KeyFieldHelper();helper.parseOption(keySpecs);actKeySpecs = helper.keySpecs().get(0).toString();assertEquals("KeyFieldHelper's parsing is garbled", eKeySpecs, actKeySpecs);}
public void testGetWordLengths() {KeyFieldHelper helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");String input = "hi";int[] result = helper.getWordLengths(input.getBytes(), 0, 2);assertTrue(equals(result, new int[] { 1 }));// set the key specshelper.setKeyFieldSpec(1, 2);// test getWordLengths with 3 wordsinput = "hi\thello there";result = helper.getWordLengths(input.getBytes(), 0, input.length());assertTrue(equals(result, new int[] { 2, 2, 11 }));// test getWordLengths with 4 words but with a different separatorhelper.setKeyFieldSeparator(" ");input = "hi hello\tthere you";result = helper.getWordLengths(input.getBytes(), 0, input.length());assertTrue(equals(result, new int[] { 3, 2, 11, 3 }));// test with non zero start indexinput = "hi hello there you where me there";// .....................result = helper.getWordLengths(input.getBytes(), 10, 33);assertTrue(equals(result, new int[] { 5, 4, 3, 5, 2, 3 }));input = "hi hello there you where me ";// ..................result = helper.getWordLengths(input.getBytes(), 10, input.length());assertTrue(equals(result, new int[] { 5, 4, 3, 5, 2, 0 }));input = "";result = helper.getWordLengths(input.getBytes(), 0, 0);assertTrue(equals(result, new int[] { 1, 0 }));input = "  abc";result = helper.getWordLengths(input.getBytes(), 0, 5);assertTrue(equals(result, new int[] { 3, 0, 0, 3 }));input = "  abc";result = helper.getWordLengths(input.getBytes(), 0, 2);assertTrue(equals(result, new int[] { 3, 0, 0, 0 }));input = " abc ";result = helper.getWordLengths(input.getBytes(), 0, 2);assertTrue(equals(result, new int[] { 2, 0, 1 }));helper.setKeyFieldSeparator("abcd");input = "abc";result = helper.getWordLengths(input.getBytes(), 0, 3);assertTrue(equals(result, new int[] { 1, 3 }));}
public void testgetStartEndOffset() {KeyFieldHelper helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");// test getStartOffset with -k1,2helper.setKeyFieldSpec(1, 2);String input = "hi\thello";String expectedOutput = input;testKeySpecs(input, expectedOutput, helper);// test getStartOffset with -k1.0,0 .. should result into start = -1helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1.0,0");testKeySpecs(input, null, helper);// test getStartOffset with -k1,0helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1,0");expectedOutput = input;testKeySpecs(input, expectedOutput, helper);// test getStartOffset with -k1.2,0helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1.2,0");expectedOutput = "i\thello";testKeySpecs(input, expectedOutput, helper);// test getWordLengths with -k1.0,2.3helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1.1,2.3");expectedOutput = "hi\thel";testKeySpecs(input, expectedOutput, helper);// test getWordLengths with -k1.2,2.3helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1.2,2.3");expectedOutput = "i\thel";testKeySpecs(input, expectedOutput, helper);// test getStartOffset with -k1.2,3.0helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k1.2,3.0");expectedOutput = "i\thello";testKeySpecs(input, expectedOutput, helper);// test getStartOffset with -k2,2helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k2,2");expectedOutput = "hello";testKeySpecs(input, expectedOutput, helper);// test getStartOffset with -k3.0,4.0helper = new KeyFieldHelper();helper.setKeyFieldSeparator("\t");helper.parseOption("-k3.1,4.0");testKeySpecs(input, null, helper);// test getStartOffset with -k2.1helper = new KeyFieldHelper();input = "123123123123123hi\thello\thow";helper.setKeyFieldSeparator("\t");helper.parseOption("-k2.1");expectedOutput = "hello\thow";testKeySpecs(input, expectedOutput, helper, 15, input.length());// test getStartOffset with -k2.1,4 with end ending on \thelper = new KeyFieldHelper();input = "123123123123123hi\thello\t\thow\tare";helper.setKeyFieldSeparator("\t");helper.parseOption("-k2.1,3");expectedOutput = "hello\t";testKeySpecs(input, expectedOutput, helper, 17, input.length());// test getStartOffset with -k2.1 with end ending on \thelper = new KeyFieldHelper();input = "123123123123123hi\thello\thow\tare";helper.setKeyFieldSeparator("\t");helper.parseOption("-k2.1");expectedOutput = "hello\thow\t";testKeySpecs(input, expectedOutput, helper, 17, 28);// test getStartOffset with -k2.1,3 with smaller lengthhelper = new KeyFieldHelper();input = "123123123123123hi\thello\thow";helper.setKeyFieldSeparator("\t");helper.parseOption("-k2.1,3");expectedOutput = "hello";testKeySpecs(input, expectedOutput, helper, 15, 23);}
private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper) {testKeySpecs(input, expectedOutput, helper, 0, -1);}
private void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1) {LOG.info("input : " + input);String keySpecs = helper.keySpecs().get(0).toString();LOG.info("keyspecs : " + keySpecs);byte[] inputBytes = input.getBytes();if (e1 == -1) {e1 = inputBytes.length;}LOG.info("length : " + e1);// get the word lengthsint[] indices = helper.getWordLengths(inputBytes, s1, e1);int start = helper.getStartOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));LOG.info("start : " + start);if (expectedOutput == null) {assertEquals("Expected -1 when the start index is invalid", -1, start);return;}int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));LOG.info("end : " + end);end = (end >= inputBytes.length) ? inputBytes.length - 1 : end;int length = end + 1 - start;LOG.info("length : " + length);byte[] outputBytes = new byte[length];System.arraycopy(inputBytes, start, outputBytes, 0, length);String output = new String(outputBytes);LOG.info("output : " + output);LOG.info("expected-output : " + expectedOutput);assertEquals(keySpecs + " failed on input '" + input + "'", expectedOutput, output);}
private boolean equals(int[] test, int[] expected) {// check array lengthif (test[0] != expected[0]) {return false;}// if length is same then check the contentsfor (int i = 0; i < test[0] && i < expected[0]; ++i) {if (test[i] != expected[i]) {return false;}}return true;}public static void init() {conf = new Configuration();conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);initCluster(true);}
public static void shutdown() {IOUtils.cleanup(null, hdfs, fsAsBruce, fsAsDiana);if (cluster != null) {cluster.shutdown();}}
public void setUp() {++pathCount;path = new Path("/p" + pathCount);snapshotName = "snapshot" + pathCount;snapshotPath = new Path(path, new Path(".snapshot", snapshotName));}
public void testOriginalAclEnforcedForSnapshotRootAfterChange() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(path, aclSpec);assertDirPermissionGranted(fsAsBruce, BRUCE, path);assertDirPermissionDenied(fsAsDiana, DIANA, path);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, path);s = hdfs.getAclStatus(snapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, snapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, snapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, snapshotPath);aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_EXECUTE), aclEntry(ACCESS, USER, "diana", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(path, aclSpec);// Original has changed, but snapshot still has old ACL.doSnapshotRootChangeAssertions(path, snapshotPath);restart(false);doSnapshotRootChangeAssertions(path, snapshotPath);restart(true);doSnapshotRootChangeAssertions(path, snapshotPath);}
private static void doSnapshotRootChangeAssertions(Path path, Path snapshotPath) {AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "diana", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010550, path);s = hdfs.getAclStatus(snapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, snapshotPath);assertDirPermissionDenied(fsAsBruce, BRUCE, path);assertDirPermissionGranted(fsAsDiana, DIANA, path);assertDirPermissionGranted(fsAsBruce, BRUCE, snapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, snapshotPath);}
public void testOriginalAclEnforcedForSnapshotContentsAfterChange() {Path filePath = new Path(path, "file1");Path subdirPath = new Path(path, "subdir1");Path fileSnapshotPath = new Path(snapshotPath, "file1");Path subdirSnapshotPath = new Path(snapshotPath, "subdir1");FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0777));FileSystem.create(hdfs, filePath, FsPermission.createImmutable((short) 0600)).close();FileSystem.mkdirs(hdfs, subdirPath, FsPermission.createImmutable((short) 0700));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_EXECUTE), aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(filePath, aclSpec);hdfs.setAcl(subdirPath, aclSpec);assertFilePermissionGranted(fsAsBruce, BRUCE, filePath);assertFilePermissionDenied(fsAsDiana, DIANA, filePath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirPath);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);// Both original and snapshot still have same ACL.AclEntry[] expected = new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) };AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, filePath);s = hdfs.getAclStatus(subdirPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirPath);s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, fileSnapshotPath);assertFilePermissionGranted(fsAsBruce, BRUCE, fileSnapshotPath);assertFilePermissionDenied(fsAsDiana, DIANA, fileSnapshotPath);s = hdfs.getAclStatus(subdirSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirSnapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirSnapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirSnapshotPath);aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_EXECUTE), aclEntry(ACCESS, USER, "diana", ALL), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(filePath, aclSpec);hdfs.setAcl(subdirPath, aclSpec);// Original has changed, but snapshot still has old ACL.doSnapshotContentsChangeAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);restart(false);doSnapshotContentsChangeAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);restart(true);doSnapshotContentsChangeAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);}
private static void doSnapshotContentsChangeAssertions(Path filePath, Path fileSnapshotPath, Path subdirPath, Path subdirSnapshotPath) {AclEntry[] expected = new AclEntry[] { aclEntry(ACCESS, USER, "diana", ALL), aclEntry(ACCESS, GROUP, NONE) };AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010570, filePath);assertFilePermissionDenied(fsAsBruce, BRUCE, filePath);assertFilePermissionGranted(fsAsDiana, DIANA, filePath);s = hdfs.getAclStatus(subdirPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010570, subdirPath);assertDirPermissionDenied(fsAsBruce, BRUCE, subdirPath);assertDirPermissionGranted(fsAsDiana, DIANA, subdirPath);expected = new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) };s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, fileSnapshotPath);assertFilePermissionGranted(fsAsBruce, BRUCE, fileSnapshotPath);assertFilePermissionDenied(fsAsDiana, DIANA, fileSnapshotPath);s = hdfs.getAclStatus(subdirSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirSnapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirSnapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirSnapshotPath);}
public void testOriginalAclEnforcedForSnapshotRootAfterRemoval() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(path, aclSpec);assertDirPermissionGranted(fsAsBruce, BRUCE, path);assertDirPermissionDenied(fsAsDiana, DIANA, path);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, path);s = hdfs.getAclStatus(snapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, snapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, snapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, snapshotPath);hdfs.removeAcl(path);// Original has changed, but snapshot still has old ACL.doSnapshotRootRemovalAssertions(path, snapshotPath);restart(false);doSnapshotRootRemovalAssertions(path, snapshotPath);restart(true);doSnapshotRootRemovalAssertions(path, snapshotPath);}
private static void doSnapshotRootRemovalAssertions(Path path, Path snapshotPath) {AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] {}, returned);assertPermission((short) 0700, path);s = hdfs.getAclStatus(snapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010750, snapshotPath);assertDirPermissionDenied(fsAsBruce, BRUCE, path);assertDirPermissionDenied(fsAsDiana, DIANA, path);assertDirPermissionGranted(fsAsBruce, BRUCE, snapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, snapshotPath);}
public void testOriginalAclEnforcedForSnapshotContentsAfterRemoval() {Path filePath = new Path(path, "file1");Path subdirPath = new Path(path, "subdir1");Path fileSnapshotPath = new Path(snapshotPath, "file1");Path subdirSnapshotPath = new Path(snapshotPath, "subdir1");FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0777));FileSystem.create(hdfs, filePath, FsPermission.createImmutable((short) 0600)).close();FileSystem.mkdirs(hdfs, subdirPath, FsPermission.createImmutable((short) 0700));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_EXECUTE), aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE), aclEntry(ACCESS, OTHER, NONE));hdfs.setAcl(filePath, aclSpec);hdfs.setAcl(subdirPath, aclSpec);assertFilePermissionGranted(fsAsBruce, BRUCE, filePath);assertFilePermissionDenied(fsAsDiana, DIANA, filePath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirPath);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);// Both original and snapshot still have same ACL.AclEntry[] expected = new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) };AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, filePath);s = hdfs.getAclStatus(subdirPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirPath);s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, fileSnapshotPath);assertFilePermissionGranted(fsAsBruce, BRUCE, fileSnapshotPath);assertFilePermissionDenied(fsAsDiana, DIANA, fileSnapshotPath);s = hdfs.getAclStatus(subdirSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirSnapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirSnapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirSnapshotPath);hdfs.removeAcl(filePath);hdfs.removeAcl(subdirPath);// Original has changed, but snapshot still has old ACL.doSnapshotContentsRemovalAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);restart(false);doSnapshotContentsRemovalAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);restart(true);doSnapshotContentsRemovalAssertions(filePath, fileSnapshotPath, subdirPath, subdirSnapshotPath);}
private static void doSnapshotContentsRemovalAssertions(Path filePath, Path fileSnapshotPath, Path subdirPath, Path subdirSnapshotPath) {AclEntry[] expected = new AclEntry[] {};AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 0500, filePath);assertFilePermissionDenied(fsAsBruce, BRUCE, filePath);assertFilePermissionDenied(fsAsDiana, DIANA, filePath);s = hdfs.getAclStatus(subdirPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 0500, subdirPath);assertDirPermissionDenied(fsAsBruce, BRUCE, subdirPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirPath);expected = new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) };s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, fileSnapshotPath);assertFilePermissionGranted(fsAsBruce, BRUCE, fileSnapshotPath);assertFilePermissionDenied(fsAsDiana, DIANA, fileSnapshotPath);s = hdfs.getAclStatus(subdirSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010550, subdirSnapshotPath);assertDirPermissionGranted(fsAsBruce, BRUCE, subdirSnapshotPath);assertDirPermissionDenied(fsAsDiana, DIANA, subdirSnapshotPath);}
public void testModifyReadsCurrentState() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", ALL));hdfs.modifyAclEntries(path, aclSpec);aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "diana", READ_EXECUTE));hdfs.modifyAclEntries(path, aclSpec);AclEntry[] expected = new AclEntry[] { aclEntry(ACCESS, USER, "bruce", ALL), aclEntry(ACCESS, USER, "diana", READ_EXECUTE), aclEntry(ACCESS, GROUP, NONE) };AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 010770, path);assertDirPermissionGranted(fsAsBruce, BRUCE, path);assertDirPermissionGranted(fsAsDiana, DIANA, path);}
public void testRemoveReadsCurrentState() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", ALL));hdfs.modifyAclEntries(path, aclSpec);hdfs.removeAcl(path);AclEntry[] expected = new AclEntry[] {};AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(expected, returned);assertPermission((short) 0700, path);assertDirPermissionDenied(fsAsBruce, BRUCE, path);assertDirPermissionDenied(fsAsDiana, DIANA, path);}
public void testDefaultAclNotCopiedToAccessAclOfNewSnapshot() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, "bruce", READ_EXECUTE));hdfs.modifyAclEntries(path, aclSpec);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);AclStatus s = hdfs.getAclStatus(path);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, "bruce", READ_EXECUTE), aclEntry(DEFAULT, GROUP, NONE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);assertPermission((short) 010700, path);s = hdfs.getAclStatus(snapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, "bruce", READ_EXECUTE), aclEntry(DEFAULT, GROUP, NONE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);assertPermission((short) 010700, snapshotPath);assertDirPermissionDenied(fsAsBruce, BRUCE, snapshotPath);}
public void testModifyAclEntriesSnapshotPath() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, "bruce", READ_EXECUTE));exception.expect(SnapshotAccessControlException.class);hdfs.modifyAclEntries(snapshotPath, aclSpec);}
public void testRemoveAclEntriesSnapshotPath() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, "bruce"));exception.expect(SnapshotAccessControlException.class);hdfs.removeAclEntries(snapshotPath, aclSpec);}
public void testRemoveDefaultAclSnapshotPath() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);exception.expect(SnapshotAccessControlException.class);hdfs.removeDefaultAcl(snapshotPath);}
public void testRemoveAclSnapshotPath() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);exception.expect(SnapshotAccessControlException.class);hdfs.removeAcl(snapshotPath);}
public void testSetAclSnapshotPath() {FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0700));SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, "bruce"));exception.expect(SnapshotAccessControlException.class);hdfs.setAcl(snapshotPath, aclSpec);}
public void testChangeAclExceedsQuota() {Path filePath = new Path(path, "file1");Path fileSnapshotPath = new Path(snapshotPath, "file1");FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0755));hdfs.allowSnapshot(path);hdfs.setQuota(path, 3, HdfsConstants.QUOTA_DONT_SET);FileSystem.create(hdfs, filePath, FsPermission.createImmutable((short) 0600)).close();hdfs.setPermission(filePath, FsPermission.createImmutable((short) 0600));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", READ_WRITE));hdfs.modifyAclEntries(filePath, aclSpec);hdfs.createSnapshot(path, snapshotName);AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_WRITE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010660, filePath);s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_WRITE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010660, filePath);aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", READ));exception.expect(NSQuotaExceededException.class);hdfs.modifyAclEntries(filePath, aclSpec);}
public void testRemoveAclExceedsQuota() {Path filePath = new Path(path, "file1");Path fileSnapshotPath = new Path(snapshotPath, "file1");FileSystem.mkdirs(hdfs, path, FsPermission.createImmutable((short) 0755));hdfs.allowSnapshot(path);hdfs.setQuota(path, 3, HdfsConstants.QUOTA_DONT_SET);FileSystem.create(hdfs, filePath, FsPermission.createImmutable((short) 0600)).close();hdfs.setPermission(filePath, FsPermission.createImmutable((short) 0600));List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", READ_WRITE));hdfs.modifyAclEntries(filePath, aclSpec);hdfs.createSnapshot(path, snapshotName);AclStatus s = hdfs.getAclStatus(filePath);AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_WRITE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010660, filePath);s = hdfs.getAclStatus(fileSnapshotPath);returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, "bruce", READ_WRITE), aclEntry(ACCESS, GROUP, NONE) }, returned);assertPermission((short) 010660, filePath);aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, "bruce", READ));exception.expect(NSQuotaExceededException.class);hdfs.removeAcl(filePath);}
public void testGetAclStatusDotSnapshotPath() {hdfs.mkdirs(path);SnapshotTestHelper.createSnapshot(hdfs, path, snapshotName);AclStatus s = hdfs.getAclStatus(new Path(path, ".snapshot"));AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);assertArrayEquals(new AclEntry[] {}, returned);}
private static void assertDirPermissionDenied(FileSystem fs, UserGroupInformation user, Path pathToCheck) {try {fs.listStatus(pathToCheck);fail("expected AccessControlException for user " + user + ", path = " + pathToCheck);} catch (AccessControlException e) {}try {fs.access(pathToCheck, FsAction.READ);fail("The access call should have failed for " + pathToCheck);} catch (AccessControlException e) {}}
private static void assertDirPermissionGranted(FileSystem fs, UserGroupInformation user, Path pathToCheck) {try {fs.listStatus(pathToCheck);fs.access(pathToCheck, FsAction.READ);} catch (AccessControlException e) {fail("expected permission granted for user " + user + ", path = " + pathToCheck);}}
private static void assertPermission(short perm, Path pathToCheck) {AclTestHelpers.assertPermission(hdfs, pathToCheck, perm);}
private static void initCluster(boolean format) {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(format).build();cluster.waitActive();hdfs = cluster.getFileSystem();fsAsBruce = DFSTestUtil.getFileSystemAs(BRUCE, conf);fsAsDiana = DFSTestUtil.getFileSystemAs(DIANA, conf);}
private static void restart(boolean checkpoint) {NameNode nameNode = cluster.getNameNode();if (checkpoint) {NameNodeAdapter.enterSafeMode(nameNode, false);NameNodeAdapter.saveNamespace(nameNode);}shutdown();initCluster(false);}public long getProcessedByteCount() {return reportedBytesReadFromCompressedStream;}
protected void updateProcessedByteCount(int count) {this.bytesReadFromCompressedStream += count;}
public void updateReportedByteCount(int count) {this.reportedBytesReadFromCompressedStream += count;this.updateProcessedByteCount(count);}
private int readAByte(InputStream inStream) {int read = inStream.read();if (read >= 0) {this.updateProcessedByteCount(1);}return read;}
public boolean skipToNextMarker(long marker, int markerBitLength) {try {if (markerBitLength > 63) {throw new IllegalArgumentException("skipToNextMarker can not find patterns greater than 63 bits");}// pick next marketBitLength bits in the streamlong bytes = 0;bytes = this.bsR(markerBitLength);if (bytes == -1) {return false;}while (true) {if (bytes == marker) {return true;} else {bytes = bytes << 1;bytes = bytes & ((1L << markerBitLength) - 1);int oneBit = (int) this.bsR(1);if (oneBit != -1) {bytes = bytes | oneBit;} elsereturn false;}}} catch (IOException ex) {return false;}}
protected void reportCRCError() {throw new IOException("crc error");}
private void makeMaps() {final boolean[] inUse = this.data.inUse;final byte[] seqToUnseq = this.data.seqToUnseq;int nInUseShadow = 0;for (int i = 0; i < 256; i++) {if (inUse[i])seqToUnseq[nInUseShadow++] = (byte) i;}this.nInUse = nInUseShadow;}
public static long numberOfBytesTillNextMarker(final InputStream in) {CBZip2InputStream anObject = new CBZip2InputStream(in, READ_MODE.BYBLOCK, true);return anObject.getProcessedByteCount();}
private void changeStateToProcessABlock() {if (skipResult == true) {initBlock();setupBlock();} else {this.currentState = STATE.EOF;}}
public int read() {if (this.in != null) {int result = this.read(array, 0, 1);int value = 0XFF & array[0];return (result > 0 ? value : result);} else {throw new IOException("stream closed");}}
public int read(final byte[] dest, final int offs, final int len) {if (offs < 0) {throw new IndexOutOfBoundsException("offs(" + offs + ") < 0.");}if (len < 0) {throw new IndexOutOfBoundsException("len(" + len + ") < 0.");}if (offs + len > dest.length) {throw new IndexOutOfBoundsException("offs(" + offs + ") + len(" + len + ") > dest.length(" + dest.length + ").");}if (this.in == null) {throw new IOException("stream closed");}if (lazyInitialization) {this.init();this.lazyInitialization = false;}if (skipDecompression) {changeStateToProcessABlock();skipDecompression = false;}final int hi = offs + len;int destOffs = offs;int b = 0;for (; ((destOffs < hi) && ((b = read0())) >= 0); ) {dest[destOffs++] = (byte) b;}int result = destOffs - offs;if (result == 0) {//report 'end of block' or 'end of stream'result = b;skipResult = this.skipToNextMarker(CBZip2InputStream.BLOCK_DELIMITER, DELIMITER_BIT_LENGTH);//Exactly when we are about to start a new block, we advertise the stream position.this.reportedBytesReadFromCompressedStream = this.bytesReadFromCompressedStream;changeStateToProcessABlock();}return result;}
private int read0() {final int retChar = this.currentChar;switch(this.currentState) {case EOF:// return -1return END_OF_STREAM;case NO_PROCESS_STATE:// return -2return END_OF_BLOCK;case START_BLOCK_STATE:throw new IllegalStateException();case RAND_PART_A_STATE:throw new IllegalStateException();case RAND_PART_B_STATE:setupRandPartB();break;case RAND_PART_C_STATE:setupRandPartC();break;case NO_RAND_PART_A_STATE:throw new IllegalStateException();case NO_RAND_PART_B_STATE:setupNoRandPartB();break;case NO_RAND_PART_C_STATE:setupNoRandPartC();break;default:throw new IllegalStateException();}return retChar;}
private void init() {int magic2 = this.readAByte(in);if (magic2 != 'h') {throw new IOException("Stream is not BZip2 formatted: expected 'h'" + " as first byte but got '" + (char) magic2 + "'");}int blockSize = this.readAByte(in);if ((blockSize < '1') || (blockSize > '9')) {throw new IOException("Stream is not BZip2 formatted: illegal " + "blocksize " + (char) blockSize);}this.blockSize100k = blockSize - '0';initBlock();setupBlock();}
private void initBlock() {if (this.readMode == READ_MODE.BYBLOCK) {// this.checkBlockIntegrity();this.storedBlockCRC = bsGetInt();this.blockRandomised = bsR(1) == 1;/**  * Allocate data here instead in constructor, so we do not allocate  * it if the input file is empty.  */if (this.data == null) {this.data = new Data(this.blockSize100k);}// currBlockNo++;getAndMoveToFrontDecode();this.crc.initialiseCRC();this.currentState = STATE.START_BLOCK_STATE;return;}char magic0 = bsGetUByte();char magic1 = bsGetUByte();char magic2 = bsGetUByte();char magic3 = bsGetUByte();char magic4 = bsGetUByte();char magic5 = bsGetUByte();if (magic0 == 0x17 && magic1 == 0x72 && magic2 == 0x45 && magic3 == 0x38 && magic4 == 0x50 && magic5 == 0x90) {// end of filecomplete();} else if (// '1'magic0 != 0x31 || // ')'magic1 != 0x41 || // 'Y'magic2 != 0x59 || // '&'magic3 != 0x26 || // 'S'magic4 != 0x53 || // 'Y'magic5 != 0x59) {this.currentState = STATE.EOF;throw new IOException("bad block header");} else {this.storedBlockCRC = bsGetInt();this.blockRandomised = bsR(1) == 1;/**  * Allocate data here instead in constructor, so we do not allocate  * it if the input file is empty.  */if (this.data == null) {this.data = new Data(this.blockSize100k);}// currBlockNo++;getAndMoveToFrontDecode();this.crc.initialiseCRC();this.currentState = STATE.START_BLOCK_STATE;}}
private void endBlock() {this.computedBlockCRC = this.crc.getFinalCRC();// A bad CRC is considered a fatal error.if (this.storedBlockCRC != this.computedBlockCRC) {// (repair feature, not yet documented, not tested)this.computedCombinedCRC = (this.storedCombinedCRC << 1) | (this.storedCombinedCRC >>> 31);this.computedCombinedCRC ^= this.storedBlockCRC;reportCRCError();}this.computedCombinedCRC = (this.computedCombinedCRC << 1) | (this.computedCombinedCRC >>> 31);this.computedCombinedCRC ^= this.computedBlockCRC;}
private void complete() {this.storedCombinedCRC = bsGetInt();this.currentState = STATE.EOF;this.data = null;if (this.storedCombinedCRC != this.computedCombinedCRC) {reportCRCError();}}
public void close() {InputStream inShadow = this.in;if (inShadow != null) {try {if (inShadow != System.in) {inShadow.close();}} finally {this.data = null;this.in = null;}}}
private long bsR(final long n) {long bsLiveShadow = this.bsLive;long bsBuffShadow = this.bsBuff;if (bsLiveShadow < n) {final InputStream inShadow = this.in;do {int thech = readAByte(inShadow);if (thech < 0) {throw new IOException("unexpected end of stream");}bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;} while (bsLiveShadow < n);this.bsBuff = bsBuffShadow;}this.bsLive = bsLiveShadow - n;return (bsBuffShadow >> (bsLiveShadow - n)) & ((1L << n) - 1);}
private boolean bsGetBit() {long bsLiveShadow = this.bsLive;long bsBuffShadow = this.bsBuff;if (bsLiveShadow < 1) {int thech = this.readAByte(in);if (thech < 0) {throw new IOException("unexpected end of stream");}bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;this.bsBuff = bsBuffShadow;}this.bsLive = bsLiveShadow - 1;return ((bsBuffShadow >> (bsLiveShadow - 1)) & 1) != 0;}
private char bsGetUByte() {return (char) bsR(8);}
private int bsGetInt() {return (int) ((((((bsR(8) << 8) | bsR(8)) << 8) | bsR(8)) << 8) | bsR(8));}
private static void hbCreateDecodeTables(final int[] limit, final int[] base, final int[] perm, final char[] length, final int minLen, final int maxLen, final int alphaSize) {for (int i = minLen, pp = 0; i <= maxLen; i++) {for (int j = 0; j < alphaSize; j++) {if (length[j] == i) {perm[pp++] = j;}}}for (int i = MAX_CODE_LEN; --i > 0; ) {base[i] = 0;limit[i] = 0;}for (int i = 0; i < alphaSize; i++) {base[length[i] + 1]++;}for (int i = 1, b = base[0]; i < MAX_CODE_LEN; i++) {b += base[i];base[i] = b;}for (int i = minLen, vec = 0, b = base[i]; i <= maxLen; i++) {final int nb = base[i + 1];vec += nb - b;b = nb;limit[i] = vec - 1;vec <<= 1;}for (int i = minLen + 1; i <= maxLen; i++) {base[i] = ((limit[i - 1] + 1) << 1) - base[i];}}
private void recvDecodingTables() {final Data dataShadow = this.data;final boolean[] inUse = dataShadow.inUse;final byte[] pos = dataShadow.recvDecodingTables_pos;final byte[] selector = dataShadow.selector;final byte[] selectorMtf = dataShadow.selectorMtf;int inUse16 = 0;/* Receive the mapping table */for (int i = 0; i < 16; i++) {if (bsGetBit()) {inUse16 |= 1 << i;}}for (int i = 256; --i >= 0; ) {inUse[i] = false;}for (int i = 0; i < 16; i++) {if ((inUse16 & (1 << i)) != 0) {final int i16 = i << 4;for (int j = 0; j < 16; j++) {if (bsGetBit()) {inUse[i16 + j] = true;}}}}makeMaps();final int alphaSize = this.nInUse + 2;/* Now the selectors */final int nGroups = (int) bsR(3);final int nSelectors = (int) bsR(15);for (int i = 0; i < nSelectors; i++) {int j = 0;while (bsGetBit()) {j++;}selectorMtf[i] = (byte) j;}/* Undo the MTF values for the selectors. */for (int v = nGroups; --v >= 0; ) {pos[v] = (byte) v;}for (int i = 0; i < nSelectors; i++) {int v = selectorMtf[i] & 0xff;final byte tmp = pos[v];while (v > 0) {// nearly all times v is zero, 4 in most other casespos[v] = pos[v - 1];v--;}pos[0] = tmp;selector[i] = tmp;}final char[][] len = dataShadow.temp_charArray2d;/* Now the coding tables */for (int t = 0; t < nGroups; t++) {int curr = (int) bsR(5);final char[] len_t = len[t];for (int i = 0; i < alphaSize; i++) {while (bsGetBit()) {curr += bsGetBit() ? -1 : 1;}len_t[i] = (char) curr;}}// finally create the Huffman tablescreateHuffmanDecodingTables(alphaSize, nGroups);}
private void createHuffmanDecodingTables(final int alphaSize, final int nGroups) {final Data dataShadow = this.data;final char[][] len = dataShadow.temp_charArray2d;final int[] minLens = dataShadow.minLens;final int[][] limit = dataShadow.limit;final int[][] base = dataShadow.base;final int[][] perm = dataShadow.perm;for (int t = 0; t < nGroups; t++) {int minLen = 32;int maxLen = 0;final char[] len_t = len[t];for (int i = alphaSize; --i >= 0; ) {final char lent = len_t[i];if (lent > maxLen) {maxLen = lent;}if (lent < minLen) {minLen = lent;}}hbCreateDecodeTables(limit[t], base[t], perm[t], len[t], minLen, maxLen, alphaSize);minLens[t] = minLen;}}
private void getAndMoveToFrontDecode() {this.origPtr = (int) bsR(24);recvDecodingTables();final InputStream inShadow = this.in;final Data dataShadow = this.data;final byte[] ll8 = dataShadow.ll8;final int[] unzftab = dataShadow.unzftab;final byte[] selector = dataShadow.selector;final byte[] seqToUnseq = dataShadow.seqToUnseq;final char[] yy = dataShadow.getAndMoveToFrontDecode_yy;final int[] minLens = dataShadow.minLens;final int[][] limit = dataShadow.limit;final int[][] base = dataShadow.base;final int[][] perm = dataShadow.perm;final int limitLast = this.blockSize100k * 100000;for (int i = 256; --i >= 0; ) {yy[i] = (char) i;unzftab[i] = 0;}int groupNo = 0;int groupPos = G_SIZE - 1;final int eob = this.nInUse + 1;int nextSym = getAndMoveToFrontDecode0(0);int bsBuffShadow = (int) this.bsBuff;int bsLiveShadow = (int) this.bsLive;int lastShadow = -1;int zt = selector[groupNo] & 0xff;int[] base_zt = base[zt];int[] limit_zt = limit[zt];int[] perm_zt = perm[zt];int minLens_zt = minLens[zt];while (nextSym != eob) {if ((nextSym == RUNA) || (nextSym == RUNB)) {int s = -1;for (int n = 1; true; n <<= 1) {if (nextSym == RUNA) {s += n;} else if (nextSym == RUNB) {s += n << 1;} else {break;}if (groupPos == 0) {groupPos = G_SIZE - 1;zt = selector[++groupNo] & 0xff;base_zt = base[zt];limit_zt = limit[zt];perm_zt = perm[zt];minLens_zt = minLens[zt];} else {groupPos--;}int zn = minLens_zt;while (bsLiveShadow < zn) {final int thech = readAByte(inShadow);if (thech >= 0) {bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;continue;} else {throw new IOException("unexpected end of stream");}}long zvec = (bsBuffShadow >> (bsLiveShadow - zn)) & ((1 << zn) - 1);bsLiveShadow -= zn;while (zvec > limit_zt[zn]) {zn++;while (bsLiveShadow < 1) {final int thech = readAByte(inShadow);if (thech >= 0) {bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;continue;} else {throw new IOException("unexpected end of stream");}}bsLiveShadow--;zvec = (zvec << 1) | ((bsBuffShadow >> bsLiveShadow) & 1);}nextSym = perm_zt[(int) (zvec - base_zt[zn])];}final byte ch = seqToUnseq[yy[0]];unzftab[ch & 0xff] += s + 1;while (s-- >= 0) {ll8[++lastShadow] = ch;}if (lastShadow >= limitLast) {throw new IOException("block overrun");}} else {if (++lastShadow >= limitLast) {throw new IOException("block overrun");}final char tmp = yy[nextSym - 1];unzftab[seqToUnseq[tmp] & 0xff]++;ll8[lastShadow] = seqToUnseq[tmp];/** This loop is hammered during decompression, hence avoid* native method call overhead of System.arraycopy for very* small ranges to copy.*/if (nextSym <= 16) {for (int j = nextSym - 1; j > 0; ) {yy[j] = yy[--j];}} else {System.arraycopy(yy, 0, yy, 1, nextSym - 1);}yy[0] = tmp;if (groupPos == 0) {groupPos = G_SIZE - 1;zt = selector[++groupNo] & 0xff;base_zt = base[zt];limit_zt = limit[zt];perm_zt = perm[zt];minLens_zt = minLens[zt];} else {groupPos--;}int zn = minLens_zt;while (bsLiveShadow < zn) {final int thech = readAByte(inShadow);if (thech >= 0) {bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;continue;} else {throw new IOException("unexpected end of stream");}}int zvec = (bsBuffShadow >> (bsLiveShadow - zn)) & ((1 << zn) - 1);bsLiveShadow -= zn;while (zvec > limit_zt[zn]) {zn++;while (bsLiveShadow < 1) {final int thech = readAByte(inShadow);if (thech >= 0) {bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;continue;} else {throw new IOException("unexpected end of stream");}}bsLiveShadow--;zvec = ((zvec << 1) | ((bsBuffShadow >> bsLiveShadow) & 1));}nextSym = perm_zt[zvec - base_zt[zn]];}}this.last = lastShadow;this.bsLive = bsLiveShadow;this.bsBuff = bsBuffShadow;}
private int getAndMoveToFrontDecode0(final int groupNo) {final InputStream inShadow = this.in;final Data dataShadow = this.data;final int zt = dataShadow.selector[groupNo] & 0xff;final int[] limit_zt = dataShadow.limit[zt];int zn = dataShadow.minLens[zt];int zvec = (int) bsR(zn);int bsLiveShadow = (int) this.bsLive;int bsBuffShadow = (int) this.bsBuff;while (zvec > limit_zt[zn]) {zn++;while (bsLiveShadow < 1) {final int thech = readAByte(inShadow);if (thech >= 0) {bsBuffShadow = (bsBuffShadow << 8) | thech;bsLiveShadow += 8;continue;} else {throw new IOException("unexpected end of stream");}}bsLiveShadow--;zvec = (zvec << 1) | ((bsBuffShadow >> bsLiveShadow) & 1);}this.bsLive = bsLiveShadow;this.bsBuff = bsBuffShadow;return dataShadow.perm[zt][zvec - dataShadow.base[zt][zn]];}
private void setupBlock() {if (this.data == null) {return;}final int[] cftab = this.data.cftab;final int[] tt = this.data.initTT(this.last + 1);final byte[] ll8 = this.data.ll8;cftab[0] = 0;System.arraycopy(this.data.unzftab, 0, cftab, 1, 256);for (int i = 1, c = cftab[0]; i <= 256; i++) {c += cftab[i];cftab[i] = c;}for (int i = 0, lastShadow = this.last; i <= lastShadow; i++) {tt[cftab[ll8[i] & 0xff]++] = i;}if ((this.origPtr < 0) || (this.origPtr >= tt.length)) {throw new IOException("stream corrupted");}this.su_tPos = tt[this.origPtr];this.su_count = 0;this.su_i2 = 0;this.su_ch2 = 256;if (this.blockRandomised) {this.su_rNToGo = 0;this.su_rTPos = 0;setupRandPartA();} else {setupNoRandPartA();}}
private void setupRandPartA() {if (this.su_i2 <= this.last) {this.su_chPrev = this.su_ch2;int su_ch2Shadow = this.data.ll8[this.su_tPos] & 0xff;this.su_tPos = this.data.tt[this.su_tPos];if (this.su_rNToGo == 0) {this.su_rNToGo = BZip2Constants.rNums[this.su_rTPos] - 1;if (++this.su_rTPos == 512) {this.su_rTPos = 0;}} else {this.su_rNToGo--;}this.su_ch2 = su_ch2Shadow ^= (this.su_rNToGo == 1) ? 1 : 0;this.su_i2++;this.currentChar = su_ch2Shadow;this.currentState = STATE.RAND_PART_B_STATE;this.crc.updateCRC(su_ch2Shadow);} else {endBlock();if (readMode == READ_MODE.CONTINUOUS) {initBlock();setupBlock();} else if (readMode == READ_MODE.BYBLOCK) {this.currentState = STATE.NO_PROCESS_STATE;}}}
private void setupNoRandPartA() {if (this.su_i2 <= this.last) {this.su_chPrev = this.su_ch2;int su_ch2Shadow = this.data.ll8[this.su_tPos] & 0xff;this.su_ch2 = su_ch2Shadow;this.su_tPos = this.data.tt[this.su_tPos];this.su_i2++;this.currentChar = su_ch2Shadow;this.currentState = STATE.NO_RAND_PART_B_STATE;this.crc.updateCRC(su_ch2Shadow);} else {this.currentState = STATE.NO_RAND_PART_A_STATE;endBlock();if (readMode == READ_MODE.CONTINUOUS) {initBlock();setupBlock();} else if (readMode == READ_MODE.BYBLOCK) {this.currentState = STATE.NO_PROCESS_STATE;}}}
private void setupRandPartB() {if (this.su_ch2 != this.su_chPrev) {this.currentState = STATE.RAND_PART_A_STATE;this.su_count = 1;setupRandPartA();} else if (++this.su_count >= 4) {this.su_z = (char) (this.data.ll8[this.su_tPos] & 0xff);this.su_tPos = this.data.tt[this.su_tPos];if (this.su_rNToGo == 0) {this.su_rNToGo = BZip2Constants.rNums[this.su_rTPos] - 1;if (++this.su_rTPos == 512) {this.su_rTPos = 0;}} else {this.su_rNToGo--;}this.su_j2 = 0;this.currentState = STATE.RAND_PART_C_STATE;if (this.su_rNToGo == 1) {this.su_z ^= 1;}setupRandPartC();} else {this.currentState = STATE.RAND_PART_A_STATE;setupRandPartA();}}
private void setupRandPartC() {if (this.su_j2 < this.su_z) {this.currentChar = this.su_ch2;this.crc.updateCRC(this.su_ch2);this.su_j2++;} else {this.currentState = STATE.RAND_PART_A_STATE;this.su_i2++;this.su_count = 0;setupRandPartA();}}
private void setupNoRandPartB() {if (this.su_ch2 != this.su_chPrev) {this.su_count = 1;setupNoRandPartA();} else if (++this.su_count >= 4) {this.su_z = (char) (this.data.ll8[this.su_tPos] & 0xff);this.su_tPos = this.data.tt[this.su_tPos];this.su_j2 = 0;setupNoRandPartC();} else {setupNoRandPartA();}}
private void setupNoRandPartC() {if (this.su_j2 < this.su_z) {int su_ch2Shadow = this.su_ch2;this.currentChar = su_ch2Shadow;this.crc.updateCRC(su_ch2Shadow);this.su_j2++;this.currentState = STATE.NO_RAND_PART_C_STATE;} else {this.su_i2++;this.su_count = 0;setupNoRandPartA();}}
final int[] initTT(int length) {int[] ttShadow = this.tt;// than others.if ((ttShadow == null) || (ttShadow.length < length)) {this.tt = ttShadow = new int[length];}return ttShadow;}protected FileContextTestHelper createFileContextHelper() {return new FileContextTestHelper();}
public boolean accept(final Path file) {return true;}
public boolean accept(Path file) {if (file.getName().contains("x") || file.getName().contains("X"))return true;elsereturn false;}
public void setUp() {File testBuildData = new File(System.getProperty("test.build.data", "build/test/data"), RandomStringUtils.randomAlphanumeric(10));Path rootPath = new Path(testBuildData.getAbsolutePath(), "root-uri");localFsRootPath = rootPath.makeQualified(LocalFileSystem.NAME, null);fc.mkdir(getTestRootPath(fc, "test"), FileContext.DEFAULT_PERM, true);}
public void tearDown() {boolean del = fc.delete(new Path(fileContextTestHelper.getAbsoluteTestRootPath(fc), new Path("test")), true);assertTrue(del);fc.delete(localFsRootPath, true);}
protected Path getDefaultWorkingDirectory() {return getTestRootPath(fc, "/user/" + System.getProperty("user.name")).makeQualified(fc.getDefaultFileSystem().getUri(), fc.getWorkingDirectory());}
protected boolean renameSupported() {return true;}
protected IOException unwrapException(IOException e) {return e;}
public void testFsStatus() {FsStatus fsStatus = fc.getFsStatus(null);Assert.assertNotNull(fsStatus);//used, free and capacity are non-negative longsAssert.assertTrue(fsStatus.getUsed() >= 0);Assert.assertTrue(fsStatus.getRemaining() >= 0);Assert.assertTrue(fsStatus.getCapacity() >= 0);}
public void testWorkingDirectory() {// First we cd to our test rootPath workDir = new Path(fileContextTestHelper.getAbsoluteTestRootPath(fc), new Path("test"));fc.setWorkingDirectory(workDir);Assert.assertEquals(workDir, fc.getWorkingDirectory());fc.setWorkingDirectory(new Path("."));Assert.assertEquals(workDir, fc.getWorkingDirectory());fc.setWorkingDirectory(new Path(".."));Assert.assertEquals(workDir.getParent(), fc.getWorkingDirectory());// Go back to our test rootworkDir = new Path(fileContextTestHelper.getAbsoluteTestRootPath(fc), new Path("test"));fc.setWorkingDirectory(workDir);Assert.assertEquals(workDir, fc.getWorkingDirectory());Path relativeDir = new Path("existingDir1");Path absoluteDir = new Path(workDir, "existingDir1");fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);fc.setWorkingDirectory(relativeDir);Assert.assertEquals(absoluteDir, fc.getWorkingDirectory());// cd using a absolute pathabsoluteDir = getTestRootPath(fc, "test/existingDir2");fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);fc.setWorkingDirectory(absoluteDir);Assert.assertEquals(absoluteDir, fc.getWorkingDirectory());// Now open a file relative to the wd we just set above.Path absolutePath = new Path(absoluteDir, "foo");fc.create(absolutePath, EnumSet.of(CREATE)).close();fc.open(new Path("foo")).close();// Now mkdir relative to the dir we cd'ed tofc.mkdir(new Path("newDir"), FileContext.DEFAULT_PERM, true);Assert.assertTrue(isDir(fc, new Path(absoluteDir, "newDir")));absoluteDir = getTestRootPath(fc, "nonexistingPath");try {fc.setWorkingDirectory(absoluteDir);Assert.fail("cd to non existing dir should have failed");} catch (Exception e) {}absoluteDir = new Path(localFsRootPath, "existingDir");fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);fc.setWorkingDirectory(absoluteDir);Assert.assertEquals(absoluteDir, fc.getWorkingDirectory());Path aRegularFile = new Path("aRegularFile");createFile(aRegularFile);try {fc.setWorkingDirectory(aRegularFile);fail("An IOException expected.");} catch (IOException ioe) {}}
public void testMkdirs() {Path testDir = getTestRootPath(fc, "test/hadoop");Assert.assertFalse(exists(fc, testDir));Assert.assertFalse(isFile(fc, testDir));fc.mkdir(testDir, FsPermission.getDefault(), true);Assert.assertTrue(exists(fc, testDir));Assert.assertFalse(isFile(fc, testDir));fc.mkdir(testDir, FsPermission.getDefault(), true);Assert.assertTrue(exists(fc, testDir));Assert.assertFalse(isFile(fc, testDir));Path parentDir = testDir.getParent();Assert.assertTrue(exists(fc, parentDir));Assert.assertFalse(isFile(fc, parentDir));Path grandparentDir = parentDir.getParent();Assert.assertTrue(exists(fc, grandparentDir));Assert.assertFalse(isFile(fc, grandparentDir));}
public void testMkdirsFailsForSubdirectoryOfExistingFile() {Path testDir = getTestRootPath(fc, "test/hadoop");Assert.assertFalse(exists(fc, testDir));fc.mkdir(testDir, FsPermission.getDefault(), true);Assert.assertTrue(exists(fc, testDir));createFile(getTestRootPath(fc, "test/hadoop/file"));Path testSubDir = getTestRootPath(fc, "test/hadoop/file/subdir");try {fc.mkdir(testSubDir, FsPermission.getDefault(), true);Assert.fail("Should throw IOException.");} catch (IOException e) {}Assert.assertFalse(exists(fc, testSubDir));Path testDeepSubDir = getTestRootPath(fc, "test/hadoop/file/deep/sub/dir");try {fc.mkdir(testDeepSubDir, FsPermission.getDefault(), true);Assert.fail("Should throw IOException.");} catch (IOException e) {}Assert.assertFalse(exists(fc, testDeepSubDir));}
public void testGetFileStatusThrowsExceptionForNonExistentFile() {try {fc.getFileStatus(getTestRootPath(fc, "test/hadoop/file"));Assert.fail("Should throw FileNotFoundException");} catch (FileNotFoundException e) {}}
public void testListStatusThrowsExceptionForNonExistentFile() {try {fc.listStatus(getTestRootPath(fc, "test/hadoop/file"));Assert.fail("Should throw FileNotFoundException");} catch (FileNotFoundException fnfe) {}}
public void testListStatus() {Path[] testDirs = { getTestRootPath(fc, "test/hadoop/a"), getTestRootPath(fc, "test/hadoop/b"), getTestRootPath(fc, "test/hadoop/c/1") };Assert.assertFalse(exists(fc, testDirs[0]));for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}// test listStatus that returns an arrayFileStatus[] paths = fc.util().listStatus(getTestRootPath(fc, "test"));Assert.assertEquals(1, paths.length);Assert.assertEquals(getTestRootPath(fc, "test/hadoop"), paths[0].getPath());paths = fc.util().listStatus(getTestRootPath(fc, "test/hadoop"));Assert.assertEquals(3, paths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/a"), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/b"), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/c"), paths));paths = fc.util().listStatus(getTestRootPath(fc, "test/hadoop/a"));Assert.assertEquals(0, paths.length);RemoteIterator<FileStatus> pathsIterator = fc.listStatus(getTestRootPath(fc, "test"));Assert.assertEquals(getTestRootPath(fc, "test/hadoop"), pathsIterator.next().getPath());Assert.assertFalse(pathsIterator.hasNext());pathsIterator = fc.listStatus(getTestRootPath(fc, "test/hadoop"));FileStatus[] subdirs = new FileStatus[3];int i = 0;while (i < 3 && pathsIterator.hasNext()) {subdirs[i++] = pathsIterator.next();}Assert.assertFalse(pathsIterator.hasNext());Assert.assertTrue(i == 3);Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/a"), subdirs));Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/b"), subdirs));Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop/c"), subdirs));pathsIterator = fc.listStatus(getTestRootPath(fc, "test/hadoop/a"));Assert.assertFalse(pathsIterator.hasNext());}
public void testListStatusFilterWithNoMatches() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA2), getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}// listStatus with filters returns empty correctlyFileStatus[] filteredPaths = fc.util().listStatus(getTestRootPath(fc, "test"), TEST_X_FILTER);Assert.assertEquals(0, filteredPaths.length);}
public void testListStatusFilterWithSomeMatches() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AAA2) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}// should return 2 paths ("/test/hadoop/axa" and "/test/hadoop/axx")FileStatus[] filteredPaths = fc.util().listStatus(getTestRootPath(fc, "test/hadoop"), TEST_X_FILTER);Assert.assertEquals(2, filteredPaths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), filteredPaths));}
public void testGlobStatusNonExistentFile() {FileStatus[] paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoopfsdf"));Assert.assertNull(paths);paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoopfsdf/?"));Assert.assertEquals(0, paths.length);paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoopfsdf/xyz*/?"));Assert.assertEquals(0, paths.length);}
public void testGlobStatusWithNoMatchesInPath() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AAA2) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}// should return nothingFileStatus[] paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/?"));Assert.assertEquals(0, paths.length);}
public void testGlobStatusSomeMatchesInDirectories() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AAA2) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}// Should return two items ("/test/hadoop" and "/test/hadoop2")FileStatus[] paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop*"));Assert.assertEquals(2, paths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop"), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, "test/hadoop2"), paths));}
public void testGlobStatusWithMultipleWildCardMatches() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AAA2) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//"/test/hadoop/axx", and "/test/hadoop2/axx")FileStatus[] paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop*/*"));Assert.assertEquals(4, paths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AAA), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AAA2), paths));}
public void testGlobStatusWithMultipleMatchesOfSingleChar() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AAA2) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//Should return only 2 items ("/test/hadoop/axa", "/test/hadoop/axx")FileStatus[] paths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/ax?"));Assert.assertEquals(2, paths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), paths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), paths));}
public void testGlobStatusFilterWithEmptyPathResults() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return an empty setFileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/?"), DEFAULT_FILTER);Assert.assertEquals(0, filteredPaths.length);}
public void testGlobStatusFilterWithSomePathMatchesAndTrivialFilter() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return all three (aaa, axa, axx)FileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/*"), DEFAULT_FILTER);Assert.assertEquals(3, filteredPaths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AAA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), filteredPaths));}
public void testGlobStatusFilterWithMultipleWildCardMatchesAndTrivialFilter() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return all three (aaa, axa, axx)FileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/a??"), DEFAULT_FILTER);Assert.assertEquals(3, filteredPaths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AAA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), filteredPaths));}
public void testGlobStatusFilterWithMultiplePathMatchesAndNonTrivialFilter() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return two (axa, axx)FileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/*"), TEST_X_FILTER);Assert.assertEquals(2, filteredPaths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), filteredPaths));}
public void testGlobStatusFilterWithNoMatchingPathsAndNonTrivialFilter() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return an empty setFileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/?"), TEST_X_FILTER);Assert.assertEquals(0, filteredPaths.length);}
public void testGlobStatusFilterWithMultiplePathWildcardsAndNonTrivialFilter() {Path[] testDirs = { getTestRootPath(fc, TEST_DIR_AAA), getTestRootPath(fc, TEST_DIR_AXA), getTestRootPath(fc, TEST_DIR_AXX), getTestRootPath(fc, TEST_DIR_AXX) };if (exists(fc, testDirs[0]) == false) {for (Path path : testDirs) {fc.mkdir(path, FsPermission.getDefault(), true);}}//This should return two (axa, axx)FileStatus[] filteredPaths = fc.util().globStatus(getTestRootPath(fc, "test/hadoop/a??"), TEST_X_FILTER);Assert.assertEquals(2, filteredPaths.length);Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXA), filteredPaths));Assert.assertTrue(containsPath(getTestRootPath(fc, TEST_DIR_AXX), filteredPaths));}
public void testWriteReadAndDeleteEmptyFile() {writeReadAndDelete(0);}
public void testWriteReadAndDeleteHalfABlock() {writeReadAndDelete(getDefaultBlockSize() / 2);}
public void testWriteReadAndDeleteOneBlock() {writeReadAndDelete(getDefaultBlockSize());}
public void testWriteReadAndDeleteOneAndAHalfBlocks() {int blockSize = getDefaultBlockSize();writeReadAndDelete(blockSize + (blockSize / 2));}
public void testWriteReadAndDeleteTwoBlocks() {writeReadAndDelete(getDefaultBlockSize() * 2);}
private void writeReadAndDelete(int len) {Path path = getTestRootPath(fc, "test/hadoop/file");fc.mkdir(path.getParent(), FsPermission.getDefault(), true);FSDataOutputStream out = fc.create(path, EnumSet.of(CREATE), CreateOpts.repFac((short) 1), CreateOpts.blockSize(getDefaultBlockSize()));out.write(data, 0, len);out.close();Assert.assertTrue("Exists", exists(fc, path));Assert.assertEquals("Length", len, fc.getFileStatus(path).getLen());FSDataInputStream in = fc.open(path);byte[] buf = new byte[len];in.readFully(0, buf);in.close();Assert.assertEquals(len, buf.length);for (int i = 0; i < buf.length; i++) {Assert.assertEquals("Position " + i, data[i], buf[i]);}Assert.assertTrue("Deleted", fc.delete(path, false));Assert.assertFalse("No longer exists", exists(fc, path));}
public void testNullCreateFlag() {Path p = getTestRootPath(fc, "test/file");fc.create(p, null);Assert.fail("Excepted exception not thrown");}
public void testEmptyCreateFlag() {Path p = getTestRootPath(fc, "test/file");fc.create(p, EnumSet.noneOf(CreateFlag.class));Assert.fail("Excepted exception not thrown");}
public void testCreateFlagCreateExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagCreateExistingFile");createFile(p);fc.create(p, EnumSet.of(CREATE));Assert.fail("Excepted exception not thrown");}
public void testCreateFlagOverwriteNonExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagOverwriteNonExistingFile");fc.create(p, EnumSet.of(OVERWRITE));Assert.fail("Excepted exception not thrown");}
public void testCreateFlagOverwriteExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagOverwriteExistingFile");createFile(p);FSDataOutputStream out = fc.create(p, EnumSet.of(OVERWRITE));writeData(fc, p, out, data, data.length);}
public void testCreateFlagAppendNonExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagAppendNonExistingFile");fc.create(p, EnumSet.of(APPEND));Assert.fail("Excepted exception not thrown");}
public void testCreateFlagAppendExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagAppendExistingFile");createFile(p);FSDataOutputStream out = fc.create(p, EnumSet.of(APPEND));writeData(fc, p, out, data, 2 * data.length);}
public void testCreateFlagCreateAppendNonExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagCreateAppendNonExistingFile");FSDataOutputStream out = fc.create(p, EnumSet.of(CREATE, APPEND));writeData(fc, p, out, data, data.length);}
public void testCreateFlagCreateAppendExistingFile() {Path p = getTestRootPath(fc, "test/testCreateFlagCreateAppendExistingFile");createFile(p);FSDataOutputStream out = fc.create(p, EnumSet.of(CREATE, APPEND));writeData(fc, p, out, data, 2 * data.length);}
public void testCreateFlagAppendOverwrite() {Path p = getTestRootPath(fc, "test/nonExistent");fc.create(p, EnumSet.of(APPEND, OVERWRITE));Assert.fail("Excepted exception not thrown");}
public void testCreateFlagAppendCreateOverwrite() {Path p = getTestRootPath(fc, "test/nonExistent");fc.create(p, EnumSet.of(CREATE, APPEND, OVERWRITE));Assert.fail("Excepted exception not thrown");}
private static void writeData(FileContext fc, Path p, FSDataOutputStream out, byte[] data, long expectedLen) {out.write(data, 0, data.length);out.close();Assert.assertTrue("Exists", exists(fc, p));Assert.assertEquals("Length", expectedLen, fc.getFileStatus(p).getLen());}
public void testWriteInNonExistentDirectory() {Path path = getTestRootPath(fc, "test/hadoop/file");Assert.assertFalse("Parent doesn't exist", exists(fc, path.getParent()));createFile(path);Assert.assertTrue("Exists", exists(fc, path));Assert.assertEquals("Length", data.length, fc.getFileStatus(path).getLen());Assert.assertTrue("Parent exists", exists(fc, path.getParent()));}
public void testDeleteNonExistentFile() {Path path = getTestRootPath(fc, "test/hadoop/file");Assert.assertFalse("Doesn't exist", exists(fc, path));Assert.assertFalse("No deletion", fc.delete(path, true));}
public void testDeleteRecursively() {Path dir = getTestRootPath(fc, "test/hadoop");Path file = getTestRootPath(fc, "test/hadoop/file");Path subdir = getTestRootPath(fc, "test/hadoop/subdir");createFile(file);fc.mkdir(subdir, FsPermission.getDefault(), true);Assert.assertTrue("File exists", exists(fc, file));Assert.assertTrue("Dir exists", exists(fc, dir));Assert.assertTrue("Subdir exists", exists(fc, subdir));try {fc.delete(dir, false);Assert.fail("Should throw IOException.");} catch (IOException e) {}Assert.assertTrue("File still exists", exists(fc, file));Assert.assertTrue("Dir still exists", exists(fc, dir));Assert.assertTrue("Subdir still exists", exists(fc, subdir));Assert.assertTrue("Deleted", fc.delete(dir, true));Assert.assertFalse("File doesn't exist", exists(fc, file));Assert.assertFalse("Dir doesn't exist", exists(fc, dir));Assert.assertFalse("Subdir doesn't exist", exists(fc, subdir));}
public void testDeleteEmptyDirectory() {Path dir = getTestRootPath(fc, "test/hadoop");fc.mkdir(dir, FsPermission.getDefault(), true);Assert.assertTrue("Dir exists", exists(fc, dir));Assert.assertTrue("Deleted", fc.delete(dir, false));Assert.assertFalse("Dir doesn't exist", exists(fc, dir));}
public void testRenameNonExistentPath() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/nonExistent");Path dst = getTestRootPath(fc, "test/new/newpath");try {rename(src, dst, false, false, false, Rename.NONE);Assert.fail("Should throw FileNotFoundException");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}try {rename(src, dst, false, false, false, Rename.OVERWRITE);Assert.fail("Should throw FileNotFoundException");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}}
public void testRenameFileToNonExistentDirectory() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);Path dst = getTestRootPath(fc, "test/nonExistent/newfile");try {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}try {rename(src, dst, false, true, false, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}}
public void testRenameFileToDestinationWithParentFile() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);Path dst = getTestRootPath(fc, "test/parentFile/newfile");createFile(dst.getParent());try {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {}try {rename(src, dst, false, true, false, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {}}
public void testRenameFileToExistingParent() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);Path dst = getTestRootPath(fc, "test/new/newfile");fc.mkdir(dst.getParent(), FileContext.DEFAULT_PERM, true);rename(src, dst, true, false, true, Rename.OVERWRITE);}
public void testRenameFileToItself() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);try {rename(src, src, false, true, false, Rename.NONE);Assert.fail("Renamed file to itself");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}// Also fails with overwritetry {rename(src, src, false, true, false, Rename.OVERWRITE);Assert.fail("Renamed file to itself");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}}
public void testRenameFileAsExistingFile() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);Path dst = getTestRootPath(fc, "test/new/existingFile");createFile(dst);// Fails without overwrite optiontry {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}// Succeeds with overwrite optionrename(src, dst, true, false, true, Rename.OVERWRITE);}
public void testRenameFileAsExistingDirectory() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);Path dst = getTestRootPath(fc, "test/new/existingDir");fc.mkdir(dst, FileContext.DEFAULT_PERM, true);// Fails without overwrite optiontry {rename(src, dst, false, false, true, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {}// File cannot be renamed as directorytry {rename(src, dst, false, false, true, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {}}
public void testRenameDirectoryToItself() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);try {rename(src, src, false, true, false, Rename.NONE);Assert.fail("Renamed directory to itself");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}// Also fails with overwritetry {rename(src, src, false, true, false, Rename.OVERWRITE);Assert.fail("Renamed directory to itself");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}}
public void testRenameDirectoryToNonExistentParent() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);Path dst = getTestRootPath(fc, "test/nonExistent/newdir");try {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}try {rename(src, dst, false, true, false, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileNotFoundException);}}
public void testRenameDirectoryAsNonExistentDirectory() {testRenameDirectoryAsNonExistentDirectory(Rename.NONE);tearDown();testRenameDirectoryAsNonExistentDirectory(Rename.OVERWRITE);}
private void testRenameDirectoryAsNonExistentDirectory(Rename... options) {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);createFile(getTestRootPath(fc, "test/hadoop/dir/file1"));createFile(getTestRootPath(fc, "test/hadoop/dir/subdir/file2"));Path dst = getTestRootPath(fc, "test/new/newdir");fc.mkdir(dst.getParent(), FileContext.DEFAULT_PERM, true);rename(src, dst, true, false, true, options);Assert.assertFalse("Nested file1 exists", exists(fc, getTestRootPath(fc, "test/hadoop/dir/file1")));Assert.assertFalse("Nested file2 exists", exists(fc, getTestRootPath(fc, "test/hadoop/dir/subdir/file2")));Assert.assertTrue("Renamed nested file1 exists", exists(fc, getTestRootPath(fc, "test/new/newdir/file1")));Assert.assertTrue("Renamed nested exists", exists(fc, getTestRootPath(fc, "test/new/newdir/subdir/file2")));}
public void testRenameDirectoryAsEmptyDirectory() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);createFile(getTestRootPath(fc, "test/hadoop/dir/file1"));createFile(getTestRootPath(fc, "test/hadoop/dir/subdir/file2"));Path dst = getTestRootPath(fc, "test/new/newdir");fc.mkdir(dst, FileContext.DEFAULT_PERM, true);// Fails without overwrite optiontry {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}// Succeeds with the overwrite optionrename(src, dst, true, false, true, Rename.OVERWRITE);}
public void testRenameDirectoryAsNonEmptyDirectory() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);createFile(getTestRootPath(fc, "test/hadoop/dir/file1"));createFile(getTestRootPath(fc, "test/hadoop/dir/subdir/file2"));Path dst = getTestRootPath(fc, "test/new/newdir");fc.mkdir(dst, FileContext.DEFAULT_PERM, true);createFile(getTestRootPath(fc, "test/new/newdir/file1"));// Fails without overwrite optiontry {rename(src, dst, false, true, false, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {Assert.assertTrue(unwrapException(e) instanceof FileAlreadyExistsException);}// Fails even with the overwrite optiontry {rename(src, dst, false, true, false, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException ex) {}}
public void testRenameDirectoryAsFile() {if (!renameSupported())return;Path src = getTestRootPath(fc, "test/hadoop/dir");fc.mkdir(src, FileContext.DEFAULT_PERM, true);Path dst = getTestRootPath(fc, "test/new/newfile");createFile(dst);// Fails without overwrite optiontry {rename(src, dst, false, true, true, Rename.NONE);Assert.fail("Expected exception was not thrown");} catch (IOException e) {}// Directory cannot be renamed as existing filetry {rename(src, dst, false, true, true, Rename.OVERWRITE);Assert.fail("Expected exception was not thrown");} catch (IOException ex) {}}
public void testInputStreamClosedTwice() {//streams should have no effect. Path src = getTestRootPath(fc, "test/hadoop/file");createFile(src);FSDataInputStream in = fc.open(src);in.close();in.close();}
public void testOutputStreamClosedTwice() {//streams should have no effect. Path src = getTestRootPath(fc, "test/hadoop/file");FSDataOutputStream out = fc.create(src, EnumSet.of(CREATE), Options.CreateOpts.createParent());//write some dataout.writeChar('H');out.close();out.close();}
public /** Test FileContext APIs when symlinks are not supported */void testUnsupportedSymlink() {Path file = getTestRootPath(fc, "file");Path link = getTestRootPath(fc, "linkToFile");if (!fc.getDefaultFileSystem().supportsSymlinks()) {try {fc.createSymlink(file, link, false);Assert.fail("Created a symlink on a file system that " + "does not support symlinks.");} catch (IOException e) {}createFile(file);try {fc.getLinkTarget(file);Assert.fail("Got a link target on a file system that " + "does not support symlinks.");} catch (IOException e) {}Assert.assertEquals(fc.getFileStatus(file), fc.getFileLinkStatus(file));}}
protected void createFile(Path path) {FSDataOutputStream out = fc.create(path, EnumSet.of(CREATE), Options.CreateOpts.createParent());out.write(data, 0, data.length);out.close();}
private void rename(Path src, Path dst, boolean renameShouldSucceed, boolean srcExists, boolean dstExists, Rename... options) {fc.rename(src, dst, options);if (!renameShouldSucceed)Assert.fail("rename should have thrown exception");Assert.assertEquals("Source exists", srcExists, exists(fc, src));Assert.assertEquals("Destination exists", dstExists, exists(fc, dst));}
private boolean containsPath(Path path, FileStatus[] filteredPaths) {for (int i = 0; i < filteredPaths.length; i++) {if (getTestRootPath(fc, path.toString()).equals(filteredPaths[i].getPath()))return true;}return false;}
public void testOpen2() {final Path rootPath = getTestRootPath(fc, "test");//final Path rootPath = getAbsoluteTestRootPath(fc);final Path path = new Path(rootPath, "zoo");createFile(path);final long length = fc.getFileStatus(path).getLen();FSDataInputStream fsdis = fc.open(path, 2048);try {byte[] bb = new byte[(int) length];fsdis.readFully(bb);assertArrayEquals(data, bb);} finally {fsdis.close();}}
public void testSetVerifyChecksum() {final Path rootPath = getTestRootPath(fc, "test");final Path path = new Path(rootPath, "zoo");FSDataOutputStream out = fc.create(path, EnumSet.of(CREATE), Options.CreateOpts.createParent());try {fc.setVerifyChecksum(true, path);out.write(data, 0, data.length);} finally {out.close();}FileStatus fileStatus = fc.getFileStatus(path);final long len = fileStatus.getLen();assertTrue(len == data.length);byte[] bb = new byte[(int) len];FSDataInputStream fsdis = fc.open(path);try {fsdis.read(bb);} finally {fsdis.close();}assertArrayEquals(data, bb);}
public void testListCorruptFileBlocks() {final Path rootPath = getTestRootPath(fc, "test");final Path path = new Path(rootPath, "zoo");createFile(path);try {final RemoteIterator<Path> remoteIterator = fc.listCorruptFileBlocks(path);if (listCorruptedBlocksSupported()) {assertTrue(remoteIterator != null);Path p;while (remoteIterator.hasNext()) {p = remoteIterator.next();System.out.println("corrupted block: " + p);}try {remoteIterator.next();fail();} catch (NoSuchElementException nsee) {}} else {fail();}} catch (UnsupportedOperationException uoe) {if (listCorruptedBlocksSupported()) {fail(uoe.toString());} else {}}}
public void testDeleteOnExitUnexisting() {final Path rootPath = getTestRootPath(fc, "test");final Path path = new Path(rootPath, "zoo");boolean registered = fc.deleteOnExit(path);// because "zoo" does not exist:assertTrue(!registered);}
public void testFileContextStatistics() {FileContext.clearStatistics();final Path rootPath = getTestRootPath(fc, "test");final Path path = new Path(rootPath, "zoo");createFile(path);byte[] bb = new byte[data.length];FSDataInputStream fsdis = fc.open(path);try {fsdis.read(bb);} finally {fsdis.close();}assertArrayEquals(data, bb);FileContext.printStatistics();}
public /*   * Test method   *  org.apache.hadoop.fs.FileContext.getFileContext(AbstractFileSystem)   */void testGetFileContext1() {final Path rootPath = getTestRootPath(fc, "test");AbstractFileSystem asf = fc.getDefaultFileSystem();// create FileContext using the protected #getFileContext(1) method:FileContext fc2 = FileContext.getFileContext(asf);// Now just check that this context can do something reasonable:final Path path = new Path(rootPath, "zoo");FSDataOutputStream out = fc2.create(path, EnumSet.of(CREATE), Options.CreateOpts.createParent());out.close();Path pathResolved = fc2.resolvePath(path);assertEquals(pathResolved.toUri().getPath(), path.toUri().getPath());}
private Path getTestRootPath(FileContext fc, String pathString) {return fileContextTestHelper.getTestRootPath(fc, pathString);}